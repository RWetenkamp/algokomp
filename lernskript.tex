\documentclass[11pt,a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{listings}
\usepackage{upgreek}
\usepackage{amsthm}
\usepackage{ulem}
\usepackage{graphicx}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage[backend=biber, style=alphabetic]{biblatex}
\addbibresource{literature.bib}
\author{Roman Wetenkamp}
\title{Algorithmen \& Komplexität}
\subtitle{Theoretische Informatik}

\newtheorem{note}{Bemerkung}
\newtheorem{definition}{Definition}
\newtheorem{satz}{Satz}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Beispiel}
\begin{document}
\vspace{3cm}
\maketitle
\begin{center}
\includegraphics[scale=0.7]{DHBW.jpg}
\end{center}
\pagebreak
\tableofcontents
\pagebreak
\section*{Vorwort}
Die theoretische Informatik ist genau das Thema, vor dem ich vor Beginn meines Studiums am meisten Respekt hatte ... Und das ist es neben den anderen Mathematik-Vorlesungen bis heute: Theoreme, Kalküle und Formeln sind nun einmal nicht die Dinge, mit denen sich die Masse der Informatikstudierenden gerne befasst, mich eingeschlossen. Mit einer (vermutlich) gerade eben bestandenen Logikklausur im ersten Semester sind dies Gründe genug dafür ergänzendes, motivierendes und begreifbares Material zusammenzustellen, dass mich einerseits akut durch die Klausur dieses Semesters bringt und hoffentlich auch für andere einen Nutzen hat. Geteiltes Leid ist halbes Leid! \\\\
Hier möchte ich den Vorlesungsstoff auf meine Art zusammenfassen und komplettieren, um Beispiele ergänzen und mit Aufgaben versehen, wie ich sie in der Vorlesung oder weiteren Büchern antraf. Ich persönlich lerne durch Aufgaben einfach am besten und für diejenigen, denen es auch so geht, gibt es davon hier ausreichend. Lösungen finden sich im Anhang. \\\\
\textit{Viel Erfolg!}  \\
\begin{flushright}
Roman Wetenkamp \\
Mannheim, den \today
\end{flushright}  
\vfill
\paragraph{Hinweis}
Dieses Dokument ist kein Vorlesungsmaterial, hat keineswegs den Anspruch auf Vollständigkeit und enthält mit Sicherheit Fehler. Desweiteren ist es noch lange nicht vollendet (es ist infrage zustellen, ob es das je sein wird), und doch möchte ich Sie ermutigen, beizutragen! Jegliche Fehler, Probleme oder Anmerkungen können Sie mir gerne über das dazugehörige GitHub-Repository unter der URL \url{https://github.com/RWetenkamp/algokomp} zukommen lassen. Ebenso sind Sie völlig frei darin, dieses Dokument für Ihre legitimen Zwecke zu nutzen -- bitte beachten Sie dennoch Ihr Gewissen. Danke!
\pagebreak
\part{Grundlagen der Algorithmik}
\section{Der Algorithmusbegriff}
Wie der Titel dieses Moduls schon verrät, befassen wir uns mit Algorithmen: Jenen nebulösen Dingen, die uns mit ähnlich Denkenden und Interessierten in Social-Media-Plattformen zusammenbringen, komplexe mathematische Berechnungen ausführen und zunehmend mehr an Einfluss in unserem Leben gewinnen. Wir betrachten hier nun die Wortherkunft, definieren den Begriff anschließend und gehen auf Eigenschaften eines solchen ein.
\paragraph{Wortherkunft}
Der Begriff {\glqq}Algorithmus{\grqq} besteht zum einen aus dem altgriechischen Wort \textit{arithmos} -- Zahl und zum anderen geht er zurück auf den persischen Mathematiker \textsc{al-Charismi} (780-846 n. Chr.), dessen Werk \textit{{\glqq}Algorismus{\grqq}} schon eine gewisse Nähe zum heute üblichen Begriff offenbart. \parencite{brockhaus:algorithmus}
\paragraph{Definition}
Unter einem Algorithmus verstehen wir eindeutige Verarbeitungsvorschriften zur Lösung eines Problems oder einer Problemklasse. Daran geknüpft ist der Gedanke des EVA-Prinzips: Eine Eingabe wird verarbeitet und auf Grundlage dessen wird eine Ausgabe erzeugt, ein Algorithmus ist hier für die Verarbeitung bis zur Ausgabe notwendig. \\
Algorithmen finden in einer Vielzahl von Bereiches unseres Lebens Einsatz finden: 
\begin{itemize}
\item Zur Untersuchung großer Datenmengen
\item Zur Kommunikation/Suche im Internet
\item In bildgebenden Verfahren oder diagnostischen Anwendungen der Medizin
\item Assistenzsystemen im Auto
\item Bei der Partnersuche über Online-Dating-Plattformen
\end{itemize}
Damit finden wir intuitiv eine Begründung für die Untersuchung von Algorithmen.
\begin{note}
Ein \textit{Algorithmus} ist von einem \textit{Programm} unbedingt abzugrenzen: Der Algorithmus bezeichnet ein abstraktes Konzept zur Lösung eines gegebenen Problems während ein Programm die konkrete Umsetzung eines Algorithmus in einer Programmiersprache ist. 
\end{note}
Bei der Betrachtung von Algorithmen betrachten wir die folgenden drei Ziele:
\begin{itemize}
\item Korrektheit
\item Effizienz
\item Einfachheit
\end{itemize}
\paragraph{Einfachheit}
Aus wirtschaftlichen Gründen besteht ein Interesse daran, dass ein Algorithmus so einfach wie möglich ist, beispielsweise in Bezug auf Implementierungskosten. Ein weiterer Aspekt besteht in der Prüfung auf Korrektheit: Zu einem zu komplexen Algorithmus lässt sich nur schwer feststellen, ob dieser korrekt ist.
\paragraph{Korrektheit}
Die Korrektheit ist die zentrale Eigenschaft eines Algorithmus -- Wenn ein Algorithmus nicht das leistet, was er vorgibt zu leisten oder was von ihm erwartet wird, ist er wertlos. Die Spezifikation beschreibt das exakte Verhalten unter Angabe von einer Vorbedingung (Zustand der Daten vor der Ausführung) und einer Nachbedingung (erwünschter Zustand nach der Ausführung).
\begin{definition}
Ein Algorithmus heißt \textbf{partiell korrekt}, falls aus erfüllter Vorbedingung die Nachbedingung folgt. \\
Ein Algorithmus heißt \textbf{total korrekt}, falls er partiell korrekt ist und nach endlich vielen Schritten terminiert.
\end{definition}
Die Korrektheit eines Algorithmus kann manuell, per Implementierung oder mathematisch verifiziert werden, wobei letztere Variante häufig vorzuziehen ist.
\begin{note}
Nicht jeder Algorithmus muss für jede Eingabe terminieren. Unter dem \textbf{Halteproblem} versteht man die Frage, ob ein Algorithmus nach endlich vielen Schritten terminiert. Diese Frage ist algorithmisch nicht entscheidbar, wie \textsc{Alan Turing} feststellte. Stattdessen muss die Termination für jeden Algorithmus einzeln entschieden werden.
\end{note}
\section{Effizienz / Komplexität}
Weitaus komplexer als die Frage der Einfachheit oder Korrektheit ist jene der Komplexität selbst:
\begin{definition}
Die Effizienz / Komplexität eines Algorithmus ist ein Maß für die Menge an Ressourcen (Rechenzeit oder Speicherbedarf), die er benötigt.
\end{definition}
Dabei ist zu beachten, dass es für ein Problem häufig mehrere Algorithmen gibt, die sich in ihrer Komplexität unterscheiden. Daher muss die Komplexität für jeden Algorithmus einzeln bestimmt werden. Dieses Skript befasst sich im Wesentlichen nur mit der Rechenzeit. \\\\
Für eine wirklich präzise Aussage über die Rechenzeit wäre es erforderlich, für jede Implementation eines Algorithmus in einer Programmiersprache einzeln festzustellen, welche Rechenzeit jede einzelne Operation auf dem spezifischen Prozessor des Testsystems benötigt. Da diese Aussagen mittlerweile aufgrund der Entwicklungen auf dem Prozessormarkt an Relevanz verlieren, verlagert sich die Betrachtung auf die Komplexität eines Algorithmus. \\\\
Die Komplexität eines Algorithmus kann folglich keine genaue Angabe in einer Zeiteinheit sein, die beispielsweise aussagt, wie schnell denn nun wirklich der größte gemeinsame Teiler von 443 und 123 mithilfe des euklidischen Algorithmus berechnet werden kann, sondern ist ein abstraktes Konzept, das Vergleiche zwischen Algorithmen ermöglichen soll:
\begin{itemize}
\item Abstraktion konstanter Faktoren
\item Abstraktion unbedeutender Terme
\item Wachstumsverhalten der Laufzeit bei Veränderung der Größe der Eingabe
\end{itemize}
Diese Abstraktion motiviert nun den folgenden Abschnitt.
\section{Maß der Komplexität: $O$-Notation}
Nach dem Mathematiker \textsc{Edmund Landau} definieren wir in Bezug auf das Wachstum von Funktionen eine {\glqq}obere Schranke{\grqq} und nennen diese $O(f)$. Falls eine Funktion $g$ nicht schneller wächst als $f$, so sagen wir, dass $g \in O(f)$. Eine ebenfalls übliche, jedoch irritierende Schreibweise ist $g = O(f)$.
\begin{definition}
Sei $\mathbb{R_{+}} := \{ x \in \mathbb{R} \mid x > 0 \}$ und $\mathbb{R_{+}^{\mathbb{N}}} := \{f \mid f \text{ist eine Funktion der Form } f : \mathbb{N} \to \mathbb{R_{+}}\}$ \\\\
Sei $g \in \mathbb{R_{+}^{\mathbb{N}}}$ gegeben. Dann ist die Menge aller Funktionen, die höchstens so schnell wachsen wie $g$ definiert als: \begin{align*}
O(g) := \{f \in \mathbb{R_{+}^{\mathbb{N}}} \mid \exists n_{0} \in \mathbb{N}: \exists c \in \mathbb{R_{+}}: \forall n \in \mathbb{N}: (n \geq n_{0} \Rightarrow f(n) \leq c \cdot g(n))\}
\end{align*}
\end{definition}
Die $O$-Notation gibt folglich eine Menge vergleichbarer Funktionen an, die es ermöglichen, die Komplexität eines Algorithmus abzuschätzen.
\begin{example}
Behauptung:
$3n^{3} + 2n^{2} + 7 \in O(n^{3})$
\begin{proof}

Gesucht ist eine Konstante $c \in \mathbb{R_{+}}$ und eine Konstante $n_{0} \in \mathbb{N}$, sodass für alle $n \in \mathbb{N}$ mit $n \geq n_{0}$ gilt: \\
$3n^{3} + 2n^{2} + 7 \leq c \cdot n^{3}$ \\\\
Wähle $n_{0} := 1 \text{ und } c := 12$ \footnote{Die Wahl von $c$ und $n$ erfolgt hier durch Ausprobieren -- In diesem Fall ergibt sich 12 als Summe aus den Potenzfaktoren.} \\
Sei nun 
\begin{align}
& 1 \leq n \\
(1)^{3}: \quad & 1 \leq n^{3} \\
(2) \cdot 7: \quad & 7 \leq 7n^{3} \\
(1) \cdot 2n^{2}: \quad & 2n^{2} \leq 2n^{3} \\
\quad & 3n^{3} \leq 3n^{3} \\
(3)+(4)+(5): \quad & 3n^{3} + 2n^{2} + 7 \leq 12n^{3}
\end{align}
Durch die Ungleichungen (3), (4) und (5) ist für jedes Polynom einzeln eine Abschätzung erfolgt. Addiert man diese Ungleichungen nun, ergibt sich (6). Damit ist gezeigt, dass die Behauptung wahr ist und die Funktion in $O(n^{3})$ liegt. Aus der Definition der $O$-Notation ergibt sich, dass jede Funktion mit höchstem Grad 3 in $O(n^{3})$ liegt, da Vorfaktoren und Konstantglieder entfallen. 
\end{proof}
\end{example} 
Die $O$-Notation eines Algorithmus lässt sich auch durch vollständige Induktion beweisen.
\begin{example}
Behauptung: $n \in O(2^{n})$
\begin{proof}
Wähle $n_{0} := 0, c:= 1$. Zu zeigen: $n \leq 2^{n} \forall n \in \mathbb{N}$\\\\
Beweis durch Induktion nach $n$: \\
Induktionsanfang (IA): $n = 0 \quad n = 0 \leq 1 = 2^{0} = 2^{n}$ \\
Induktionsvoraussetzung (IV): $n \leq 2^{n}$ z.Z. $n+1 \leq 2^{n+1}$ \\
Induktionsschritt (IS): $n \mapsto n + 1$ \\
Per einfacher Induktion kann man nun zeigen: 
\begin{align}
& n \leq 2^{n} \\
\text{Daraus folgt mit IV:} \quad & n+1 \leq 2^{n} + 2^{n} = 2*2^{n} = 2^{n+1}
\end{align}
\end{proof}
\end{example} 
Die Eigenschaft $f \in O(g)$ induktiv zu zeigen, ist mühsam. Stattdessen können die folgenden Propositionen genutzt werden: 
\begin{note}
\begin{align}
f \in O(f) \\
f \in O(g) \Rightarrow d \cdot f \in O(g) \\
f \in O(n) \land g \in O(n) \Rightarrow f + g \in O(n) \\
f_{1} \in O(g_{1}) \land f_{2} \in O(g_{2}) \Rightarrow f_{1} \cdot f_{2} \in O(g_{1} \cdot g_{2}) \\
f_{1} \in O(g_{1}) \land f_{2} \in O(g_{2}) \Rightarrow \frac{f_{1}}{f_{2}} \in O(\frac{g_{1}}{g_{2}}) \\
f \in O(g) \land g \in O(n) \Rightarrow f \in O(n)
\end{align}
\end{note}
Darüber hinaus gilt folgender Satz: 
\begin{satz}
Seien $f, g: \mathbb{N} \to \mathbb{R_{+}}$. Dann gilt: \\
\[f(n) \in O(g(n)) \iff (\frac{f(n)}{g(n)})_{n \in \mathbb{N}} \text{ ist beschränkt}\]
\end{satz}
Außerdem finden Grenzwertbetrachtungen Eingang:
\begin{lemma}
\begin{align}
f \in O(g) \text{ und } g \in O(f), \text{ wenn } \lim \limits_{n \to \infty} (\frac{f(n)}{g(n)}) = c, c \neq 0 \\
f \in O(g) \text{ und } g \notin O(f), \text{ wenn } \lim \limits_{n \to \infty} (\frac{f(n)}{g(n)}) = 0 \\
f \notin O(g) \text{ und } g \in O(f), \text{ wenn } \lim \limits_{n \to \infty} (\frac{f(n)}{g(n)}) = \infty
\end{align}
\end{lemma}
Neben der $O$-Notation gibt es zwei weitere Notationen, die im Folgenden vorgestellt werden.
\subsection{$\Omega$-Notation}
Neben der Menge aller Funktionen, die höchstens so schnell wächst wie die betrachtete Funktion, bezeichnen wir die Menge der Funktionen, die mindestens so schnell wächst wie die Funktion als $\Omega(n)$.
\begin{definition} Sei $g \in \mathbb{R_{+}}$. Die Menge aller Funktionen, die mindestens so schnell wachsen wie $g$ ist: 
\[\Omega(n) := \{ f \in \mathbb{R_{+}^{\mathbb{N}}} \mid \exists n_{0} \in \mathbb{N}: \exists c \in \mathbb{R_{+}}: \forall n \in \mathbb{N}: (n \geq n_{0} \Rightarrow F(n) \geq c \cdot g(n))\}\]
\end{definition}
Diese Notation fristet ein Schattendasein: In der Regel ist die $O$-Notation geeigneter, da eine Orientierung am {\glqq}Worst-Case-Szenario{\grqq} üblicher ist.
\subsection{$\Theta$-Notation}
Aus $O$-Notation und $\Omega$-Notation ergibt sich nun die Menge der Funktionen, die genau so schnell wachsen wie die betrachtete Funktion. Wir bezeichnen sie mit $\Theta$.
\begin{definition}
\[\Theta(g) = O(g) \cap \Omega(g)\]
\end{definition}
\subsection{Zusammenfassung}
\begin{itemize}
\item Die $O$-Notation umfasst alle Funktionen, die nicht schneller wachsen als $f$. Damit beschreiben wir eine obere Schranke für die Komplexitätsfunktion.
\item Die $\Omega$-Notation umfasst alle Funktionen, die mindestens so schnell wachsen wie $f$. Dies ist eine untere Schranke.
\item Die $\Theta$-Notation umfasst alle Funktionen, die genau so schnell wachsen wie f. Damit haben wir ein asymptotisches Maß.  
\end{itemize}
\begin{lemma}
\begin{itemize}
\item $f \in O(g) \text{ und } g \in O(f), \text{ also } f \in O(g) und f \in \Omega (g), \text{ wenn } \lim \limits_{n \to \infty} \frac{f(n)}{g(n)} = c, c \neq 0, \text{ also: } f \in \Theta (g)$
\item $f \in O(g) \text{ und } g \notin O(f), \text{ wenn } \lim \limits_{n \to \infty} \frac{f(n)}{g(n)} = 0, \text{ also: } g \in \Omega (f) \text{ und } f \notin \Omega (g)$
\item $ f \notin O(g) \text{ und } g \in O(f), \text{ also } g \notin \Omega (f) \text{ und } f \in \Omega (g), \text{ wenn } \lim \limits_{n \to \infty} \frac{f(n)}{g(n)} = \infty$
\end{itemize}
\end{lemma}
\section{Rekursionsgleichungen}
Nun haben wir mit den Landau-Symbolen einen mathematischen Weg gefunden, die Komplexität von Algorithmen zu beschreiben. Im Folgenden wenden wir dieses Modell auf konkrete Algorithmen an. \\\\
Bekannte Algorithmen sind beispielsweise Such- oder Sortierverfahren. Diese sind häufig rekursiv, d.h. eine Iteration oder verarbeitet direkt das Ergebnis einer vorherigen. Dadurch ergeben sich nun sogenannte \textbf{Rekurrenzgleichungen} oder auch \textbf{Rekursionsgleichungen}.
\begin{example}
$T(n) = 2 \cdot T(n-) + 1$ ist eine Rekurrenzgleichung, da der Funktionswert $n$ direkt vom Funktionswert $n-1$ abhängt. Aus den Programmierkonzepten ist uns Rekursion als ein {\glqq}Wiederaufruf von sich selbst{\grqq} bekannt, dieses Muster findet sich hier in unseren betrachteten Algorithmen.
\end{example}
Um die Komplexität eines rekursiven Algorithmus bestimmen zu können, bedienen wir uns je nach Rekursionsgleichung unterschiedlichen Verfahren. Eines davon ist das sogenannte Mastertheorem. 
\subsection{Mastertheorem}
\begin{satz} Sofern alle Teilprobleme die gleiche Größe haben, können wir zwei Fälle unterscheiden: 
\begin{itemize}
\item Basisfall: $f(n)$ ist konstant für hinreichend kleine Werte von $n$
\item Für größere $n$ lässt sich eine Rekursionsgleichung der folgenden Form bestimmen: 
\[f(n) = a \cdot f(\frac{n}{b}) + O(n^{d})\]
$a$ ist hier die Anzahl der rekursiven Aufrufe, $b$ der Verkleinerungsfaktor des Problems und $d$ der Exponent für die Laufzeit der Vorbereitung der Rekursion und/oder der Zusammensetzung der Teilprobleme beschreibt. \\\\
Nun gilt Folgendes: \\
\begin{align*}
f(n) \in 
\begin{cases}
O(n^{d}) \quad & \text{ falls } a < b^{d} \\
O(n^{d} \cdot log_{b}(n)) \quad & \text{ falls } a = b^{d} \\
O(n^{log_{b}(a)}) \quad & \text{ falls } a > b^{d}
\end{cases}
\end{align*}
\end{itemize}
\end{satz}
Mithilfe dieses Theorems kann nun die Komplexität eines rekursiven Algorithmus bestimmt werden:
\begin{example}
Gegegeben sei \[f(n) = 2 \cdot f(\frac{n}{2}) + O(n^{2})\] 
Daraus folgt $ a = 2, b = 2, d = 2$. \\\\
Da $2 < 2^{2}$ trifft der erste Fall ein und wir halten fest: $f(n) \in O(n^{2})$.
\end{example}
Über das Mastertheorem sind eine Zahl an Rekursionsgleichungen entscheidbar, jedoch nicht jede. Für die Rekursionsgleichungen, deren Form nicht der für das Mastertheorem benötigten entspricht, werden andere Verfahren nötig.
\subsection{Auflösungsverfahren}
Eine gegebene Rekursionsgleichung lässt sich in eine geschlossene Form überführen, in der keine Abhängigkeit zu vorherigen Funktionswerten mehr besteht. Dafür existieren zwei Ansätze: 
\begin{itemize}
\item \textbf{bottom-up:} Aus berechneten Funktionswerten für kleine n wird eine weitere Berechnungsvorschrift ermittelt. Diese muss anschließend induktiv bewiesen werden.
\item \textbf{top-down:} Die vorherigen Funktionswerte werden solange durch die entsprechende Gleichung ersetzt, bis der definierte Startwert eingesetzt werden kann. Anschließend wird der Term zusammengefasst und es ergibt sich eine geschlossene Rechenvorschrift. 
\end{itemize}
\begin{example}
Gegeben ist die folgende Rekursionsgleichung: $f(1) = 1, f(n) = 2 \cdot f(n-1) + 1$. \\\\
\textbf{bottom-up} \\
\begin{align}
f(1) = 1 \\
f(2) = 2 \cdot 1 + 1 = 3 \\
f(3) = 2 \cdot 3 + 1 = 7 \\
f(4) = 2 \cdot 7 + 1 = 15
\end{align}
Aus den Ergebnissen wird relativ schnell eine Verwandtschaft zu den ersten Potenzen der Zahl 2 deutlich und so lässt sich nun folgende Gleichung aufstellen: 
\[f(n) = 2^{n} - 1\]
Diese Gleichung kann nun für die gegebenen Werte von $n$ überprüft werden und muss anschließend induktiv bewiesen werden. \\\\
\textbf{top-down} \\
\begin{align}
f(n) =& \quad 2 \cdot f(n-1) + 1 \\
=& \quad 2 \cdot (2 \cdot f(n-2) + 1) + 1 \\
=& \quad 2 \cdot (2 \cdot (... \cdot (2 \cdot 1) + 1) ... ) + 1) + 1 \\
\Rightarrow & \quad 2^{n-1} + 2^{n-2} + ... + 2^{n-(n-1)} + 1 \\
=& \quad \sum_{i = 0}^{n-1} 2^{i} = \frac{2^{(n-1)+1}-1}{2-1} = 2^{n}-1 \in O(2^{n}) 
\end{align}
\end{example}
Je nach Rekursionsgleichung bietet sich mitunter eines der beiden Verfahren mehr an als das andere. Vernachlässigt werden darf beim \textit{bottum-up}-Verfahren nicht, dass ein Induktionsbeweis erforderlich ist! Bei Rekursionsgleichungen mit zweifacher Rekursion (in Beziehung zu den vorherigen zwei Funktionswerten) ist ein \textit{top-down}-Verfahren deutlich komplexer und häufig nicht ratsam.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Veranschaulichen Sie die $O$-, $\Theta$- und $\Omega$-Notation anhand der Funktion $f(n) = 3n^{3} + 7n^{2} + 16$ grafisch. 
\begin{flushright}
Lösung auf Seite \pageref{a1:lsg}
\end{flushright}
\paragraph{Aufgabe 2} Geben Sie zu folgenden Funktionen jeweils die $O$-Notation an.
\begin{enumerate}
\item $f(n) = 6n^{4} + 3 \cdot log_{2}(n)$
\item $f(n) = 6627816n + 13$
\item $f(n) = \frac{72n^{3} + 27n^{2} + 8n + 9}{n!}$
\item $f(n) = 3 \cdot f(n-1) + \frac{f(n-2)}{n^{2}}$
\end{enumerate}
\begin{flushright}
Lösung auf Seite \pageref{a2:lsg}
\end{flushright}
\paragraph{Aufgabe 3}
\begin{enumerate}
\item Zeigen Sie, dass $n^{2} \in O(2n)$.
\item Zeigen Sie, dass $n^{3} \in O(2n)$.
\end{enumerate}
\begin{flushright}
Lösung auf Seite \pageref{a3:lsg} \\
entnommen aus VL
\end{flushright}
\paragraph{Aufgabe 4}
Bestimmen Sie unter Anwendung des Mastertheorems die jeweilige Komplexitätsklasse $O(n)$.
\begin{itemize}
\item $f(n) = 2 \cdot f(\frac{n}{2n+1}) + n^{2}$
\item $g(n) = log(n) \cdot g(\frac{n}{2}) + 3$
\item $h(n) = sin(n) \cdot h(\frac{n}{\frac{n}{2}}) + O(n^{3})$
\end{itemize}
\begin{flushright}
Lösung auf Seite \pageref{a4:lsg} \\
\end{flushright}
\paragraph{Versionshinweis} Weitere Aufgaben zu Rekursionsgleichungen, praktischen Anwendungen auf Pseudo-Code und für $\Theta , \Omega$ folgen in späteren Versionen
\pagebreak
\part{Suchalgorithmen}
Sie studieren Informatik, sind es satt, noch mehr Stunden vor Bildschirmen zu verbringen und suchen sich deshalb ein echtes, physisches Buch aus der Bibliothek Ihrer Hochschule heraus. Sie fragten nach {\glqq}\textsc{Wirth}: Algorithmen und Datenstrukturen{\grqq} von 1976, weil man es Ihnen in Ihrer Vorlesung empfahl, die Bibliothekarin deutet auf ein Regal und lässt sie damit allein. \\\\
Wie finden Sie das gesuchte Buch aus der Regalreihe mit etwa 300 Büchern?
\section{Lineare Suche}
Sie beginnen ganz links. Nun lesen Sie den Titel des ersten Buches, er passt nicht. Sie lesen den Titel des zweiten Buches, er passt ebenfalls nicht. Sie fahren fort, iterieren über das gesamte Bücherregal und brechen ab, falls Sie es gefunden haben oder am Ende angelangt sind. Dieses Verfahren ist die \textbf{Lineare Suche}.
\paragraph{Idee}
Durchlaufe sukzessive das Feld A, bis das gesuchte Element gefunden ist bzw. das Ende des Feldes erreicht ist.
\paragraph{Implementierung}
\begin{lstlisting}
LinearSearch(A[], E) {
	i = 0;
	While i < A[].length {
		If (A[i] == E) {
			return i;			
		}
		i = i + 1;	
	}
	Return ElementNotFound
}
\end{lstlisting}
\paragraph{Komplexität}
Ganz offensichtlich ist der Aufwand dieses Suchverfahrens durch die Feldlänge (also die Anzahl der Bücher in unserem Regal) bestimmt. \\
\textit{günstigster Fall} \quad Das gewünschte Element steht an Indexposition 0 (also dem ersten Element bzw. es ist das erste Buch im Regal) $\to$ ein Vergleich ist erforderlich
\\
\textit{durchschnittlicher Fall} \quad Das gewünschte Element steht an der mittleren Indexposition $\frac{n}{2}$ bei $n$ Feldelementen. $\to \frac{n}{2}$ Vergleiche sind erforderlich
\\
\textit{ungünstigster Fall} \quad Das gewünschte Element ist nicht im Feld vorhanden. $\to n$ Vergleiche
\\\\
Insgesamt ergibt sich -- da wir bei der Angabe der Komplexität durch die $O$-Notation jeweils den ungünstigsten Fall betrachten -- für die Lineare Suche eine Komplexität von $O(n)$. 
\section{Binäre Suche}
Sie befinden sich in der perfekten Bibliothek: Alle Bücher sind aufsteigend nach ihren Autor*innen sortiert! Dieses Wissen können Sie sich zunutze machen, in dem Sie ihr Wunschbuch mit der \textbf{binären Suche} finden: Sie teilen die Bücherreihe in zwei Hälften und betrachten nun das mittlere Buch: Muss Ihr Buch links oder rechts von dem mittleren Buch stehen (Vergleich anhand des Namens der Autoren)? Oder ist es sogar ihr Buch? Nein, das ist es nicht und der Autor Ihres Buches hat einen im Alphabet weiter hinten stehenden Anfangsbuchstaben, also betrachten Sie die Hälfte rechts des mittleren Buches. Diese Hälfte teilen Sie nun erneut, betrachten die neue Mitte und fahren fort, bis ihre Hälften aus einem Buch bestehen.
\paragraph{Idee} Vergleiche zu suchendes Element $E$ mit der Mitte des Suchbereiches $A[m]$. Falls gefunden: Abbruch. Falls $E < A[m]$: Suche weiter in linker Hälfte. Falls $E > A[m]$: Suche weiter in rechter Hälfte. Das Weitersuchen erfolgt durch einen rekursiven Aufruf des Algorithmus mit der entsprechenden Hälfte des Feldes.
\paragraph{Implementierung}
\begin{lstlisting}
BinarySearch(A[], E, indexLeft, indexRight) {
	if (indexLeft > indexRight) {
		return ElementNotFound	
	}
	else {
		mid = (indexLeft+indexRight) / 2;
		if (E == A[mid]) {
			return mid;		
		}
		else if (E < A[mid] {
			return BinarySearch(A[], E, indexLeft, mid-1);	
		}
		else {
			return BinarySearch(A[], E, mid+1, indexRight);	
		}
	}
}
\end{lstlisting} 
Die Parameter \texttt{indexLeft} und \texttt{indexRight} schränken jeweils das zu betrachtende Feld ein. Zu Beginn ist \texttt{indexLeft} mit $0$ und \texttt{indexRight} mit \texttt{A[].length()} initialisiert. 
\paragraph{Komplexität}
Hier handelt es sich um ein rekursives Verfahren, folglich werden wir eine Rekursionsgleichung erhalten und diese mit den bekannten Verfahren auswerten. Doch zunächst betrachten wir wieder Fälle: \\
\textit{günstigster Fall}: \quad E ist das mittlere Element $\to$ 1 Vergleich\\
\textit{ungünstigster Fall}: \quad E ist nicht im Feld \\
Die Rekursionsgleichung mit den Parametern $a, b \text{und} d$ ergibt sich nun wie folgt:
\begin{itemize}
\item Es gibt einen rekursiven Aufruf, d.h. $a = 1$.
\item Pro Rekursionsaufruf halbieren wir das Feld, daraus folgt $b = 2$.
\item Die Vorbereitung der Rekursion verläuft konstant und ist minimal, also $d = 1$.
\end{itemize}
Die Rekursionsgleichung lautet nun: 
\[f(n) = 1 \cdot f(\frac{n}{2}) + 1\]
Für die Anwendung des Mastertheorems formen wir um:
\[\iff f(n) = 1 \cdot f(\frac{n}{2}) + O(n^{0})\]
Nun ergibt sich: 
\[a = b^{d} \iff 1 = 2^{0} \Rightarrow f(n) \in O(n^{0} \cdot log_{2}(n)) = O(log(n))\]
\section{Textsuche}
Spätestens jetzt sollten Sie das Buch gefunden haben, wenn Sie die binäre Suche verwendet haben, vermutlich schneller als mit der linearen Suche. Jetzt haben Sie einen Arbeitsplatz gefunden, setzen sich und schlagen das Buch auf: Für ein Lesen in Ruhe haben Sie keine Zeit, daher suchen Sie sich die interessanten Teile heraus. Sie suchen, wo \textsc{Niklaus Wirth} Ihnen die $O$-Notation erklären wird. \\\\
Formal suchen Sie folglich alle Vorkommen eines Musters (in Form einer Zeichenkette) in einem Text. Dafür betrachten wir zwei Verfahren.
\subsection{Einfache Textsuche}
Analog zur linearen Suche beginnen Sie einfach, Ihren gesuchten Begriff unter den Text zu legen. Stimmt der erste Buchstabe überein, vergleichen Sie den zweiten. Stellen Sie eine Differenz fest, verschieben Sie Ihr Suchwort um eine Position nach rechts. Derart fahren Sie fort, bis Sie am Ende des Textes angelangt sind.
\paragraph{Idee}
Beginnend beim ersten Zeichen des Textes legt man das Muster der Reihe nach an jede Stelle des Textes an und vergleicht zeichenweise von links nach rechts, ob eine Übereinstimmung vorliegt
\paragraph{Implementierung}
\begin{lstlisting}
int BruteForceSearch(char[] text, char[] pattern) {
	int n = text.length;
	int m = pattern.length;
	int i = 0;
	int j = 0;
	for (int k = 0; i <= n-m; i++) {
		j = 0;
		while (j < m && Text[i+j] == pattern[j]) {		
			j++;		
		}
		if (j == m) {
			return i;		
		}
	}
	return ElementNotFound;
}
\end{lstlisting}
\paragraph{Komplexität}
Offenbar hängt der Aufwand dieses Verfahrens ganz wesentlich davon ab, wie lang das Muster und der Text in Gänze sind. Weniger relevant hingegen ist der Aufbau des Textes, da alle Vorkommen des Musters gesucht werden. Insofern ist es nicht entscheidend, an welcher Stelle das Muster das erste Mal auftritt. \\\\
\textit{günstigster Fall} \quad Im günstigsten Fall werden m Vergleiche benötigt, wenn nämlich die Länge des Textes kleiner als das doppelte der Länge des Musters $m$ ist und das Muster direkt am Beginn des Textes steht. $\to m$ Vergleiche 
\\\\ \textit{ungünstigster Fall}\quad Im ungünstigsten Fall werden $(n-m+1) \cdot m$ Vergleiche benötigt, wobei dieser Fall allgemeingültig ist -- es werden alle Vorkommen des Musters im Text gesucht und nicht lediglich das erste. \\\\
Die Laufzeit des Algorithmus liegt nun in $O(n \cdot m)$.
\subsection{Knuth-Morris-Pratt-Algorithmus (KMP)}
Ihnen ist das zu mühsam geworden, Zeichen für Zeichen des Textes und des Musters zu vergleichen. Sie sehnen sich nach einem intelligenteren, einfacheren Weg. Und, wahrlich den gibt es: Der \textbf{Knuth-Morris-Pratt}-Suchalogorithmus nach \textsc{Donald Ervin Knuth}, \textsc{James Hiram Morris} und \textsc{Vaughan Ronald Pratt}. \\\\
Die Grundlage für diesen Algorithmus bildet die vorangegangene einfache Textsuche. Diese optimieren wir nun, in dem wir beim ersten sich unterscheidenden Zeichen von Text und Muster das Muster nicht um ein einziges Zeichen nach rechts verschieben, sondern um eine bestimmte Anzahl an Zeichen, die sich aus unserem Muster ergibt und als \textbf{Rand} bezeichnet wird.
\begin{definition}
Der \textbf{Rand} eines Musters $M$ mit der Länge $m$ ist eine tabellarische Auflistung aller Teilzeichenketten der Längen $0$ bis $m$ des Musters, für die jeweils die Anzahl der Zeichen eines Prefixes, das zugleich Postfix ist, bestimmt wird.
\end{definition}
\begin{example}
Betrachten wir das Wort \texttt{abbababbab}. \\
Wir beginnen eine tabellarische Auflistung: \\\\
\begin{tabular}{|l|c|c|l|}
\hline
Teilzeichenkette & Länge & Rand & Bemerkung \\
\hline
& 0 & -1 & per Definition, für Teilschritt 2 (Suche) erforderlich \\
\texttt{a} & 1 & 0 & per Definition \\ 
\texttt{ab} & 2 & 0 & $a \neq b$ \\
\texttt{abb} & 3 & 0 & \\
\texttt{abba} & 4 & 0 & $ab \neq ba$, Palindrome haben den Rand 0 \\
\texttt{abbab} & 5 & 2 & gleich: $ab$, Länge: 2 \\
\texttt{abbaba} & 6 & 1 & gleich: a, Länge: 1 \\
\texttt{abbabab} & 7 & 2 & gleich: ab, Länge: 2 \\
\texttt{abbababb} & 8 & 3 & gleich: abb, Länge: 3 \\
\texttt{abbababba} & 9 & 4 & gleich: abba, Länge: 4 \\
\texttt{abbababbab} & 10 & 5 & gleich: abbab, Länge: 5 \\
\hline 
\end{tabular} \\\\
Im folgenden Suchalgorithmus wird jeweils der Rand verwendet, um Verschiebungspositionen zu bestimmen. Daher ist die gesamte Tabelle erforderlich.
\end{example} \pagebreak
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
Rand[0] = -1; 		/* Definition */
Rand[1] = 0;		/* Definition */

for (j = 2; j <= m; j++) {
	while ((i >= 0) and (M[i] != M[j-1])) {
		i = Rand[i];
		i++;
	}
	Rand[j] = i;
}
\end{lstlisting}
Folglich iterieren wir nun über alle möglichen Teilzeichenkettenlängen (\texttt{j}) und vergleichen jeweils mit dem vorherigen Rand (durch \texttt{i}). Wenn unser Rand maximal ist, verlassen wir die \texttt{while}-Schleife und speichern den ermittelten Rand. \\\\
Eine Veranschaulichung der algorithmischen Rand-Bestimmung finden Sie in der Wikipedia. \parencite{KMPRand}
\paragraph{Der Suchalgorithmus}
Ähnlich wie bei der einfachen Textsuche beginnen Sie, indem Sie das zu suchende Muster unter den Suchraum schreiben und vergleichen Zeichen für Zeichen. Entscheidend ist nun die erste Abweichung: Sie bestimmen den \texttt{Rand[]} der Indexposition der Abweichung im Muster und verschieben ihr Muster nun mit unterer Formel. So nutzen Sie aus, dass bis zur ersten Abweichung Gleichheit bestand -- falls bei vier gleichen Zeichen ein Rand von zwei vorliegt, können Sie ihr Muster so verschieben, dass der Beginn nun dort liegt, wo zuvor das mit dem neuen Anfang identische Enge lag. Nun fahren Sie mit den neu gewonnen Index-Positionen fort, die sich aus dem Verschiebung ergeben haben. \\\\
Die Formel für Verschiebungen ist: \\
\texttt{Neuer Beginn des Musters (neue Suchtextposition) = Suchtextposition (aktueller Beginn des Musters) + (Anzahl übereinstimmender Zeichen -- Randlänge)}
\begin{example}
Sie suchen das Muster \texttt{ababaab} im Text \texttt{baabbaababaabbbabba}. \\
\textbf{Schritt 1}: Bestimmung des Randes \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|}
 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
\hline
-1 & 0 & 0 & 1 & 2 & 1 & 1 & 2 \\
\end{tabular} \\\\
\textbf{Schritt 2}: Suche \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0 & 1 & 2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
a&b&a&b&a&a&b& & & & & & & & & & & & \\
\hline
\end{tabular} \\
Erste Abweichung an Index 0 $\to$ Rand[0] = -1, nach Formel oben: $0+(0-(-1)) = 1$. Also verschieben wir das Muster an Indexposition 1. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 &a&b&a&b&a&a&b& & & & & & & & & & &  \\
\hline
\end{tabular} \\
Abweichung an Index 2, Rand[2] = 0, mit Formel: $1+(1-0) = 2$. Wir verschieben also so, dass das erste Zeichen des Musters an Index 2 steht.  \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & &a&b&a&b&a&a&b& & & & & & & & & &   \\
\hline
\end{tabular}\\
Abweichung an Index 4, Rand[2] = 0, mit Formel: $2+(2-0) = 4$. Neue Indexposition des Musters: 4. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & &a&b&a&b&a&a&b& & & & & & & &    \\
\hline
\end{tabular}\\
Abweichung an Index 4, Rand[0] = -1, mit Formel: $4+(0-(-1)) = 5$. Neue Indexposition: 5. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & & &a&b&a&b&a&a&b& & & & & & &    \\
\hline
\end{tabular}\\
Abweichung an Index 6, Rand[1] = 0, mit Formel: $5+(1-0) = 6$. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & & & &a&b&a&b&a&a&b& & & & & &    \\
\hline
\end{tabular}\\
Nun tritt keine Abweichung ein, das erste Vorkommen des Musters im Suchtext ist gefunden. Da wir nun an allen Vorkommen interessiert sind, führen wir unsere Suche fort. Dafür bestimmen wir zunächst den Rand[7] = 2. Mit der Formel folgt: $6+(7-2) = 11$. An Position 11 können wir nun unsere Suche fortsetzen.\\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & & & & & & & & &a&b&a&b&a&a&b&    \\
\hline
\end{tabular}\\
$11+(2-0) = 13$. Damit verlassen wir den Suchtext, der Algorithmus terminiert, da kein weiteres Vorkommen gefunden wurde.
\end{example}
Die Stärken des KMP-Algorithmus liegen folglich in der Berücksichtigung von Rändern an Anfang und Ende des Musters, die größere Sprünge als eins (im Vergleich zur einfachen Textsuche) ermöglichen. Liegt nun ein Muster ohne jegliche oder mit wenigen Rändern vor, ist der Laufzeitgewinn marginal.
\paragraph{Implementierung}
als Pseudo-Code
\begin{lstlisting}
KnuthMorrisPrattAlgorithm(Pattern, SearchSpace, Borders) {
	int i = 0;	/* Current position in search space */
	int j = 0;	/* Current position in pattern */

	while (i < SearchSpace.length) {
		// Move pattern until first character of 
		// SearchSpace and Pattern is equal
		while (j >= 0 && SearchSpace[i] != Pattern[j]) {
			j = Borders[j]		
		} 
		
		// Now compare the next character
		i = i + 1;
		j = j + 1;

		// End of pattern: return result		
		if (j == (Pattern.length-1)) {
			print(i - (Pattern.length-1));		
		}
	}
}
\end{lstlisting}
Wie ersichtlich ist, beginnen wir damit, unser Muster unter Anwendung der Ränder an die erste Position zu verschieben, an der das erste Zeichen übereinstimmt. Nun inkrementieren wir beide Zählervariablen und betrachten folgende Zeichen, in dem wir im nächsten Schleifendurchlauf bei Abweichungen weiterverschieben, bis die Länge des Musters erreicht wurde. Dann geben wir diese Indexposition aus und fahren fort. 
\paragraph{Komplexität} 
Da dieser Algorithmus aus zwei Teilproblemen besteht, zerfällt auch die Komplexität: \\
Das Bestimmen der Ränder erfolgt linear für das gesamte Muster, daraus folgt in \textsc{Landau}-Notation: $\text{Rand}(x) \in O(n)$ bei $n$ als Länge des Musters.
Der Suchalgorithmus selbst verläuft ebenfalls linear -- insgesamt erfolgen maximal so viele Durchläufe wie der Text Zeichen enthält. Daraus folgt: $\text{KMPSearch}(x, y, R[]) \in O(m)$ \\
Da beide Teilprobleme getrennt voneinander und aufeinander aufbauend durchlaufen werden, ergibt sich eine Komplexität des gesamten Algorithmus von $O(n + m)$. \parencite{KMPRand}
\subsection{Boyer-Moore-Algorithmus}
Ähnlich dem Ansatz von \textsc{Knuth}, \textsc{Morris} und \textsc{Pratt} haben auch \textsc{Robert S. Boyer} und \textsc{J Strother Moore} einen vergleichbaren Algorithmus zur effizienteren Suche von Zeichenketten in Texten entwickelt. Im Gegensatz zum KMP-Algorithmus werden dort keine Ränder bestimmt, sondern erst im Falle einer Abweichung greifen zwei Heuristiken: Die \textbf{Bad-character}- und \textbf{Good-suffix}-Heuristik, mithilfe derer ebenfalls Sprünge ermittelt werden. Dieses Verfahren hat eine Komplexität von $O(n \cdot m)$ und ist damit im Allgemeinen (dem ungünstigsten Fall) weniger effizient als der KMP. Jedoch verläuft dieses Algorithmus nicht linear, sodass in einem günstigen Fall eine geringere Laufzeit erreicht werden kann. \\\\

Dieses Verfahren wird hier nicht näher dargestellt, da es nicht Teil der Vorlesung war. Weitere Informationen finden Sie bei \parencite[][S. 476 -- 478]{Grundkurs}.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Führen Sie eine binäre Suche nach dem Element \texttt{7} in \texttt{[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 17, 18, 19, 22]} durch. Notieren Sie jeweils Ihre Zwischenergebnisse. 
\begin{flushright}
Lösung auf Seite \pageref{a2.1:lsg}
\end{flushright}
\paragraph{Aufgabe 2} Bestimmen Sie die Ränder der folgenden Muster:
\begin{itemize}
\item \texttt{OTTOSMOPSKOTZT}
\item \texttt{ANANASBANANA}
\item \texttt{BONOBO}
\end{itemize}
Geben Sie jeweils an, wie sehr sich der Einsatz des KMP-Algorithmus bei diesen Mustern lohnt.
\begin{flushright}
Lösung auf Seite \pageref{a2.2:lsg}
\end{flushright}
\paragraph{Aufgabe 3} Wenden Sie den KMP-Algorithmus mit dem Muster \texttt{BONOBO} auf den Suchtext \texttt{BEINAHEBOTNOCHDASBONOBONOBOHAUSPLATZ} an. Stellen Sie Ihre Zwischenergebnisse dar.
\begin{flushright}
Lösung auf Seite \pageref{a2.3:lsg}
\end{flushright}
\paragraph{Aufgabe 4} Erläutern Sie den Algorithmus der binären Suche in Ihren eigenen Worten.
\begin{flushright}
Lösung auf Seite \pageref{a2.4:lsg}
\end{flushright}
\pagebreak
\part{Sortieralgorithmen}
Nach langen, qualvollen Stunden vor dem Bücherregal hassen Sie ihr Leben, ihr Studium und Sie beschließen, es wegzuwerfen -- Sie fangen in der Bibliothek als Fachkraft für Lagerlogistik an und sind nun für die korrekte Anordnung der Bücher im Regal verantwortlich. Auch hier zeigen Sie ihre geistige Brillianz, indem Sie das Problem des Sortierens auf abstrakter Ebene betrachten.
\section{Allgemeines und Eigenschaften}
\begin{definition}
Ein Sortieralgorithmus verarbeitet eine Folge von Elementen $e_{1}, ..., e_{n}$ so, dass am Ende eine Ausgabe $e_{1}, ..., e_{n}$ mit $e_{1} \leq e_{2} \leq ... \leq e_{n}$ erzeugt wird.
\end{definition} 
Als zugrunde liegende Datenstrukturen kommen Arrays, verkettete Listen oder weitere, spezielle Datenstrukturen mit zu Mengen/Relationen ähnlichem Aufbau infrage. \\\\
Da Sie bei Sortiervorgängen häufig Daten im \textit{{key: value}}-Format betrachten, unterscheiden wir zwischen \textbf{Schlüsseldaten}(also den keys) und \textbf{Satellitendaten}(den zugehörigen Werten). So können wie beispielsweise Wetterdaten betrachten: [{08.05.2021: 31.5}, {09.05.2021, 30.2}, {07.05.2021, 16.3}] -- Hier sind die Tage jeweils die Schlüsseldaten und die Temperaturangaben (die Gleitkommazahlen) jeweils Satellitendaten.
\begin{definition} Ein Algorithmus sortiert Daten \textbf{stabil}, falls die Reihenfolge bereits sortierter Elemente mit gleichem Schlüssel nicht geändert wird und so insbesondere nach einem weiteren Kriterium sortierte Daten nach Ausführung des Algorithmus in diesem Kriterium ihre Sortierung beibehalten. Als \textbf{instabil} bezeichnen wir einen Sortieralgorithmus, falls das Gegenteil der Fall ist.
\end{definition}
Ähnlich den Suchalgorithmen unterscheiden sich auch Sortieralgorithmen in weiteren Aspekten: 
\begin{itemize}
\item Laufzeiteffizienz ($\to$ \textsc{Landau}-Notation)
\item Speicherplatzeffizienz
\item Ablaufschema: Rekursiv oder Iterativ
\item Stabilität der Daten
\end{itemize}
Für die Speicherplatzeffizienz ergibt sich folgende Unterscheidung:
\begin{definition}
Falls ein Sortieralgorithmus parallel zur Laufzeit keinen oder einen konstanten zusätzlichen Speicherbedarf benötigt, so wird er \textbf{in-place} ausgeführt. Ein Algorithmus, der hingegen zusätzlichen Speicherbedarf in Abhängigkeit zu den Daten benötigt, wird als \textbf{out-place} bezeichnet.
\end{definition}
\section{Insertion Sort}
Der erste Sortieralgorithmus, den wir betrachten, heißt \textbf{Insertion Sort}. Wie der Name schon andeutet, geht es darum, Elemente einzufügen -- in einen bestehenden, bereits sortierten Teil. \\\\
Sie beginnen links in Ihrer Bücherreihe, starten mit dem zweiten Buch. Sie prüfen nun, ob es im Vergleich zum ersten richtig steht, andernfalls tauschen sie es. Nun betrachten Sie das dritte Buch und fügen es vor/zwischen/nach Buch 1 und 2 ein, je nach richtiger Position. So fahren Sie fort: Sie fügen das jeweils aktuelle Buch in die Reihe der bereits sortierten ein.
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
InsertionSort(Array) {
	int i = 0;
	while (i < (Array.length-1)) {
		x = Array[i];
		int y = 0;
		while (x < Array[i-y]) {
			y++;		
		} 
		Array[i-y] = x;
		i++;
	}
}
\end{lstlisting}
\begin{example} Zu sortieren Sie das Array \texttt{[44, 55, 12, 42, 94, 18, 06, 67]}. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
Ausgangswerte & 44 & \uline{55} & 12 & 42 & 94 & 18 & 06 & 67 \\
nach i = 1    & 44 & 55 & \uline{12} & 42 & 94 & 18 & 06 & 67 \\
nach i = 2    & 12 & 44 & 55 & \uline{42} & 94 & 18 & 06 & 67 \\
nach i = 3    & 12 & 42 & 44 & 55 & \uline{94} & 18 & 06 & 67 \\
nach i = 4    & 12 & 42 & 44 & 55 & 94 & \uline{18} & 06 & 67 \\
nach i = 5    & 12 & 18 & 42 & 44 & 55 & 94 & \uline{06} & 67 \\
nach i = 6    & 06 & 12 & 18 & 42 & 44 & 55 & 94 & \uline{67} \\
nach i = 7    & 06 & 12 & 18 & 42 & 44 & 55 & 67 & 94 \\
\hline
\end{tabular} \\
Quelle: \parencite[][S. 493 ff.]{Grundkurs}
\end{example}
\paragraph{Komplexität}
Im günstigsten Fall operiert dieses Verfahren auf einem bereits sortierten Feld. Dann sind -- um dies festzustellen -- $n-1$ Vergleiche erforderlich. Im schlechtesten Fall ist das Feld in genau falscher Richtung sortiert: Dann sind $\frac{n^{2}}{2}$ Vergleiche erforderlich, da die zweite Schleife auch maximal oft durchlaufen wird. \\\\
Insgesamt ergibt sich in \textsc{Landau}-Notation: $\in O(n^{2})$
\paragraph{Eigenschaften}
\begin{itemize}
\item Das Verfahren ist iterativ (erkennbar an den beiden Schleifen)
\item Es ist stabil, da Vertauschungen nur im notwendigen Fall auftreten
\item Das Verfahren operiert in-place, da kein bzw. nur konstanter zusätzlicher Speicher benötigt wird
\end{itemize}

Optimieren ließe sich das Verfahren durch den Einsatz von binärer Suche für die Einfügeposition -- dann reduzieren sich die Vergleiche, nicht jedoch die Verschiebungen.
\section{Bubblesort}
Mit \textbf{Bubble Sort} lernen wir einen weiteren Algorithmus zum Sortieren kennen: Hier vergleichen wir zunächst benachbarte Elemente und tauschen diese bei Bedarf. So durchlaufen wir das gesamte Array, bis keine Vertauschungen mehr notwendig sind und alle Elemente in der korrekten Reihenfolge angeordnet sind.
\paragraph{Idee} Durchlaufe Feld mehrmals und tausche benachbarte Elemente bei Bedarf. Terminiere, wenn in einem Durchlauf keine Vertauschung mehr erfolgt ist.
\begin{example}
Zu sortieren ist das Feld \texttt{[6, 8, 2, 9, 1, 5, 4, 3]}. \\
6 $<$ 8, aber 8 $>$ 2: \texttt{[6, 2, 8, 9, 1, 5, 4, 3]}. \\
8 $<$ 9, aber 9 $>$ 1: \texttt{[6, 2, 8, 1, 9, 5, 4, 3]} \\
9 $>$ 5: \texttt{[6, 2, 8, 1, 5, 9, 4, 3]} \\
9 $>$ 4: \texttt{[6, 2, 8, 1, 5, 4, 9, 3]} \\
9 $>$ 3: \texttt{[6, 2, 8, 1, 5, 4, 3, 9]} \\\\
Nächste Iteration: \\
6 $>$ 2: \texttt{[2, 6, 8, 1, 5, 4, 3, 9]} \\
6 $<$ 8, aber 8 $>$ 1: \texttt{[2, 6, 1, 8, 5, 4, 3, 9]} \\
8 $>$ 5: \texttt{[2, 6, 1, 5, 8, 4, 3, 9]} \\
8 $>$ 4: \texttt{[2, 6, 1, 5, 4, 8, 3, 9]} \\
8 $>$ 3: \texttt{[2, 6, 1, 5, 4, 3, 8, 9]} \\
8 $<$ 9. \\\\
Nächste Iteration:\\
2 $<$ 6, aber 6 $>$ 1: \texttt{[2, 1, 6, 5, 4, 3, 8, 9]} \\
6 $>$ 5: \texttt{[2, 1, 5, 6, 4, 3, 8, 9]} \\
6 $>$ 4: \texttt{[2, 1, 5, 4, 6, 3, 8, 9]} \\
6 $>$ 3: \texttt{[2, 1, 5, 4, 3, 6, 8, 9]} \\
6 $<$ 8 und 8 $<$ 9. \\\\
Nächste Iteration:\\
2 $>$ 1: \texttt{[1, 2, 5, 4, 3, 6, 8, 9]} \\
2 $<$ 5, aber 5 $>$ 4: \texttt{[1, 2, 4, 5, 3, 6, 8, 9]} \\
5 $>$ 3: \texttt{[1, 2, 4, 3, 5, 6, 8, 9]} \\
5 $<$ 6, 6 $<$ 8 und 8 $<$ 9. \\\\
Nächste Iteration: \\
1 $<$ 2, 2 $<$ 4, aber 4 $>$ 3: \texttt{[1, 2, 3, 4, 5, 6, 8, 9]} \\
4 $<$ 5, 5 $<$ 6, 6 $<$ 8 und 8 $<$ 9. Terminiert.
\end{example}
Aus dem Beispiel können wir ableiten, dass in jeder Iteration $n-1$ Vergleiche erfolgen. Dies ist ein Indiz für die Komplexität, die wir später betrachten möchten.
\paragraph{Implementation} als Pseudo-Code
\begin{lstlisting}
BubbleSort(Array) {
	int i = 0;
	int j = 0;
	bool SwapFlag = True;
	while (SwapFlag == true) {
		SwapFlag = false;
		while (i < (Array.length-1) {	
			if (Array[i] > Array[i+1]) {
				swap(Array[i], Array[i+1]);
				SwapFlag = true;			
			}
		}	
	}
}
\end{lstlisting}
Die \texttt{SwapFlag} ist Terminationskriterium: Falls in einer Iteration kein Tausch mehr durchgeführt wurde, ist das Array vollständig sortiert.
\paragraph{Komplexität}
Im günstigsten Fall ist das Array bereits vollständig sortiert, dann werden $n-1$ Vergleiche notwendig. Im ungünstigsten Fall ist das Aray absteigend sortiert, nun sind $\frac{n^{2}}{2}$ Vergleiche erforderlich. \\\\
Somit ergibt sich erneut eine Komplexität von $O(n^{2})$. \\\\
Im Gegensatz zu ursprünglichen Implementierungen haben wir hier bereits eine Optimierung vorgenommen: Die SwapFlag bewirkt, dass im günstigsten Fall $O(n)$ die passende Komplexitätsklasse ist.
\paragraph{Eigenschaften}
\begin{itemize}
\item Auch Bubble Sort arbeitet offensichtlich iterativ.
\item Bubble Sort ist ebenfalls stabil.
\item Der Speicheraufwand ist in-place.
\end{itemize}
Quelle und weiterführende Literatur: \parencite[][S.86 ff.]{Wirth} \parencite[][S.22]{Taschenbuch}
\section{Mergesort}
Die nächste Idee, wie Sie Ihr Bücherregal sortieren können: Mergesort! Sie teilen das Regal in zwei Hälften, sortieren jede Hälfte, in dem Sie diese wiederum halbieren, bis die Hälften aus einem Buch bestehen. Sortieren Sie diesen Teil, indem Sie die Bücher richtig anordnen und die Hälften so wieder von klein nach groß nach einer Art Reißverschlussprinzip verbinden, bis Sie die Gesamtheit der Elemente in sortierter Reihenfolge erhalten. 
\begin{example} Zu sortieren: \texttt{[1, 7, 6, 2, 9, 4, 3, 8, 10, 5]}.\\
Halbieren: \\\texttt{[1, 7, 6, 2, 9] | [4, 3, 8, 10, 5]} \\
\texttt{[1, 7, 6][2, 9] | [4, 3, 8][10, 5]} \\
\texttt{[1, 7][6][2][9] | [4, 3] [8][10][5]} \\
\texttt{[1][7][6][2][9] | [4][3][8][10][5]}\\\\
Sortieren (nach rekursiven Aufrufen): \\
\texttt{[1][2][6][7][9] | [3][4][5][8][10]} \\\\
Merge: \\
\texttt{[1][2][3][4][5][6][7][8][9][10]}
\end{example}
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
MergeSort(Array) {
	if (Array.length <= 1) {
		return Array  ;
  	}
  	else {
		leftHalf = MergeSort(Array[0:(Array.length-2)]);
		rightHalf = MergeSort(Array[(Array.length-a):-1]);
		return merge(leftHalf, rightHalf);
	}
}
merge(A[], B[], al, ar, bl, br, C[]) {
	int i = al;
	int j = bl;
	for (int k = 0; k <= ar-al+br-bl+1; k++) {
		if (i > ar) {
			C[k] = B[j+1];
			continue;		
		}
		if (j > br) {
			C[k] = A[i+1];
			continue;		
		}
		C[k] = (A[i] < B[j] ? A[i++] : B[j++];	
	}	
}
\end{lstlisting} Quelle: \parencite[][S. 28]{Taschenbuch}
\paragraph{Komplexität}
Für Mergesort ergibt sich -- da es sich um einen rekursiven Algorithmus handelt -- eine Rekursionsgleichung:
\[T(n) = T(\frac{n}{2}) + T(\frac{n}{2}) + n = 2T(\frac{n}{2}) + n\]
Jeder Rekursionsteil beschreibt die Komplexität der einzelnen Hälften und $n$ das anschließende Zusammenfügen. Somit ergibt sich unter Anwendung des Mastertheorems eine Komplexität von 
\[T(n) \in O(n \cdot log(n))\]
\paragraph{Eigenschaften}
\begin{itemize}
\item rekursiv
\item stabil
\item Je nach Implementierung in-place oder out-place
\end{itemize}
\section{Quicksort}
Willkommen in der Königksklasse: Quicksort trägt schon einen bezeichnenden, etwas anmaßenden Titel. Wir betrachten nun, ob der Algorithmus dem gerecht wird.
\paragraph{Idee}
Wir sortieren eine Menge an Elementen, in dem wir das erste Element $p$ (das wir als \textbf{Pivot} bezeichnen) herausgreifen und nun unser Feld/Array so verschieben, dass alle kleineren Elemente ${...} < p$ links von $p$ stehen und alle größeren rechts davon - wohlgemerkt noch nicht sortiert. Nun betrachten wir die neu gebildeten Abschnitte $(a_{0}, ..., a_{p-1})$ und $a_{p+1}, ..., a_{n})$ und verfahren auf diesen Teilstapeln oder Teilmengen rekursiv. Folglich betrachten wir erneut die jeweils ersten Elemente und verfahren ebenso. Da wir keinen die Operationen in-place ausführen, ist kein merge-Schritt notwendig.
\begin{example} Wir betrachten ein Feld \texttt{[8, 17, 28, 15, 11, 1, 3, 20, 25, 6, 12, 5]}. \\
Das erste Element ist folglich 8, also erhalten wir: \\
\texttt{[1, 3, 6, 5, 8, 17, 28, 15, 11, 20, 25, 12]}.\\
Wir betrachten nun die beiden Teilmengen \texttt{[1, 3, 6, 5]} und \texttt{[17, 28, 15, 11, 20, 25, 12]}. \\
Da 1 offensichtlich kleinstes Element der ersten Teilmenge ist, 3 anschließend ebenfalls beim nächsten Aufruf, erhalten wir schnell \texttt{[1, 3, 5, 6]}. \\\\
In der zweiten Menge erhalten wir \texttt{[15, 11, 12]} und \texttt{[28, 20, 25]}, die wir schnell zu \texttt{[11, 12, 15]} und \texttt{[20, 25, 28]}, zusammen \texttt{[11, 12, 15, 17, 20, 25, 28]}. \\\\
Abschließend erhalten wir das sortierte Feld \texttt{[1, 3, 5, 6, 8, 11, 12, 15, 17, 20, 25, 28]}.
\end{example}
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
QuickSort(Array, lo, hi) {
	if (lo < hi) {
		pivot = Array[lo];
		i = lo;
		j = hi+1;	
	}
	while (true) {
		while (Array[++i] < pivot && i < hi) {
					
		}
		while (Array[--j] > pivot && j > lo) {
		
		}
		if (i < j) {
			swap(Array, i, j);		
		}	
		else {
			break;		
		}
	}
	swap(Array, j, lo);
	
	quickSort(Array, lo, j-1);
	quickSort(Array, j+1, hi);
}
swap(Array, i, j) {
	t = Array[i];
	Array[i] = Array[j];
	Array[j] = t;
}
\end{lstlisting} abgewandelt nach \parencite[][S. 29]{Taschenbuch}
\paragraph{Komplexität}
Offensichtlich handelt es sich bei Quicksort ebenfalls um einen rekursiven Sortieralgorithmus. Im günstigsten Fall ergibt sich eine Komplexität von $O(n \cdot log(n))$, da dann das Pivot-Element genau der Median ist und so beide Hälften glatt rekursiv abgearbeitet werden können. Im schlechtesten Fall beträgt die Komplexität $O(n^{2})$, da dann das Pivot-Element jeweils das kleinste oder größte ist. \\\\
Wir geben die Komplexität von Quicksort im Durchschnittsfall mit $O(n \cdot log(n))$ an, behalten allerdings im Kopf, dass in einem ungünstigen Fall eine deutlich schlechtere Laufzeit in Kauf genommen werden muss.
\paragraph{Eigenschaften}
\begin{itemize}
\item rekursiv
\item nicht stabil
\item in-place, da kein Hilfsfeld benötigt wird
\item optimierbar beispielsweise durch Einfügen einer Schranke, bis zu der Insertion Sort genutzt wird (z.B. bei einer geringen zweistelligen Anzahl von Elementen) oder durch 3-Way-Partitioning, bei dem neben $<, >$ auch der Fall $=$ betrachtet wird. Dies ist logischerweise nur dann sinnvoll, falls es mehrere Elemente mit gleichen Schlüsseln gibt und diese bevorzugt als Pivot-Element genutzt werden.
\end{itemize}
\section{Weitere Sortieralgorithmen}
Auf weitere Sortieralgorithmen wie z.B. Selection Sort \parencite{Taschenbuch}, Shellsort oder Shakesort \parencite{Wirth} und \parencite{Grundkurs} kann hier nicht näher eingegangen werden. In genannten Büchern finden Sie nähere Informationen zu diesen Algorithmen.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Sortieren Sie das Feld \texttt{[56, 10, 15, 98, 99, 12, 30, 80]} aufsteigend. Notieren Sie Zwischenschritte
\begin{itemize}
\item Insertion Sort
\item Bubblesort
\item Quicksort
\item Mergesort
\end{itemize}
\begin{flushright}
Lösung auf Seite \pageref{a3.1:lsg} \\
entnommen aus \parencite{GrUeb}
\end{flushright}
\paragraph{Aufgabe 2} 
Beschreiben Sie Quicksort mit eigenen Worten.
\begin{flushright}
Lösung auf Seite \pageref{a3.2:lsg}
\end{flushright}
\paragraph{Aufgabe 3} 
Begründen Sie, mit welchen Verfahren Sie folgende Felder in optimaler Laufzeit aufsteigend sortieren können.
\begin{itemize}
\item \texttt{[1, 9, 2, 8, 3, 7, 4, 6, 5]}
\item \texttt{[9, 7, 7, 2, 5, 4, 7, 3, 1]}
\item \texttt{[9, 8, 7, 6, 5, 4, 3, 2, 1]}
\end{itemize}
\begin{flushright}
Lösung auf Seite \pageref{a3.3:lsg}
\end{flushright}
\paragraph{Aufgabe 4}
Entwickeln Sie für Mergesort und Quicksort eine anschauliche Darstellungsform.
\begin{flushright}
Lösung auf Seite \pageref{a3.4:lsg}
\end{flushright}
\pagebreak
\part{Anhang}
\section*{Lösungen}
\subsection{Grundlagen der Algorithmik}
\label{a1:lsg}
\label{a2:lsg}
\label{a3:lsg}
\label{a4:lsg}
\label{a2.1:lsg}
\label{a2.2:lsg}
\label{a2.3:lsg}
\label{a2.4:lsg}
\label{a3.1:lsg}
\label{a3.2:lsg}
\label{a3.3:lsg}
\label{a3.4:lsg}
\section*{Tabellen und Formeln}
\printbibliography
\end{document}