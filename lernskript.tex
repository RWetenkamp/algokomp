\documentclass[11pt,a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{chngcntr}
\usepackage{upgreek}
\usepackage{pgfplots}
\pgfplotsset{compat = newest}
\usepackage{amsthm}
\usepackage{ulem}
\usepackage{tikz}
\usepackage{graphicx}
\usetikzlibrary{positioning}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage[backend=biber, style=alphabetic]{biblatex}
\addbibresource{literature.bib}
\author{Roman Wetenkamp}
\title{Algorithmen \& Komplexität}
\subtitle{Theoretische Informatik}

\newtheorem{note}{Bemerkung}
\newtheorem{definition}{Definition}
\newtheorem{satz}{Satz}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Beispiel}
\begin{document}
\vspace{3cm}
\maketitle
\begin{center}
\includegraphics[scale=0.7]{DHBW.jpg}
\end{center}
\pagebreak
\tableofcontents
\pagebreak
\section*{Vorwort}
Die theoretische Informatik ist genau das Thema, vor dem ich vor Beginn meines Studiums am meisten Respekt hatte ... Und das ist es neben den anderen Mathematik-Vorlesungen bis heute: Theoreme, Kalküle und Formeln sind nun einmal nicht die Dinge, mit denen sich die Masse der Informatikstudierenden gerne befasst, mich eingeschlossen. Mit einer  gerade eben bestandenen Logikklausur im ersten Semester sind dies Gründe genug dafür, ergänzendes, motivierendes und begreifbares Material zusammenzustellen, das mich einerseits akut durch die Klausur dieses Semesters bringt und hoffentlich auch für andere einen Nutzen hat. Geteiltes Leid ist halbes Leid! \\\\
Hier möchte ich den Vorlesungsstoff auf meine Art zusammenfassen und komplettieren, um Beispiele ergänzen und mit Aufgaben versehen, wie ich sie in der Vorlesung oder weiteren Büchern antraf. Ich persönlich lerne durch Aufgaben einfach am besten und für diejenigen, denen es auch so geht, gibt es davon hier ausreichend. Lösungen finden sich (in einigen Wochen) im Anhang. \\\\
\textit{Viel Erfolg!}  \\
\begin{flushright}
Roman Wetenkamp \\
Mannheim, den \today
\end{flushright}  
\vfill
\paragraph{Warnung}
Das Studium an einer Dualen Hochschule unterscheidet sich von dem Studium an Universitäten oder regulären Fachhochschulen insbesondere dadurch, dass aufgrund der Dualität zwischen Theorie und Praxis meist nur die Hälfte der Zeit zur Vermittlung des Stoffes zur Verfügung steht (wenn dann auch intensiver). Daher gehen Sie bitte nicht davon aus, dass Sie dieses Skript für Klausuren in regulären Vollzeitstudiengängen vorbereitet!
\paragraph{Hinweis}
Dieses Dokument ist kein Vorlesungsmaterial, hat keineswegs den Anspruch auf Vollständigkeit und enthält mit Sicherheit Fehler. Desweiteren ist es noch lange nicht vollendet (es ist infrage zustellen, ob es das je sein wird), und doch möchte ich Sie ermutigen, beizutragen! Jegliche Fehler, Probleme oder Anmerkungen können Sie mir gerne über das dazugehörige GitHub-Repository unter der URL \url{https://github.com/RWetenkamp/algokomp} zukommen lassen. Ebenso sind Sie völlig frei darin, dieses Dokument für Ihre legitimen Zwecke zu nutzen -- bitte beachten Sie dennoch Ihr Gewissen. Danke!
\pagebreak
\part{Grundlagen der Algorithmik}
\section{Der Algorithmusbegriff}
Wie der Titel dieses Moduls schon verrät, befassen wir uns mit Algorithmen: Jenen nebulösen Dingen, die uns mit ähnlich Denkenden und Interessierten in Social-Media-Plattformen zusammenbringen, komplexe mathematische Berechnungen ausführen und zunehmend mehr an Einfluss in unserem Leben gewinnen. Wir betrachten hier nun die Wortherkunft, definieren den Begriff anschließend und gehen auf Eigenschaften eines solchen ein.
\paragraph{Wortherkunft}
Der Begriff {\glqq}Algorithmus{\grqq} besteht zum einen aus dem altgriechischen Wort \textit{arithmos} -- Zahl und zum anderen geht er zurück auf den persischen Mathematiker \textsc{al-Charismi} (780-846 n. Chr.), dessen Werk \textit{{\glqq}Algorismus{\grqq}} schon eine gewisse Nähe zum heute üblichen Begriff offenbart. \parencite{brockhaus:algorithmus}
\begin{definition}
Unter einem Algorithmus verstehen wir eindeutige Verarbeitungsvorschriften zur Lösung eines Problems oder einer Problemklasse. Daran geknüpft ist der Gedanke des EVA-Prinzips: Eine Eingabe wird verarbeitet und auf Grundlage dessen wird eine Ausgabe erzeugt, ein Algorithmus ist hier für die Verarbeitung bis zur Ausgabe notwendig. 
\end{definition}
Algorithmen finden in einer Vielzahl von Bereiches unseres Lebens Einsatz: 
\begin{itemize}
\item Zur Untersuchung großer Datenmengen
\item Zur Kommunikation/Suche im Internet
\item In bildgebenden Verfahren oder diagnostischen Anwendungen der Medizin
\item In Assistenzsystemen im Auto
\item Bei der Partnersuche über Online-Dating-Plattformen
\end{itemize}
Damit finden wir intuitiv eine Begründung für die Untersuchung von Algorithmen.
\begin{note}
Ein \textit{Algorithmus} ist von einem \textit{Programm} unbedingt abzugrenzen: Der Algorithmus bezeichnet ein abstraktes Konzept zur Lösung eines gegebenen Problems während ein Programm die konkrete Umsetzung eines Algorithmus in einer Programmiersprache ist. (Implementierung) 
\end{note}
Bei der Betrachtung von Algorithmen betrachten wir die folgenden drei Ziele:
\begin{itemize}
\item Korrektheit
\item Effizienz
\item Einfachheit
\end{itemize}
\paragraph{Einfachheit}
Aus wirtschaftlichen Gründen besteht ein Interesse daran, dass ein Algorithmus so einfach wie möglich ist, beispielsweise in Bezug auf Implementierungskosten. Ein weiterer Aspekt besteht in der Prüfung auf Korrektheit: Zu einem zu komplexen Algorithmus lässt sich nur schwer feststellen, ob dieser korrekt ist.
\paragraph{Korrektheit}
Die Korrektheit ist die zentrale Eigenschaft eines Algorithmus -- Wenn ein Algorithmus nicht das leistet, was er vorgibt zu leisten oder was von ihm erwartet wird, ist er wertlos. Die Spezifikation beschreibt das exakte Verhalten unter Angabe von einer Vorbedingung (Zustand der Daten vor der Ausführung) und einer Nachbedingung (erwünschter Zustand nach der Ausführung).
\begin{definition}
Ein Algorithmus heißt \textbf{partiell korrekt}, falls aus erfüllter Vorbedingung die Nachbedingung folgt. \\
Ein Algorithmus heißt \textbf{total korrekt}, falls er partiell korrekt ist und nach endlich vielen Schritten terminiert.
\end{definition}
Die Korrektheit eines Algorithmus kann manuell, per Implementierung oder mathematisch verifiziert werden, wobei letztere Variante häufig vorzuziehen ist.
\begin{note}
Nicht jeder Algorithmus muss für jede Eingabe terminieren. Unter dem \textbf{Halteproblem} versteht man die Frage, ob ein Algorithmus nach endlich vielen Schritten terminiert. Diese Frage ist algorithmisch nicht entscheidbar, wie \textsc{Alan Turing} feststellte. Stattdessen muss die Termination für jeden Algorithmus einzeln entschieden werden.
\end{note}
\section{Effizienz / Komplexität}
Weitaus komplexer als die Frage der Einfachheit oder Korrektheit ist jene der Komplexität selbst:
\begin{definition}
Die Effizienz / Komplexität eines Algorithmus ist ein Maß für die Menge an Ressourcen (Rechenzeit oder Speicherbedarf), die er benötigt.
\end{definition}
Dabei ist zu beachten, dass es für ein Problem häufig mehrere Algorithmen gibt, die sich in ihrer Komplexität unterscheiden. Daher muss die Komplexität für jeden Algorithmus einzeln bestimmt werden. Dieses Skript befasst sich im Wesentlichen nur mit der Rechenzeit. \\\\
Für eine wirklich präzise Aussage über die Rechenzeit wäre es erforderlich, für jede Implementation eines Algorithmus in einer Programmiersprache einzeln festzustellen, welche Rechenzeit jede einzelne Operation auf dem spezifischen Prozessor des Testsystems benötigt. Da diese Aussagen mittlerweile aufgrund der Entwicklungen auf dem Prozessormarkt an Relevanz verlieren, verlagert sich die Betrachtung auf die Komplexität eines Algorithmus. \\\\
Die Komplexität eines Algorithmus kann folglich keine genaue Angabe in einer Zeiteinheit sein, die beispielsweise aussagt, wie schnell denn nun wirklich der größte gemeinsame Teiler von 443 und 123 mithilfe des euklidischen Algorithmus berechnet werden kann, sondern ist ein abstraktes Konzept, das Vergleiche zwischen Algorithmen ermöglichen soll:
\begin{itemize}
\item Abstraktion konstanter Faktoren
\item Abstraktion unbedeutender Terme
\item Wachstumsverhalten der Laufzeit bei Veränderung der Größe der Eingabe
\end{itemize}
Diese Abstraktion motiviert nun den folgenden Abschnitt.
\section{Maß der Komplexität: $O$-Notation}
Nach dem Mathematiker \textsc{Edmund Landau} definieren wir in Bezug auf das Wachstum von Funktionen eine {\glqq}obere Schranke{\grqq} und nennen diese $O(f)$. Falls eine Funktion $g$ nicht schneller wächst als $f$, so sagen wir, dass $g \in O(f)$. Eine ebenfalls übliche, jedoch irritierende Schreibweise ist $g = O(f)$.
\begin{definition}
Sei $\mathbb{R_{+}} := \{ x \in \mathbb{R} \mid x > 0 \}$ und $\mathbb{R_{+}^{\mathbb{N}}} := \{f \mid f \text{ist eine Funktion der Form } f : \mathbb{N} \to \mathbb{R_{+}}\}$ \\\\
Sei $g \in \mathbb{R_{+}^{\mathbb{N}}}$ gegeben. Dann ist die Menge aller Funktionen, die höchstens so schnell wachsen wie $g$ definiert als: \begin{align*}
O(g) := \{f \in \mathbb{R_{+}^{\mathbb{N}}} \mid \exists n_{0} \in \mathbb{N}: \exists c \in \mathbb{R_{+}}: \forall n \in \mathbb{N}: (n \geq n_{0} \Rightarrow f(n) \leq c \cdot g(n))\}
\end{align*}
\end{definition}
Die $O$-Notation gibt folglich eine Menge vergleichbarer Funktionen an, die es ermöglicht, die Komplexität eines Algorithmus abzuschätzen.
\begin{example}
Behauptung:
$3n^{3} + 2n^{2} + 7 \in O(n^{3})$
\begin{proof}

Gesucht ist eine Konstante $c \in \mathbb{R_{+}}$ und eine Konstante $n_{0} \in \mathbb{N}$, sodass für alle $n \in \mathbb{N}$ mit $n \geq n_{0}$ gilt: \\
$3n^{3} + 2n^{2} + 7 \leq c \cdot n^{3}$ \\\\
Wähle $n_{0} := 1 \text{ und } c := 12$ \footnote{Die Wahl von $c$ und $n$ erfolgt hier durch Ausprobieren -- In diesem Fall ergibt sich 12 als Summe aus den Potenzfaktoren.} \\
Sei nun 
\begin{align}
& 1 \leq n \\
(1)^{3}: \quad & 1 \leq n^{3} \\
(2) \cdot 7: \quad & 7 \leq 7n^{3} \\
(1) \cdot 2n^{2}: \quad & 2n^{2} \leq 2n^{3} \\
\quad & 3n^{3} \leq 3n^{3} \\
(3)+(4)+(5): \quad & 3n^{3} + 2n^{2} + 7 \leq 12n^{3}
\end{align}
Durch die Ungleichungen (3), (4) und (5) ist für jedes Polynom einzeln eine Abschätzung erfolgt. Addiert man diese Ungleichungen nun, ergibt sich (6). Damit ist gezeigt, dass die Behauptung wahr ist und die Funktion in $O(n^{3})$ liegt. Aus der Definition der $O$-Notation ergibt sich, dass jede Funktion mit höchstem Grad 3 in $O(n^{3})$ liegt, da Vorfaktoren und Konstantglieder entfallen. 
\end{proof}
\end{example} 
Die $O$-Notation eines Algorithmus lässt sich auch durch vollständige Induktion beweisen.
\begin{example}
Behauptung: $n \in O(2^{n})$
\begin{proof}
Wähle $n_{0} := 0, c:= 1$. Zu zeigen: $n \leq 2^{n} \forall n \in \mathbb{N}$\\\\
\uline{Beweis durch Induktion nach $n$:} \\
Induktionsanfang (IA): $n = 0 \quad n = 0 \leq 1 = 2^{0} = 2^{n}$ \\
Induktionsvoraussetzung (IV): $n \leq 2^{n}$ z.Z. $n+1 \leq 2^{n+1}$ \\
Induktionsschritt (IS): $n \mapsto n + 1$ \\
Per einfacher Induktion kann man nun zeigen: 
\begin{align}
& n \leq 2^{n} \\
\text{Daraus folgt mit IV:} \quad & n+1 \leq 2^{n} + 2^{n} = 2*2^{n} = 2^{n+1}
\end{align}
\end{proof}
\end{example} 
Die Eigenschaft $f \in O(g)$ induktiv zu zeigen, ist mühsam. Stattdessen können die folgenden Propositionen genutzt werden: 
\begin{note}
\begin{align}
f \in O(f) \\
f \in O(g) \Rightarrow d \cdot f \in O(g) \\
f \in O(n) \land g \in O(n) \Rightarrow f + g \in O(n) \\
f_{1} \in O(g_{1}) \land f_{2} \in O(g_{2}) \Rightarrow f_{1} \cdot f_{2} \in O(g_{1} \cdot g_{2}) \\
f_{1} \in O(g_{1}) \land f_{2} \in O(g_{2}) \Rightarrow \frac{f_{1}}{f_{2}} \in O(\frac{g_{1}}{g_{2}}) \\
f \in O(g) \land g \in O(n) \Rightarrow f \in O(n)
\end{align}
\end{note}
Darüber hinaus gilt folgender Satz: 
\begin{satz}
Seien $f, g: \mathbb{N} \to \mathbb{R_{+}}$. Dann gilt: \\
\[f(n) \in O(g(n)) \iff (\frac{f(n)}{g(n)})_{n \in \mathbb{N}} \text{ ist beschränkt}\]
\end{satz}
Außerdem finden Grenzwertbetrachtungen Eingang:
\begin{lemma}
\begin{align}
f \in O(g) \text{ und } g \in O(f), \text{ wenn } \lim \limits_{n \to \infty} (\frac{f(n)}{g(n)}) = c, c \neq 0 \\
f \in O(g) \text{ und } g \notin O(f), \text{ wenn } \lim \limits_{n \to \infty} (\frac{f(n)}{g(n)}) = 0 \\
f \notin O(g) \text{ und } g \in O(f), \text{ wenn } \lim \limits_{n \to \infty} (\frac{f(n)}{g(n)}) = \infty
\end{align}
\end{lemma}
Neben der $O$-Notation gibt es zwei weitere Notationen, die im Folgenden vorgestellt werden.
\subsection{$\Omega$-Notation}
Neben der Menge aller Funktionen, die höchstens so schnell wächst wie die betrachtete Funktion, bezeichnen wir die Menge der Funktionen, die mindestens so schnell wächst wie die Funktion als $\Omega(n)$.
\begin{definition} Sei $g \in \mathbb{R_{+}}$. Die Menge aller Funktionen, die mindestens so schnell wachsen wie $g$ ist: 
\[\Omega(n) := \{ f \in \mathbb{R_{+}^{\mathbb{N}}} \mid \exists n_{0} \in \mathbb{N}: \exists c \in \mathbb{R_{+}}: \forall n \in \mathbb{N}: (n \geq n_{0} \Rightarrow f(n) \geq c \cdot g(n))\}\]
\end{definition}
Diese Notation fristet ein Schattendasein: In der Regel ist die $O$-Notation geeigneter, da eine Orientierung am {\glqq}Worst-Case-Szenario{\grqq} üblicher ist.
\subsection{$\Theta$-Notation}
Aus $O$-Notation und $\Omega$-Notation ergibt sich nun die Menge der Funktionen, die genau so schnell wachsen wie die betrachtete Funktion. Wir bezeichnen sie mit $\Theta$.
\begin{definition}
\[\Theta(g) = O(g) \cap \Omega(g)\]
\end{definition}
\subsection{Zusammenfassung}
\begin{itemize}
\item Die $O$-Notation umfasst alle Funktionen, die nicht schneller wachsen als $f$. Damit beschreiben wir eine obere Schranke für die Komplexitätsfunktion.
\item Die $\Omega$-Notation umfasst alle Funktionen, die mindestens so schnell wachsen wie $f$. Dies ist eine untere Schranke.
\item Die $\Theta$-Notation umfasst alle Funktionen, die genau so schnell wachsen wie f. Damit erhalten wir ein asymptotisches Maß.  
\end{itemize}
\begin{lemma}
\begin{itemize}
\item $f \in O(g) \text{ und } g \in O(f), \text{ also } f \in O(g) \text{ und } f \in \Omega (g), \text{ wenn } \lim \limits_{n \to \infty} \frac{f(n)}{g(n)} = c, c \neq 0, \text{ also: } f \in \Theta (g)$
\item $f \in O(g) \text{ und } g \notin O(f), \text{ wenn } \lim \limits_{n \to \infty} \frac{f(n)}{g(n)} = 0, \text{ also: } g \in \Omega (f) \text{ und } f \notin \Omega (g)$
\item $ f \notin O(g) \text{ und } g \in O(f), \text{ also } g \notin \Omega (f) \text{ und } f \in \Omega (g), \text{ wenn } \lim \limits_{n \to \infty} \frac{f(n)}{g(n)} = \infty$
\end{itemize}
\end{lemma}
\section{Rekursionsgleichungen}
Nun haben wir mit den Landau-Symbolen einen mathematischen Weg gefunden, die Komplexität von Algorithmen zu beschreiben. Im Folgenden wenden wir dieses Modell auf konkrete Algorithmen an. \\\\
Bekannte Algorithmen sind beispielsweise Such- oder Sortierverfahren. Diese sind häufig rekursiv, d.h. eine Iteration verarbeitet direkt das Ergebnis einer vorherigen. Dadurch ergeben sich nun sogenannte \textbf{Rekurrenzgleichungen} oder auch \textbf{Rekursionsgleichungen}.
\begin{example}
$T(n) = 2 \cdot T(n-1) + 1$ ist eine Rekurrenzgleichung, da der Funktionswert $n$ direkt vom Funktionswert $n-1$ abhängt. Aus den Programmierkonzepten ist uns Rekursion als ein {\glqq}Wiederaufruf von sich selbst{\grqq} bekannt, dieses Muster findet sich hier in unseren betrachteten Algorithmen.
\end{example}
Um die Komplexität eines rekursiven Algorithmus bestimmen zu können, bedienen wir uns je nach Rekursionsgleichung unterschiedlichen Verfahren. Eines davon ist das sogenannte Mastertheorem. 
\subsection{Mastertheorem}
\begin{satz} Sofern alle Teilprobleme die gleiche Größe haben, können wir zwei Fälle unterscheiden: 
\begin{itemize}
\item Basisfall: $f(n)$ ist konstant für hinreichend kleine Werte von $n$
\item Für größere $n$ lässt sich eine Rekursionsgleichung der folgenden Form bestimmen: 
\[f(n) = a \cdot f(\frac{n}{b}) + O(n^{d})\]
$a$ ist hier die Anzahl der rekursiven Aufrufe, $b$ der Verkleinerungsfaktor des Problems und $d$ der Exponent für die Laufzeit der Vorbereitung der Rekursion und/oder die Zusammensetzung der Teilprobleme. \\\\
Nun gilt Folgendes: \\
\begin{align*}
f(n) \in 
\begin{cases}
O(n^{d}) \quad & \text{ falls } a < b^{d} \\
O(n^{d} \cdot log_{b}(n)) \quad & \text{ falls } a = b^{d} \\
O(n^{log_{b}(a)}) \quad & \text{ falls } a > b^{d}
\end{cases}
\end{align*}
\end{itemize}
\end{satz}
Mithilfe dieses Theorems kann nun die Komplexität eines rekursiven Algorithmus bestimmt werden:
\begin{example}
Gegegeben sei \[f(n) = 2 \cdot f(\frac{n}{2}) + O(n^{2})\] 
Daraus folgt $ a = 2, b = 2, d = 2$. \\\\
Da $2 < 2^{2}$ tritt der erste Fall ein und wir halten fest: $f(n) \in O(n^{2})$.
\end{example}
Über das Mastertheorem sind eine Zahl an Rekursionsgleichungen entscheidbar, jedoch nicht jede. Für die Rekursionsgleichungen, deren Form nicht der für das Mastertheorem benötigten entspricht, werden andere Verfahren nötig.
\subsection{Auflösungsverfahren}
Eine gegebene Rekursionsgleichung lässt sich in eine geschlossene Form überführen, in der keine Abhängigkeit zu vorherigen Funktionswerten mehr besteht. Dafür existieren zwei Ansätze: 
\begin{itemize}
\item \textbf{bottom-up:} Aus berechneten Funktionswerten für kleine n wird eine weitere Berechnungsvorschrift ermittelt. Diese muss anschließend induktiv bewiesen werden.
\item \textbf{top-down:} Die vorherigen Funktionswerte werden solange durch die entsprechende Gleichung ersetzt, bis der definierte Startwert eingesetzt werden kann. Anschließend wird der Term zusammengefasst und es ergibt sich eine geschlossene Rechenvorschrift. 
\end{itemize}
\begin{example}
Gegeben ist die folgende Rekursionsgleichung: $f(1) = 1, f(n) = 2 \cdot f(n-1) + 1$. \\\\
\textbf{bottom-up} \\
\begin{align}
f(1) = 1 \\
f(2) = 2 \cdot 1 + 1 = 3 \\
f(3) = 2 \cdot 3 + 1 = 7 \\
f(4) = 2 \cdot 7 + 1 = 15
\end{align}
Aus den Ergebnissen wird relativ schnell eine Verwandtschaft zu den ersten Potenzen der Zahl 2 deutlich und so lässt sich nun folgende Gleichung aufstellen: 
\[f(n) = 2^{n} - 1\]
Diese Gleichung kann nun für die gegebenen Werte von $n$ überprüft werden und muss anschließend (induktiv) bewiesen werden. \\
\begin{proof}
Zu zeigen: $f(n) = 2 \cdot f(n-1) + 1 \in O(2^{n})$ \\
\begin{align*}
\text{Induktionshypothese: }& \quad f(n) = 2^{n} - 1 \quad \forall n \geq 1 \\
\text{Induktionsanfang: }& \quad f(1) = 2^{1} - 1 = 1 \\
\text{Induktionsvoraussetzung: }& \quad n \to n + 1 \quad f(n+1) = 2^{n+1} -1\\
\text{Induktionsschritt: }& \quad f(n+1) = 2 \cdot f(n) + 1 = 2 \cdot (2^{n} - 1) + 1 = 2^{n+1} - 1
\end{align*}
nach \parencite{UniHH}
\end{proof}
Wir führen Induktionsbeweise für Rekurrenzgleichungen, indem wir zunächst im Induktionsanfang feststellen, dass unsere ermittelte Funktion den Rekursionsstartwert erfüllt. Ist dies nicht der Fall, brechen wir hier ab. Nun induzieren wir, in dem wir $n$ auf $n+1$ abbilden, bilden die Rekursionsgleichung für $n+1$ und setzen nun für $f(n)$ unsere geschlossene Funktion ein. Wir stellen fest, dass unsere Induktionsvoraussetzung erfüllt ist. Damit ist unsere Auflösung der Rekursionsgleichung gezeigt.\\\\
\textbf{top-down} \\
\begin{align}
f(n) =& \quad 2 \cdot f(n-1) + 1 \\
=& \quad 2 \cdot (2 \cdot f(n-2) + 1) + 1 \\
=& \quad 2 \cdot (2 \cdot (... \cdot (2 \cdot 1) + 1) ... ) + 1) + 1 \\
\Rightarrow & \quad 2^{n-1} + 2^{n-2} + ... + 2^{n-(n-1)} + 1 \\
=& \quad \sum_{i = 0}^{n-1} 2^{i} = \frac{2^{(n-1)+1}-1}{2-1} = 2^{n}-1 \in O(2^{n}) 
\end{align}
\end{example}
Je nach Rekursionsgleichung bietet sich mitunter eines der beiden Verfahren mehr an als das andere. Vernachlässigt werden darf beim \textit{bottum-up}-Verfahren nicht, dass ein Induktionsbeweis erforderlich ist! Bei Rekursionsgleichungen mit zweifacher Rekursion (in Beziehung zu den vorherigen zwei Funktionswerten) ist ein \textit{top-down}-Verfahren deutlich komplexer und häufig nicht ratsam.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Veranschaulichen Sie die $O$-, $\Theta$- und $\Omega$-Notation anhand der Funktion $f(n) = 3n^{3} + 7n^{2} + 16$ grafisch. 
\begin{flushright}
Lösung auf Seite \pageref{a1:lsg}
\end{flushright}
\paragraph{Aufgabe 2} Geben Sie zu folgenden Funktionen jeweils die $O$-Notation an.
\begin{enumerate}
\item $f(n) = 6n^{4} + 3 \cdot log_{2}(n)$
\item $f(n) = 6627816n + 13$
\item $f(n) = \frac{72n^{3} + 27n^{2} + 8n + 9}{n!}$
\item $f(n) = 3n \cdot \frac{n!}{2} + n^n $
\end{enumerate}
\begin{flushright}
Lösung auf Seite \pageref{a2:lsg}
\end{flushright}
\paragraph{Aufgabe 3}
\begin{enumerate}
\item Zeigen Sie, dass $n^{2} \in O(2^n)$.
\item Zeigen Sie, dass $n^{3} \in O(2^n)$.
\end{enumerate}
\begin{flushright}
Lösung auf Seite \pageref{a3:lsg} \\
entnommen aus VL
\end{flushright}
\paragraph{Aufgabe 4}
Bestimmen Sie unter Anwendung des Mastertheorems die jeweilige Komplexitätsklasse $O(n)$.
\begin{itemize}
\item $f(n) = 2 \cdot f(\frac{n}{2n+1}) + n^{2}$
\item $g(n) = log(n) \cdot g(\frac{n}{2}) + 3$
\item $h(n) = sin(n) \cdot h(\frac{n}{\frac{n}{2}}) + O(n^{3})$
\end{itemize}
\begin{flushright}
Lösung auf Seite \pageref{a4:lsg} \\
\end{flushright}
\paragraph{Hinweis} Für einige Aufgaben kann es hilfreich sein, die Tabelle der Komplexitätsklassen auf Seite \pageref{kmplxtab} zu verwenden. Im Rahmen der Klausurvorbereitung sollten Sie außerdem die Komplexität von Algorithmen aus Pseudocode heraus angeben können.
\pagebreak
\section{Bestimmung von Komplexitäten}
Nun haben wir in ausreichender Tiefe mathematisch-formal die $O$-Notation als Maß der Komplexität eingeführt. In der Praxis -- und insbesondere auch in Klausuren -- ist davon auszugehen, dass Sie einen Algorithmus in einer Programmiersprache implementiert oder als Pseudocode erhalten, zudem Sie dann aus dem Code heraus die Komplexität bestimmen sollen. \\\\
Dies wollen wir hier näher erläutern und üben.
\subsection{Rechengesetze der $O$-Notation}
\begin{satz}
Seien $d \in \mathbb{R}_{\geq 0}$ und $f, g, h: \mathbb{N}_0 \to \mathbb{R}_{\geq 0}$, dann gilt:
\begin{align}
 d = O(1) \\
 f = O(f) \\
 d \cdot f = O(f) \\
 f + g = O(g), \text{ falls } f = O(g) \\
 f + g = O(max\{f, g\}) \\
 f \cdot g = O(f \cdot h), \text{ falls } g = O(h)
\end{align}
Quelle: \parencite[][S. 67]{AlgInfTheo}
\end{satz}
Mithilfe dieser Rechengesetze können wir $O$-Notationen leichter bestimmen. Sie sind die Grundlage für Vereinfachungen, die Vergleiche der Komplexität im Ende ermöglichen.
\subsection{Komplexitätsklassen}
Betrachten wir nun allgemein die häufigsten Komplexitätsklassen, mit denen sich bereits viele Algorithmen vergleichen lassen. Dabei sind jeweils Klassen angegeben, d.h. beispielsweise $n^4$ läge zwischen $n^3$ und $2^n$ und fällt hier in die Klasse der \textit{kubischen} Komplexitäten. Aus dieser Tabelle und den vorangegangenen Rechengesetzen lässt sich nun beispielsweise auch die Komplexität einer zusammengesetzten Funktion wie beispielsweise $f(n) = n^3 + n!$ bestimmen. Da $n!$ um ein wesentliches schneller wächst als $n^3$, fällt $f$ in die Komplexitätsklasse $n!$. 
\label{kmplxtab}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
Klasse & Bezeichnung & Rang \\
\hline
$O(1)$ & konstant & exzellent \\
$O(log_x(n)$ & logarithmisch & sehr gut \\
$O(n)$ & linear & gut \\
$O(n \cdot log(n))$ & überlinear & gut \\
$O(n^2)$ & quadratisch & eher ineffizient \\
$O(n^3)$ & kubisch & ineffizient \\
$O(2^n)$ & exponentiell & sehr ineffizient \\
$O(n!)$ & faktoriell & extrem ineffizient \\
\hline
\end{tabular}
\caption{Komplexitätsklassen nach \parencite{Grundkurs}}
\end{table}
Nun ergibt sich die Frage, wie wir einen gegebenen Algorithmus nun einer dieser Klassen zuordnen, ohne, dass bereits vorher eine Funktionsgleichung gegeben ist. Dafür können wir diese Tabelle nutzen, die aufschlüsselt, wo die jeweilige Komplexität in Algorithmen vorkommt.
\begin{table}[h!]
\centering
\begin{tabular}{|c|p{10cm}|}
\hline
Klasse & Typischer Aufbau eines Algorithmus dieser Komplexität \\
\hline
$O(1)$ & die meisten Anweisungen werden nur einmal oder konstant oft ausgeführt (unabhängig von Eingaben) \\
$O(log(n)$ & Lösen eine Problems durch Aufteilen in kleinere Teilprobleme, dabei wird die Laufzeit um einen konstanten Anteil verringert \\
$O(n)$ & Falls $n$ Eingabedaten verarbeitet werden müssen, ist dies der optimale Fall, da jedes Element nur einmal bearbeitet wird \\
$O(n \cdot log(n))$ & Lösen eines Problems durch Aufteilen in kleinere Probleme, die unabhängig voneinander gelöst und abschließend rekombiniert werden \\
$O(n^2)$ & typisch für die paarweise Verarbeitung von $n$ Elementen $\to$ zwei verschachtelte For-Schleifen \\
$O(n^3)$ & drei verschachtelte For-Schleifen \\
$O(a^n)$ & Brute-Force-Lösungen (Ausprobieren aller Optionen) \\
\hline
\end{tabular}
\caption{Zuordnung von Algorithmen zu Verarbeitungsschritten nach \parencite[][S. 436]{Grundkurs}}
\end{table}
\subsection{Komplexität von Programmierkonzepten}
Eine einzelne Anweisung hat immer die Komplexität $O(1)$. Auch mehrere Anweisungen -- deren Anzahl jedoch nicht von der Menge der Eingabedaten $n$ abhängt -- verändern die Komplexität nicht.
\paragraph{Schleifen} 
Sofern jedoch eine Schleife in Abhängigkeit von $n$ (auch hier müssen nicht \textit{exakt} $n$ Schleifendurchläufe passieren, es genügt eine generelle Abhängigkeit!) durchlaufen wird, erhalten wir einen Aufwand von $O(n)$ mit $n$ = Anzahl der Daten. \\\\
Betrachten wir nun ineinander verschachtelte Schleifen, deren Durchlaufanzahl jeweils von $n$ abhängt, so multiplizieren sich die Komplexitäten und wir erhalten beispielsweise für zwei ineinander verschachtelte For-Schleifen eine Komplexität von $O(n^2)$. Nach diesem Muster gehen wir für beliebig häufig ineinander verschachtelte Schleifen vor.
\begin{example}
Gegeben sei folgender Algorithmus:
\begin{lstlisting}
public void validateInput(int[] data) {
	int i = 0;
	for (dataElement in data) {
		calculateSomething(dataElement);
		while (i < 10) {
			pingHost('localhost:3000/');
			i++;		
		}
	}
}
\end{lstlisting}
Wir betrachten zur Bestimmung der Komplexität:
\begin{itemize}
\item \texttt{int i = 0} ist eine einzelne Anweisung ohne Abhängigkeit zu $n$, hat also die Komplexität $O(1)$.
\item Nun beginnt eine For-Schleife mit $n$(= Anzahl der Feldelemente) Iterationen, also mit Komplexität $O(n)$.
\item Innerhalb der For-Schleife findet pro Iteration ein Aufruf der Funktion \texttt{calculateSomething} statt, deren Komplexität in derer der Schleife bereits inbegriffen ist (da es sich um einen einzelnen Aufruf pro Iteration handelt).
\item Nun beginnt eine weitere Schleife -- deren Iterationsanzahl jedoch von $n$ unabhängig ist und konstant 10 beträgt. Auch der Inhalt der Schleifen ist konstant und fällt daher nicht ins Gewicht. Daher beträgt die Komplexität hier erneut $O(1)$. Da wir Klassen von Komplexität betrachten, ist $O(10)$ zwar nicht falsch, jedoch nicht sinnvoll, weil so die Vergleichbarkeit entfällt/erschwert wird.
\item Auch \texttt{i++} ist eine Anweisung mit konstantem Zeitbedarf, also $O(1)$.
\end{itemize}
Nun addieren wir die einzelnen Komplexitäten und erhalten $O(n)$, da konstante Terme entfallen.
\end{example}
\begin{satz}
Eine Schleife, in der die Datenmenge in jeder Iteration halbiert wird, hat eine Komplexität von $O(log(n))$.
\end{satz}
Neben Schleifen, deren Komplexitätsbetrachtung wir nun in aller Ausführlichkeit ausgeführt haben, widmen wir uns nun einem weiteren, in Algorithmen sehr häufig anzutreffenden Programmierkonzept:
\paragraph{Rekursion}
Die Komplexität von rekursiven Algorithmen zu bestimmen ist im Allgemeinen aufwendiger. Wir betrachten verschiedene Fälle:
\begin{itemize}
\item Falls in jedem Schritt die betrachtete Datenmenge um eins kürzer wird und die zusätzlich ausgeführte Operation einen konstanten Zeitbedarf hat, so liegt die Komplexität in $O(n)$.
\item Ist dieser Zufallsaufwand nun nicht konstant sondern linear (verändert sich also in Abhängigkeit von $n$, so verändert sich der Aufwand und wir erhalten eine Komplexität von $O(\frac{n^2}{2})$.
\end{itemize}
An dieser Stelle sei auch noch einmal auf das \textbf{Master-Theorem} verwiesen, das bei Rekursionsgleichungen Anwendung findet, die gemäß {\glqq}Divide and Conquer{\grqq} (Teile und Herrsche) Probleme in kleinere Teilprobleme zerteilen, diese getrennt voneinander verarbeiten und anschließend wieder zusammensetzen. ($\to$ Mergesort, S.  \pageref{mergesort})
\vspace{1cm}
\section*{Aufgaben}
\begin{itemize}
\item Gegeben sind die folgenden Pseudocode-Darstellungen verschiedener Algorithmen. 
\item Notieren Sie zu jeder Anweisung die entsprechende Komplexität.
\item Geben Sie die Gesamtkomplexitätsklasse des Algorithmus in Landau-Symbolen an.
\end{itemize}
Verwenden Sie falls erforderlich das Master-Theorem.
\paragraph{Aufgabe 1} \quad
\begin{lstlisting}
public int[] giveEvenNumbers(int[] data) {
	int[] result;
	for (dataElement in data) {
		if (dataElement % 2 == 0) {
			result.append(dataElement);		
		}	
	}
	return result;
}
\end{lstlisting}
\begin{flushright}
Lösung auf Seite \pageref{a1.5:lsg}
\end{flushright}
\paragraph{Aufgabe 2} \quad
\begin{lstlisting}
for (int i = 0; i < n; i = 2 * i) {
	for (int j = n; j > 0; j = j / 2) {
		for (int k = 1; k < n; k = k + 1) {
			// some constant procedures ...		
		}	
	}
}
\end{lstlisting}
\begin{flushright}
Lösung auf Seite \pageref{a1.6:lsg} \\
entnommen aus \parencite{GrUeb}
\end{flushright}
\paragraph{Aufgabe 3} \quad
\begin{lstlisting}
float mittel(float a[], int start, int end) {
	int n = end - start + 1;
	if (n == 1) {
		return a[start];	
	}
	else {
		int mitte = n / 2 - 1;
		float m1 = mittel(a, start, start+mitte);
		float m2 = mittel(a, start + mitte + 1, end);
		float m = (	m1 * (mitte + 1) + 
		m2 * (end - (start + mitte + 1) + 1)) / n;
		return m;	
	}
}
\end{lstlisting}
\begin{flushright}
Lösung auf Seite \pageref{a1.7:lsg} \\
entnommen aus \parencite{GrUeb}
\end{flushright}
\part{Suchalgorithmen}
Sie studieren Informatik, sind es satt, noch mehr Stunden vor Bildschirmen zu verbringen und suchen sich deshalb ein echtes, physisches Buch aus der Bibliothek Ihrer Hochschule heraus. Sie fragten nach \glqq\textsc{Wirth}: Algorithmen und Datenstrukturen{\grqq} von 1983, weil man es Ihnen in Ihrer Vorlesung empfahl. Die Bibliothekarin deutet auf ein Regal und lässt sie damit allein. \\\\
Wie finden Sie das gesuchte Buch in der Regalreihe mit etwa 300 Büchern?
\section{Lineare Suche}
Sie beginnen ganz links. Nun lesen Sie den Titel des ersten Buches, er passt nicht. Sie lesen den Titel des zweiten Buches, er passt ebenfalls nicht. Sie fahren fort, iterieren über das gesamte Bücherregal und brechen ab, falls Sie es gefunden haben oder am Ende angelangt sind. Dieses Verfahren ist die \textbf{Lineare Suche}.
\paragraph{Idee}
Durchlaufe sukzessive das Feld A, bis das gesuchte Element gefunden ist bzw. das Ende des Feldes erreicht ist.
\paragraph{Implementierung}
\begin{lstlisting}
LinearSearch(A[], E) {
	i = 0;
	While i < A[].length {
		If (A[i] == E) {
			return i;			
		}
		i = i + 1;	
	}
	Return ElementNotFound
}
\end{lstlisting}
\paragraph{Komplexität}
Ganz offensichtlich ist der Aufwand dieses Suchverfahrens durch die Feldlänge (also die Anzahl der Bücher in unserem Regal) bestimmt. \\
\textit{günstigster Fall} \quad Das gewünschte Element steht an Indexposition 0 (also dem ersten Element bzw. es ist das erste Buch im Regal) $\to$ ein Vergleich ist erforderlich
\\
\textit{durchschnittlicher Fall} \quad Das gewünschte Element steht an der mittleren Indexposition $\frac{n}{2}$ bei $n$ Feldelementen. $\to \frac{n}{2}$ Vergleiche sind erforderlich
\\
\textit{ungünstigster Fall} \quad Das gewünschte Element ist nicht im Feld vorhanden. $\to n$ Vergleiche
\\\\
Insgesamt ergibt sich -- da wir bei der Angabe der Komplexität durch die $O$-Notation jeweils den ungünstigsten Fall betrachten -- für die Lineare Suche eine Komplexität von $O(n)$. 
\section{Binäre Suche}
Sie befinden sich in der perfekten Bibliothek: Alle Bücher sind aufsteigend nach ihren Autor*innen sortiert! Dieses Wissen können Sie sich zunutze machen, in dem Sie ihr Wunschbuch mit der \textbf{binären Suche} finden: Sie teilen die Bücherreihe in zwei Hälften und betrachten nun das mittlere Buch: Muss Ihr Buch links oder rechts von dem mittleren Buch stehen (Vergleich anhand des Namens der Autoren)? Oder ist es sogar ihr Buch? Nein, das ist es nicht und der Autor Ihres Buches hat einen im Alphabet weiter hinten stehenden Anfangsbuchstaben, also betrachten Sie die Hälfte rechts des mittleren Buches. Diese Hälfte teilen Sie nun erneut, betrachten die neue Mitte und fahren fort, bis ihre Hälften aus einem Buch bestehen.
\paragraph{Idee} Vergleiche zu suchendes Element $E$ mit der Mitte des Suchbereiches $A[m]$. Falls gefunden: Abbruch. Falls $E < A[m]$: Suche weiter in linker Hälfte. Falls $E > A[m]$: Suche weiter in rechter Hälfte. Das Weitersuchen erfolgt durch einen rekursiven Aufruf des Algorithmus mit der entsprechenden Hälfte des Feldes.
\paragraph{Implementierung}
\begin{lstlisting}
BinarySearch(A[], E, indexLeft, indexRight) {
	if (indexLeft > indexRight) {
		return ElementNotFound	
	}
	else {
		mid = (indexLeft+indexRight) / 2;
		if (E == A[mid]) {
			return mid;		
		}
		else if (E < A[mid] {
			return BinarySearch(A[], E, indexLeft, mid-1);	
		}
		else {
			return BinarySearch(A[], E, mid+1, indexRight);	
		}
	}
}
\end{lstlisting} 
Die Parameter \texttt{indexLeft} und \texttt{indexRight} schränken jeweils das zu betrachtende Feld ein. Zu Beginn ist \texttt{indexLeft} mit $0$ und \texttt{indexRight} mit \texttt{A[].length()} initialisiert. 
\paragraph{Komplexität}
Hier handelt es sich um ein rekursives Verfahren, folglich werden wir eine Rekursionsgleichung erhalten und diese mit den bekannten Verfahren auswerten. Doch zunächst betrachten wir wieder Fälle: \\
\textit{günstigster Fall}: \quad E ist das mittlere Element $\to$ 1 Vergleich\\
\textit{ungünstigster Fall}: \quad E ist nicht im Feld \\
Die Rekursionsgleichung mit den Parametern $a, b \text{ und } d$ ergibt sich nun wie folgt:
\begin{itemize}
\item Es gibt einen rekursiven Aufruf, d.h. $a = 1$.
\item Pro Rekursionsaufruf halbieren wir das Feld, daraus folgt $b = 2$.
\item Die Vorbereitung der Rekursion verläuft konstant und ist minimal, also $d = 1$.
\end{itemize}
Die Rekursionsgleichung lautet nun: 
\[f(n) = 1 \cdot f(\frac{n}{2}) + 1\]
Für die Anwendung des Mastertheorems formen wir um:
\[\iff f(n) = 1 \cdot f(\frac{n}{2}) + O(n^{0})\]
Nun ergibt sich: 
\[a = b^{d} \iff 1 = 2^{0} \Rightarrow f(n) \in O(n^{0} \cdot log_{2}(n)) = O(log(n))\]
\section{Textsuche}
Spätestens jetzt sollten Sie das Buch gefunden haben, wenn Sie die binäre Suche verwendet haben, vermutlich schneller als mit der linearen Suche. Jetzt haben Sie einen Arbeitsplatz gefunden, setzen sich und schlagen das Buch auf: Für ein Lesen in Ruhe haben Sie keine Zeit, daher suchen Sie sich die interessanten Teile heraus. Sie suchen, wo \textsc{Niklaus Wirth} Ihnen die $O$-Notation erklären wird. \\\\
Formal suchen Sie folglich alle Vorkommen eines Musters (in Form einer Zeichenkette) in einem Text. Dafür betrachten wir zwei Verfahren.
\subsection{Einfache Textsuche}
Analog zur linearen Suche beginnen Sie einfach, Ihren gesuchten Begriff unter den Text zu legen. Stimmt der erste Buchstabe überein, vergleichen Sie den zweiten. Stellen Sie eine Differenz fest, verschieben Sie Ihr Suchwort um eine Position nach rechts. Derart fahren Sie fort, bis Sie am Ende des Textes angelangt sind.
\paragraph{Idee}
Beginnend beim ersten Zeichen des Textes legt man das Muster der Reihe nach an jede Stelle des Textes an und vergleicht zeichenweise von links nach rechts, ob eine Übereinstimmung vorliegt
\paragraph{Implementierung}
\begin{lstlisting}
int BruteForceSearch(char[] text, char[] pattern) {
	int n = text.length;
	int m = pattern.length;
	int i = 0;
	int j = 0;
	for (int k = 0; i <= n-m; i++) {
		j = 0;
		while (j < m && Text[i+j] == pattern[j]) {		
			j++;		
		}
		if (j == m) {
			return i;		
		}
	}
	return ElementNotFound;
}
\end{lstlisting}
\paragraph{Komplexität}
Offenbar hängt der Aufwand dieses Verfahrens ganz wesentlich davon ab, wie lang das Muster und der Text in Gänze sind. Weniger relevant hingegen ist der Aufbau des Textes, da alle Vorkommen des Musters gesucht werden. Insofern ist es nicht entscheidend, an welcher Stelle das Muster das erste Mal auftritt. \\\\
\textit{günstigster Fall} \quad Im günstigsten Fall werden m Vergleiche benötigt, wenn nämlich die Länge des Textes kleiner als das doppelte der Länge des Musters $m$ ist und das Muster direkt am Beginn des Textes steht. $\to m$ Vergleiche 
\\\\ \textit{ungünstigster Fall}\quad Im ungünstigsten Fall werden $(n-m+1) \cdot m$ Vergleiche benötigt, wobei dieser Fall allgemeingültig ist -- es werden alle Vorkommen des Musters im Text gesucht und nicht lediglich das erste. \\\\
Die Laufzeit des Algorithmus liegt nun in $O(n \cdot m)$.
\subsection{Knuth-Morris-Pratt-Algorithmus (KMP)}
Ihnen ist das zu mühsam geworden, Zeichen für Zeichen des Textes und des Musters zu vergleichen. Sie sehnen sich nach einem intelligenteren, einfacheren Weg. Und, wahrlich den gibt es: Der \textbf{Knuth-Morris-Pratt}-Suchalogorithmus nach \textsc{Donald Ervin Knuth}, \textsc{James Hiram Morris} und \textsc{Vaughan Ronald Pratt}. \\\\
Die Grundlage für diesen Algorithmus bildet die vorangegangene einfache Textsuche. Diese optimieren wir nun, in dem wir beim ersten sich unterscheidenden Zeichen von Text und Muster das Muster nicht um ein einziges Zeichen nach rechts verschieben, sondern um eine bestimmte Anzahl an Zeichen, die sich aus unserem Muster ergibt und als \textbf{Rand} bezeichnet wird.
\begin{definition}
Der \textbf{Rand} eines Musters $M$ mit der Länge $m$ ist eine tabellarische Auflistung aller Teilzeichenketten der Längen $0$ bis $m$ des Musters, für die jeweils die Anzahl der Zeichen eines Prefixes, das zugleich Postfix ist, bestimmt wird.
\end{definition}
\begin{example}
Betrachten wir das Wort \texttt{abbababbab}. \\
Wir beginnen eine tabellarische Auflistung: \\\\
\begin{tabular}{|l|c|c|l|}
\hline
Teilzeichenkette & Länge & Rand & Bemerkung \\
\hline
& 0 & -1 & per Definition, für Teilschritt 2 (Suche) erforderlich \\
\texttt{a} & 1 & 0 & per Definition \\ 
\texttt{ab} & 2 & 0 & $a \neq b$ \\
\texttt{abb} & 3 & 0 & \\
\texttt{abba} & 4 & 0 & $ab \neq ba$, Palindrome haben den Rand 0 \\
\texttt{abbab} & 5 & 2 & gleich: $ab$, Länge: 2 \\
\texttt{abbaba} & 6 & 1 & gleich: a, Länge: 1 \\
\texttt{abbabab} & 7 & 2 & gleich: ab, Länge: 2 \\
\texttt{abbababb} & 8 & 3 & gleich: abb, Länge: 3 \\
\texttt{abbababba} & 9 & 4 & gleich: abba, Länge: 4 \\
\texttt{abbababbab} & 10 & 5 & gleich: abbab, Länge: 5 \\
\hline 
\end{tabular} \\\\
Im folgenden Suchalgorithmus wird jeweils der Rand verwendet, um Verschiebungspositionen zu bestimmen. Daher ist die gesamte Tabelle erforderlich.
\end{example} \pagebreak
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
Rand[0] = -1; 		/* Definition */
Rand[1] = 0;		/* Definition */

for (j = 2; j <= m; j++) {
	while ((i >= 0) and (M[i] != M[j-1])) {
		i = Rand[i];
		i++;
	}
	Rand[j] = i;
}
\end{lstlisting}
Folglich iterieren wir nun über alle möglichen Teilzeichenkettenlängen (\texttt{j}) und vergleichen jeweils mit dem vorherigen Rand (durch \texttt{i}). Wenn unser Rand maximal ist, verlassen wir die \texttt{while}-Schleife und speichern den ermittelten Rand. \\\\
Eine Veranschaulichung der algorithmischen Rand-Bestimmung finden Sie in der Wikipedia. \parencite{KMPRand}
\paragraph{Der Suchalgorithmus}
Ähnlich wie bei der einfachen Textsuche beginnen Sie, indem Sie das zu suchende Muster unter den Suchraum schreiben und vergleichen Zeichen für Zeichen. Entscheidend ist nun die erste Abweichung: Sie bestimmen den \texttt{Rand[]} der Indexposition der Abweichung im Muster und verschieben ihr Muster nun mit unterer Formel. So nutzen Sie aus, dass bis zur ersten Abweichung Gleichheit bestand -- falls bei vier gleichen Zeichen ein Rand von zwei vorliegt, können Sie ihr Muster so verschieben, dass der Beginn nun dort liegt, wo zuvor das mit dem neuen Anfang identische Enge lag. Nun fahren Sie mit den neu gewonnen Index-Positionen fort, die sich aus dem Verschiebung ergeben haben. \\\\
Die Formel für Verschiebungen ist: \\
\texttt{Neuer Beginn des Musters (neue Suchtextposition) = Suchtextposition (aktueller Beginn des Musters) + (Anzahl übereinstimmender Zeichen -- Randlänge[Index im Muster])}
\begin{example}
Sie suchen das Muster \texttt{ababaab} im Text \texttt{baabbaababaabbbabba}. \\
\textbf{Schritt 1}: Bestimmung des Randes \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
\hline
-1 & 0 & 0 & 1 & 2 & 1 & 1 & 2 \\
\hline
\end{tabular} \\\\
\textbf{Schritt 2}: Suche \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0 & 1 & 2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
\cellcolor{red}b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
\cellcolor{red}a&b&a&b&a&a&b& & & & & & & & & & & & \\
\hline
\end{tabular} \\
Erste Abweichung an Index 0 $\to$ Rand[0] = -1, nach Formel oben: $0+(0-(-1)) = 1$. \\Also verschieben wir das Muster an Indexposition 1. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&\cellcolor{green}a&\cellcolor{red}a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 &\cellcolor{green}a&\cellcolor{red}b&a&b&a&a&b& & & & & & & & & & &  \\
\hline
\end{tabular} \\
Abweichung an Index 2 (1 im Muster), Rand[1] = 0, mit Formel: $1+(1-0) = 2$. \\Wir verschieben also so, dass das erste Zeichen des Musters an Index 2 steht.  \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&\cellcolor{green}a&\cellcolor{green}b&\cellcolor{red}b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & &\cellcolor{green}a&\cellcolor{green}b&\cellcolor{red}a&b&a&a&b& & & & & & & & & &   \\
\hline
\end{tabular}\\
Abweichung an Index 4 (2 im Muster), Rand[2] = 0, mit Formel: $2+(2-0) = 4$. \\Neue Indexposition des Musters: 4. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&\cellcolor{red}b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & &\cellcolor{red}a&b&a&b&a&a&b& & & & & & & &    \\
\hline
\end{tabular}\\
Abweichung an Index 4 (0 im Muster), Rand[0] = -1, mit Formel: $4+(0-(-1)) = 5$. \\ Neue Indexposition: 5. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&\cellcolor{green}a&\cellcolor{red}a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & & &\cellcolor{green}a&\cellcolor{red}b&a&b&a&a&b& & & & & & &    \\
\hline
\end{tabular}\\
Abweichung an Index 6 (1 im Muster), Rand[1] = 0, mit Formel: $5+(1-0) = 6$. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&\cellcolor{green}a&\cellcolor{green}b&\cellcolor{green}a&\cellcolor{green}b&\cellcolor{green}a&\cellcolor{green}a&\cellcolor{green}b&b&b&a&b&b&a \\
\hline
 & & & & & &\cellcolor{green}a&\cellcolor{green}b&\cellcolor{green}a&\cellcolor{green}b&\cellcolor{green}a&\cellcolor{green}a&\cellcolor{green}b& & & & & &    \\
\hline
\end{tabular}\\
Nun tritt keine Abweichung ein, das erste Vorkommen des Musters im Suchtext ist gefunden. Da wir nun an allen Vorkommen interessiert sind, führen wir unsere Suche fort. Dafür bestimmen wir zunächst den Rand[7] = 2. Mit der Formel folgt: $6+(7-2) = 11$. An Position 11 können wir nun unsere Suche fortsetzen.\\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&\cellcolor{green}a&\cellcolor{green}b&\cellcolor{red}b&b&a&b&b&a \\
\hline
 & & & & & & & & & & &\cellcolor{green}a&\cellcolor{green}b&\cellcolor{red}a&b&a&a&b&    \\
\hline
\end{tabular}\\
$11+(2-0) = 13$. Damit verlassen wir den Suchtext, der Algorithmus terminiert, da kein weiteres Vorkommen gefunden wurde.
\end{example}
Die Stärken des KMP-Algorithmus liegen folglich in der Berücksichtigung von Rändern an Anfang und Ende des Musters, die größere Sprünge als eins (im Vergleich zur einfachen Textsuche) ermöglichen. Liegt nun ein Muster ohne jegliche oder mit wenigen Rändern vor, ist der Laufzeitgewinn marginal.
\paragraph{Implementierung}
als Pseudo-Code
\begin{lstlisting}
KnuthMorrisPrattAlgorithm(Pattern, SearchSpace, Borders) {
	int i = 0;	/* Current position in search space */
	int j = 0;	/* Current position in pattern */

	while (i < SearchSpace.length) {
		// Move pattern until first character of 
		// SearchSpace and Pattern is equal
		while (j >= 0 && SearchSpace[i] != Pattern[j]) {
			j = Borders[j]		
		} 
		
		// Now compare the next character
		i = i + 1;
		j = j + 1;

		// End of pattern: return result		
		if (j == (Pattern.length-1)) {
			print(i - (Pattern.length-1));		
		}
	}
}
\end{lstlisting}
Wie ersichtlich ist, beginnen wir damit, unser Muster unter Anwendung der Ränder an die erste Position zu verschieben, an der das erste Zeichen übereinstimmt. Nun inkrementieren wir beide Zählervariablen und betrachten folgende Zeichen, in dem wir im nächsten Schleifendurchlauf bei Abweichungen weiterverschieben, bis die Länge des Musters erreicht wurde. Dann geben wir diese Indexposition aus und fahren fort. 
\paragraph{Komplexität} 
Da dieser Algorithmus aus zwei Teilproblemen besteht, zerfällt auch die Komplexität: \\
Das Bestimmen der Ränder erfolgt linear für das gesamte Muster, daraus folgt in \textsc{Landau}-Notation: $\text{Rand}(x) \in O(n)$ bei $n$ als Länge des Musters.
Der Suchalgorithmus selbst verläuft ebenfalls linear -- insgesamt erfolgen maximal so viele Durchläufe wie der Text Zeichen enthält. Daraus folgt: $\text{KMPSearch}(x, y, R[]) \in O(m)$ \\
Da beide Teilprobleme getrennt voneinander und aufeinander aufbauend durchlaufen werden, ergibt sich eine Komplexität des gesamten Algorithmus von $O(n + m)$. \parencite{KMPRand}
\subsection{Boyer-Moore-Algorithmus}
Ähnlich dem Ansatz von \textsc{Knuth}, \textsc{Morris} und \textsc{Pratt} haben auch \textsc{Robert S. Boyer} und \textsc{J Strother Moore} einen vergleichbaren Algorithmus zur effizienteren Suche von Zeichenketten in Texten entwickelt. Im Gegensatz zum KMP-Algorithmus werden dort keine Ränder bestimmt, sondern erst im Falle einer Abweichung greifen zwei Heuristiken: Die \textbf{Bad-character}- und \textbf{Good-suffix}-Heuristik, mithilfe derer ebenfalls Sprünge ermittelt werden. Dieses Verfahren hat eine Komplexität von $O(n \cdot m)$ und ist damit im Allgemeinen (dem ungünstigsten Fall) weniger effizient als der KMP. Jedoch verläuft dieses Algorithmus nicht linear, sodass in einem günstigen Fall eine geringere Laufzeit erreicht werden kann. \\\\

Dieses Verfahren wird hier nicht näher dargestellt, da es nicht Teil der Vorlesung war. Weitere Informationen finden Sie bei \parencite[][S. 476 -- 478]{Grundkurs}.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Führen Sie eine binäre Suche nach dem Element \texttt{7} in \texttt{[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 17, 18, 19, 22]} durch. Notieren Sie jeweils Ihre Zwischenergebnisse. 
\begin{flushright}
Lösung auf Seite \pageref{a2.1:lsg}
\end{flushright}
\paragraph{Aufgabe 2} Bestimmen Sie die Ränder der folgenden Muster:
\begin{itemize}
\item \texttt{OTTOSMOPSKOTZT}
\item \texttt{ANANASBANANA}
\item \texttt{BONOBO}
\end{itemize}
Geben Sie jeweils an, wie sehr sich der Einsatz des KMP-Algorithmus bei diesen Mustern lohnt.
\begin{flushright}
Lösung auf Seite \pageref{a2.2:lsg}
\end{flushright}
\paragraph{Aufgabe 3} Wenden Sie den KMP-Algorithmus mit dem Muster \texttt{BONOBO} auf den Suchtext \texttt{BEINAHEBOTNOCHDASBONOBONOBOHAUSPLATZ}\footnote{Ja, der Text ist tatsächlich so gemeint, die Dopplung ist bewusst geschehen.} an. Stellen Sie Ihre Zwischenergebnisse dar.
\begin{flushright}
Lösung auf Seite \pageref{a2.3:lsg}
\end{flushright}
\paragraph{Aufgabe 4} Erläutern Sie den Algorithmus der binären Suche in Ihren eigenen Worten.
\begin{flushright}
Lösung auf Seite \pageref{a2.4:lsg}
\end{flushright}
\pagebreak
\part{Sortieralgorithmen}
Nach langen, qualvollen Stunden vor dem Bücherregal hassen Sie ihr Leben, ihr Studium und Sie beschließen, es wegzuwerfen -- Sie fangen in der Bibliothek als Fachkraft für Lagerlogistik an und sind nun für die korrekte Anordnung der Bücher im Regal verantwortlich. Auch hier zeigen Sie ihre geistige Brillianz, indem Sie das Problem des Sortierens auf abstrakter Ebene betrachten.
\section{Allgemeines und Eigenschaften}
\begin{definition}
Ein Sortieralgorithmus verarbeitet eine Folge von Elementen $e_{1}, ..., e_{n}$ so, dass am Ende eine Ausgabe $e_{1}, ..., e_{n}$ mit $e_{1} \leq e_{2} \leq ... \leq e_{n}$ erzeugt wird.
\end{definition} 
Als zugrunde liegende Datenstrukturen kommen Arrays, verkettete Listen oder weitere, spezielle Datenstrukturen mit zu Mengen/Relationen ähnlichem Aufbau infrage. \\\\
Da Sie bei Sortiervorgängen häufig Daten im \textit{{key: value}}-Format betrachten, unterscheiden wir zwischen \textbf{Schlüsseldaten}(also den keys) und \textbf{Satellitendaten}(den zugehörigen Werten). So können wie beispielsweise Wetterdaten betrachten: [{08.05.2021: 31.5}, {09.05.2021, 30.2}, {07.05.2021, 16.3}] -- Hier sind die Tage jeweils die Schlüsseldaten und die Temperaturangaben (die Gleitkommazahlen) jeweils Satellitendaten.
\begin{definition} Ein Algorithmus sortiert Daten \textbf{stabil}, falls die Reihenfolge bereits sortierter Elemente mit gleichem Schlüssel nicht geändert wird und so insbesondere nach einem weiteren Kriterium sortierte Daten nach Ausführung des Algorithmus in diesem Kriterium ihre Sortierung beibehalten. Als \textbf{instabil} bezeichnen wir einen Sortieralgorithmus, falls das Gegenteil der Fall ist.
\end{definition}
Ähnlich den Suchalgorithmen unterscheiden sich auch Sortieralgorithmen in weiteren Aspekten: 
\begin{itemize}
\item Laufzeiteffizienz ($\to$ \textsc{Landau}-Notation)
\item Speicherplatzeffizienz
\item Ablaufschema: Rekursiv oder Iterativ
\item Stabilität der Daten
\end{itemize}
Für die Speicherplatzeffizienz ergibt sich folgende Unterscheidung:
\begin{definition}
Falls ein Sortieralgorithmus parallel zur Laufzeit keinen oder einen konstanten zusätzlichen Speicherbedarf benötigt, so wird er \textbf{in-place} ausgeführt. Ein Algorithmus, der hingegen zusätzlichen Speicherbedarf in Abhängigkeit zu den Daten benötigt, wird als \textbf{out-place} bezeichnet.
\end{definition}
\section{Insertion Sort}
Der erste Sortieralgorithmus, den wir betrachten, heißt \textbf{Insertion Sort}. Wie der Name schon andeutet, geht es darum, Elemente einzufügen -- in einen bestehenden, bereits sortierten Teil. \\\\
Sie beginnen links in Ihrer Bücherreihe, starten mit dem zweiten Buch. Sie prüfen nun, ob es im Vergleich zum ersten richtig steht, andernfalls tauschen sie es. Nun betrachten Sie das dritte Buch und fügen es vor/zwischen/nach Buch 1 und 2 ein, je nach richtiger Position. So fahren Sie fort: Sie fügen das jeweils aktuelle Buch in die Reihe der bereits sortierten ein.
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
InsertionSort(Array) {
	int i = 0;
	while (i < (Array.length-1)) {
		x = Array[i];
		int y = 0;
		while (x < Array[i-y]) {
			y++;		
		} 
		Array[i-y] = x;
		i++;
	}
}
\end{lstlisting}
\begin{example} Zu sortieren Sie das Array \texttt{[44, 55, 12, 42, 94, 18, 06, 67]}. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
Ausgangswerte & 44 & \uline{55} & 12 & 42 & 94 & 18 & 06 & 67 \\
nach i = 1    & 44 & 55 & \uline{12} & 42 & 94 & 18 & 06 & 67 \\
nach i = 2    & 12 & 44 & 55 & \uline{42} & 94 & 18 & 06 & 67 \\
nach i = 3    & 12 & 42 & 44 & 55 & \uline{94} & 18 & 06 & 67 \\
nach i = 4    & 12 & 42 & 44 & 55 & 94 & \uline{18} & 06 & 67 \\
nach i = 5    & 12 & 18 & 42 & 44 & 55 & 94 & \uline{06} & 67 \\
nach i = 6    & 06 & 12 & 18 & 42 & 44 & 55 & 94 & \uline{67} \\
nach i = 7    & 06 & 12 & 18 & 42 & 44 & 55 & 67 & 94 \\
\hline
\end{tabular} \\
Quelle: \parencite[][S. 493 ff.]{Grundkurs}
\end{example}
\paragraph{Komplexität}
Im günstigsten Fall operiert dieses Verfahren auf einem bereits sortierten Feld. Dann sind -- um dies festzustellen -- $n-1$ Vergleiche erforderlich. Im schlechtesten Fall ist das Feld in genau falscher Richtung sortiert: Dann sind $\frac{n^{2}}{2}$ Vergleiche erforderlich, da die zweite Schleife auch maximal oft durchlaufen wird. \\\\
Insgesamt ergibt sich in \textsc{Landau}-Notation: $\in O(n^{2})$
\paragraph{Eigenschaften}
\begin{itemize}
\item Das Verfahren ist iterativ (erkennbar an den beiden Schleifen)
\item Es ist stabil, da Vertauschungen nur im notwendigen Fall auftreten
\item Das Verfahren operiert in-place, da kein bzw. nur konstanter zusätzlicher Speicher benötigt wird
\end{itemize}

Optimieren ließe sich das Verfahren durch den Einsatz von binärer Suche für die Einfügeposition -- dann reduzieren sich die Vergleiche, nicht jedoch die Verschiebungen.
\section{Bubblesort}
Mit \textbf{Bubble Sort} lernen wir einen weiteren Algorithmus zum Sortieren kennen: Hier vergleichen wir zunächst benachbarte Elemente und tauschen diese bei Bedarf. So durchlaufen wir das gesamte Array, bis keine Vertauschungen mehr notwendig sind und alle Elemente in der korrekten Reihenfolge angeordnet sind.
\paragraph{Idee} Durchlaufe Feld mehrmals und tausche benachbarte Elemente bei Bedarf. Terminiere, wenn in einem Durchlauf keine Vertauschung mehr erfolgt ist.
\begin{example}
Zu sortieren ist das Feld \texttt{[6, 8, 2, 9, 1, 5, 4, 3]}. \\
6 $<$ 8, aber 8 $>$ 2: \texttt{[6, 2, 8, 9, 1, 5, 4, 3]}. \\
8 $<$ 9, aber 9 $>$ 1: \texttt{[6, 2, 8, 1, 9, 5, 4, 3]} \\
9 $>$ 5: \texttt{[6, 2, 8, 1, 5, 9, 4, 3]} \\
9 $>$ 4: \texttt{[6, 2, 8, 1, 5, 4, 9, 3]} \\
9 $>$ 3: \texttt{[6, 2, 8, 1, 5, 4, 3, 9]} \\\\
Nächste Iteration: \\
6 $>$ 2: \texttt{[2, 6, 8, 1, 5, 4, 3, 9]} \\
6 $<$ 8, aber 8 $>$ 1: \texttt{[2, 6, 1, 8, 5, 4, 3, 9]} \\
8 $>$ 5: \texttt{[2, 6, 1, 5, 8, 4, 3, 9]} \\
8 $>$ 4: \texttt{[2, 6, 1, 5, 4, 8, 3, 9]} \\
8 $>$ 3: \texttt{[2, 6, 1, 5, 4, 3, 8, 9]} \\
8 $<$ 9. \\\\
Nächste Iteration:\\
2 $<$ 6, aber 6 $>$ 1: \texttt{[2, 1, 6, 5, 4, 3, 8, 9]} \\
6 $>$ 5: \texttt{[2, 1, 5, 6, 4, 3, 8, 9]} \\
6 $>$ 4: \texttt{[2, 1, 5, 4, 6, 3, 8, 9]} \\
6 $>$ 3: \texttt{[2, 1, 5, 4, 3, 6, 8, 9]} \\
6 $<$ 8 und 8 $<$ 9. \\\\
Nächste Iteration:\\
2 $>$ 1: \texttt{[1, 2, 5, 4, 3, 6, 8, 9]} \\
2 $<$ 5, aber 5 $>$ 4: \texttt{[1, 2, 4, 5, 3, 6, 8, 9]} \\
5 $>$ 3: \texttt{[1, 2, 4, 3, 5, 6, 8, 9]} \\
5 $<$ 6, 6 $<$ 8 und 8 $<$ 9. \\\\
Nächste Iteration: \\
1 $<$ 2, 2 $<$ 4, aber 4 $>$ 3: \texttt{[1, 2, 3, 4, 5, 6, 8, 9]} \\
4 $<$ 5, 5 $<$ 6, 6 $<$ 8 und 8 $<$ 9. Terminiert.
\end{example}
Aus dem Beispiel können wir ableiten, dass in jeder Iteration $n-1$ Vergleiche erfolgen. Dies ist ein Indiz für die Komplexität, die wir später betrachten möchten.
\paragraph{Implementation} als Pseudo-Code
\begin{lstlisting}
BubbleSort(Array) {
	int i = 0;
	int j = 0;
	bool SwapFlag = True;
	while (SwapFlag == true) {
		SwapFlag = false;
		while (i < (Array.length-1) {	
			if (Array[i] > Array[i+1]) {
				swap(Array[i], Array[i+1]);
				SwapFlag = true;			
			}
		}	
	}
}
\end{lstlisting}
Die \texttt{SwapFlag} ist Terminationskriterium: Falls in einer Iteration kein Tausch mehr durchgeführt wurde, ist das Array vollständig sortiert.
\paragraph{Komplexität}
Im günstigsten Fall ist das Array bereits vollständig sortiert, dann werden $n-1$ Vergleiche notwendig. Im ungünstigsten Fall ist das Aray absteigend sortiert, nun sind $\frac{n^{2}}{2}$ Vergleiche erforderlich. \\\\
Somit ergibt sich erneut eine Komplexität von $O(n^{2})$. \\\\
Im Gegensatz zu ursprünglichen Implementierungen haben wir hier bereits eine Optimierung vorgenommen: Die SwapFlag bewirkt, dass im günstigsten Fall $O(n)$ die passende Komplexitätsklasse ist.
\paragraph{Eigenschaften}
\begin{itemize}
\item Auch Bubble Sort arbeitet offensichtlich iterativ.
\item Bubble Sort ist ebenfalls stabil.
\item Der Speicheraufwand ist in-place.
\end{itemize}
Quelle und weiterführende Literatur: \parencite[][S.86 ff.]{Wirth} \parencite[][S.22]{Taschenbuch}
\section{Mergesort}
\label{mergesort}
Die nächste Idee, wie Sie Ihr Bücherregal sortieren können: Mergesort! Sie teilen das Regal in zwei Hälften, sortieren jede Hälfte, in dem Sie diese wiederum halbieren, bis die Hälften aus einem Buch bestehen. Sortieren Sie diesen Teil, indem Sie die Bücher richtig anordnen und die Hälften so wieder von klein nach groß nach einer Art Reißverschlussprinzip verbinden, bis Sie die Gesamtheit der Elemente in sortierter Reihenfolge erhalten. 
\begin{example} Zu sortieren: \texttt{[1, 7, 6, 2, 9, 4, 3, 8, 10, 5]}.\\
Halbieren: \\\texttt{[1, 7, 6, 2, 9] | [4, 3, 8, 10, 5]} \\
\texttt{[1, 7, 6][2, 9] | [4, 3, 8][10, 5]} \\
\texttt{[1, 7][6][2][9] | [4, 3] [8][10][5]} \\
\texttt{[1][7][6][2][9] | [4][3][8][10][5]}\\\\
Sortieren (nach rekursiven Aufrufen): \\
\texttt{[1][2][6][7][9] | [3][4][5][8][10]} \\\\
Merge: \\
\texttt{[1][2][3][4][5][6][7][8][9][10]}
\end{example}
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
MergeSort(int A[], int al, int ar) {
	if (ar>al) {
		int m = (ar+al) / 2;
		MergeSort(A[], al, m);
		MergeSort(A[], m+1, ar);
		
		int B[] = new int[ar-al+a];
		merge(A, al, m, a, m+1, ar, B);
		
		for (int i = 0; i < ar-al+1; i++) {
			A{al+1] = B[i];		
		}
	}
}
merge(A[], B[], al, ar, bl, br, C[]) {
	int i = al;
	int j = bl;
	for (int k = 0; k <= ar-al+br-bl+1; k++) {
		if (i > ar) {
			C[k] = B[j+1];
			continue;		
		}
		if (j > br) {
			C[k] = A[i+1];
			continue;		
		}
		C[k] = (A[i] < B[j] ? A[i++] : B[j++];	
	}	
}
\end{lstlisting} Quelle: \parencite[][S. 28]{Taschenbuch}
\paragraph{Komplexität}
Für Mergesort ergibt sich -- da es sich um einen rekursiven Algorithmus handelt -- eine Rekursionsgleichung:
\[T(n) = T(\frac{n}{2}) + T(\frac{n}{2}) + n = 2T(\frac{n}{2}) + n\]
Jeder Rekursionsteil beschreibt die Komplexität der einzelnen Hälften und $n$ das anschließende Zusammenfügen. Somit ergibt sich unter Anwendung des Mastertheorems eine Komplexität von 
\[T(n) \in O(n \cdot log(n))\]
\paragraph{Eigenschaften}
\begin{itemize}
\item rekursiv
\item stabil
\item Je nach Implementierung in-place oder out-place
\end{itemize}
\section{Quicksort}
Willkommen in der Königksklasse: Quicksort trägt schon einen bezeichnenden, etwas anmaßenden Titel. Wir betrachten nun, ob der Algorithmus dem gerecht wird.
\paragraph{Idee}
Wir sortieren eine Menge an Elementen, in dem wir das erste Element $p$ (das wir als \textbf{Pivot} bezeichnen) herausgreifen und nun unser Feld/Array so verschieben, dass alle kleineren Elemente ${...} < p$ links von $p$ stehen und alle größeren rechts davon - wohlgemerkt noch nicht sortiert. Nun betrachten wir die neu gebildeten Abschnitte $(a_{0}, ..., a_{p-1})$ und $a_{p+1}, ..., a_{n})$ und verfahren auf diesen Teilstapeln oder Teilmengen rekursiv. Folglich betrachten wir erneut die jeweils ersten Elemente und verfahren ebenso. Da wir die Operationen in-place ausführen, ist kein merge-Schritt notwendig (im Gegensatz zu MergeSort).
\begin{example} Wir betrachten ein Feld \texttt{[8, 17, 28, 15, 11, 1, 3, 20, 25, 6, 12, 5]}. \\
Das erste Element ist folglich 8, also erhalten wir: \\
\texttt{[1, 3, 6, 5, 8, 17, 28, 15, 11, 20, 25, 12]}.\\
Wir betrachten nun die beiden Teilmengen \texttt{[1, 3, 6, 5]} und \texttt{[17, 28, 15, 11, 20, 25, 12]}. \\
Da 1 offensichtlich kleinstes Element der ersten Teilmenge ist, 3 anschließend ebenfalls beim nächsten Aufruf, erhalten wir schnell \texttt{[1, 3, 5, 6]}. \\\\
In der zweiten Menge erhalten wir \texttt{[15, 11, 12]} und \texttt{[28, 20, 25]}, die wir schnell zu \texttt{[11, 12, 15]} und \texttt{[20, 25, 28]}, zusammen \texttt{[11, 12, 15, 17, 20, 25, 28]}. \\\\
Abschließend erhalten wir das sortierte Feld \texttt{[1, 3, 5, 6, 8, 11, 12, 15, 17, 20, 25, 28]}.
\end{example}
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
QuickSort(Array, lo, hi) {
	if (lo < hi) {
		pivot = Array[lo];
		i = lo;
		j = hi+1;	
	}
	while (true) {
		while (Array[++i] < pivot && i < hi) {
					
		}
		while (Array[--j] > pivot && j > lo) {
		
		}
		if (i < j) {
			swap(Array, i, j);		
		}	
		else {
			break;		
		}
	}
	swap(Array, j, lo);
	
	quickSort(Array, lo, j-1);
	quickSort(Array, j+1, hi);
}
swap(Array, i, j) {
	t = Array[i];
	Array[i] = Array[j];
	Array[j] = t;
}
\end{lstlisting} abgewandelt nach \parencite[][S. 29]{Taschenbuch}
\paragraph{Komplexität}
Offensichtlich handelt es sich bei Quicksort ebenfalls um einen rekursiven Sortieralgorithmus. Im günstigsten Fall ergibt sich eine Komplexität von $O(n \cdot log(n))$, da dann das Pivot-Element genau der Median ist und so beide Hälften glatt rekursiv abgearbeitet werden können. Im schlechtesten Fall beträgt die Komplexität $O(n^{2})$, da dann das Pivot-Element jeweils das kleinste oder größte ist. \\\\
Wir geben die Komplexität von Quicksort im Durchschnittsfall mit $O(n \cdot log(n))$ an, behalten allerdings im Kopf, dass in einem ungünstigen Fall eine deutlich schlechtere Laufzeit in Kauf genommen werden muss.
\paragraph{Eigenschaften}
\begin{itemize}
\item rekursiv
\item nicht stabil
\item in-place, da kein Hilfsfeld benötigt wird
\item optimierbar beispielsweise durch Einfügen einer Schranke, bis zu der Insertion Sort genutzt wird (z.B. bei einer geringen zweistelligen Anzahl von Elementen) oder durch 3-Way-Partitioning, bei dem neben $<, >$ auch der Fall $=$ betrachtet wird. Dies ist logischerweise nur dann sinnvoll, falls es mehrere Elemente mit gleichen Schlüsseln gibt und diese bevorzugt als Pivot-Element genutzt werden.
\end{itemize}
\section{Weitere Sortieralgorithmen}
Auf weitere Sortieralgorithmen wie z.B. Selection Sort \parencite{Taschenbuch}, Shellsort oder Shakesort \parencite{Wirth} und \parencite{Grundkurs} kann hier nicht näher eingegangen werden. In genannten Büchern finden Sie nähere Informationen zu diesen Algorithmen.
\section{Allgemeine Komplexitätsbetrachtung von Sortieralgorithmen}
\begin{satz}
Ein Sortieralgorithmus, der auf Vergleichen von Datenelementen beruht, hat mindestens eine Komplexität von $O(n \cdot log(n))$.
\end{satz}
Um diesen Satz zu verstehen, verdeutlichen wir uns zunächst, was eine Sortierung im Allgemeinen bedeutet: \\
Aus der Menge aller möglichen Permutationen (also Anordnungen) von den Elementen unserer Menge suchen wir genau die eine heraus, in der alle Elemente \textit{sortiert} sind, nach unserem Kriterium. Insgesamt gibt es bei $n$ Elementen $n!$ Vergleiche. \\
\paragraph{Beweisidee}
Wir betrachten die Menge aller Permutationen, ordnen ihnen einen Wert zu und vergleichen nun jede Permutation mit der gewünschten, also derjenigen, in dem die Elemente vollständig sortiert sind. Nun können wir eine Hälfte an Permutationen ausmachen, deren Wert {\glqq}kleiner{\grqq} ist und eine weitere Hälfte, deren Wert {\glqq}größer oder gleich{\grqq} der Permutation ist. Hier sind nun $\frac{n!}{2}$ Vergleiche erforderlich. \\
Wir halbieren die Hälften fortlaufend und erhalten im letzten Schritt die eine verbleibende Permutation. \\
Sie ist nach $log(n!)$ Vergleichen erreicht.
\[O(log(n!)) \in O(n \cdot log(n))\]
\begin{proof}
\begin{align}
n^n \geq n! \\
1 \cdot 2 \cdot 3 \cdot ... \cdot (n-1) \cdot n = n \cdot (n-1) \cdot ... \cdot 3 \cdot 2 \cdot 1 \\
(n-k) \cdot (k+1) \geq n \quad \forall 0 \leq k \leq n \\
(n-1) \cdot ((n-1) \cdot 2) \cdot ... \cdot ((n-k) \cdot (k+1)) \cdot (1\cdot n) \geq n \cdot n \cdot ... \cdot n \\
n! \cdot n! \geq n^n \\
(1) \text{ und } (4): \quad \quad n^{2n} \geq n! \cdot n! \geq n^{n} \\
2n \cdot log(n) \geq 2 \cdot log(n!) \geq n \cdot log(n) \\
O(log(n!)) = O(n \cdot log(n))
\end{align}
\end{proof}
Tatsächlich ist damit gezeigt, dass es keinen Sortieralgorithmus gibt, dessen Komplexität geringer ist als $O(n \cdot log(n))$.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Sortieren Sie das Feld \texttt{[56, 10, 15, 98, 99, 12, 30, 80]} aufsteigend. Notieren Sie Zwischenschritte
\begin{itemize}
\item Insertion Sort
\item Bubblesort
\item Quicksort
\item Mergesort
\end{itemize}
\begin{flushright}
Lösung auf Seite \pageref{a3.1:lsg} \\
entnommen aus \parencite{GrUeb}
\end{flushright}
\paragraph{Aufgabe 2} 
Beschreiben Sie Quicksort mit eigenen Worten.
\begin{flushright}
Lösung auf Seite \pageref{a3.2:lsg}
\end{flushright}
\paragraph{Aufgabe 3} 
Begründen Sie, mit welchen Verfahren Sie folgende Felder in optimaler Laufzeit aufsteigend sortieren können.
\begin{itemize}
\item \texttt{[1, 9, 2, 8, 3, 7, 4, 6, 5]}
\item \texttt{[9, 7, 7, 2, 5, 4, 7, 3, 1]}
\item \texttt{[9, 8, 7, 6, 5, 4, 3, 2, 1]}
\end{itemize}
\begin{flushright}
Lösung auf Seite \pageref{a3.3:lsg}
\end{flushright}
\paragraph{Aufgabe 4}
Entwickeln Sie für Mergesort und Quicksort eine anschauliche Darstellungsform.
\begin{flushright}
Lösung auf Seite \pageref{a3.4:lsg}
\end{flushright}
\pagebreak
\part{Bäume}
Vielleicht haben Sie die letzte Aufgabe vom letzten Teil gelöst -- Mit relativ hoher Wahrscheinlichkeit werden Sie eine Struktur beschrieben oder skizziert haben, die wir im Folgenden näher betrachten: Bäume! \\\\
Damit Sie vor lauter Bäumen den Wald (und die Wahrheit) nicht aus den Augen verlieren, widmen wir uns in aller Ausführlichkeit diesem in der Informatik sehr häufig anzutreffenden Gebilde, denken Sie beispielsweise an Ableitungs-, Entscheidungs-, Syntax- oder Codebäume.
\section{Der Wurzelbaum}
\begin{definition}
Ein Wurzelbaum $B = (V, E, r)$ besteht aus einer endlichen Menge von Konten $V$, einer endlichen Menge von gerichteten Kanten $E \subset V \times V$ und aus der Wurzel $r \in V$. \\
\begin{itemize}
\item Ein Knoten $r$ ist ein Wurzelbaum ($B = (\{r\}, \emptyset, r)$). 
\item Sind $B_{1} = (V_{1}, E_{1}), ..., B_{k} = (V_{k}, E_{k})$ Bäume mit den Wurzeln $r_{1}, ..., r_{k}$, so erweitern wir die Knotenmenge $V$ um eine neue Wurzel $r$ und die Kantenmenge $E$ um die Kanten $(r, r_{i}), i = 1, ... k$.\\\\
Der Baum
\[(V_{1} \cup ... \cup V_{n} \cup \{r\}, \{(r, r_{i}) \mid i = 1, ..., k\} \cup E_{1} \cup ... \cup E_{k}, r)\]
ist ein Wurzelbaum.
\end{itemize}
\end{definition}

Folglich betrachten wir hier nun also einen Baum mit einem Knoten, der den Ursprung darstellt, aus dem sich die anderen Knoten ableiten. In etwa so: \\
\begin{example}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt]
 \node {8}
    child {node {2}}
    child {node {32}
      child {node {16}}
      child {node {64}}
    };
\end{tikzpicture}
\caption{Wurzelbaum}
\end{figure}
\end{example}
\begin{definition}
Ein Baum, der keine Knoten und Kanten besitzt, heißt \textbf{leerer Baum}.
\end{definition}
Zu diesen Bäumen, von denen es die unterschiedlichsten Formen gibt, betrachten wir nun eine Reihe von Eigenschaften und Begrifflichkeiten.
\begin{definition}
\begin{itemize}
\item Ist $e = (v, w) \in E$, so heißt $v$ \textbf{Vater} von $w$ und $w$ \textbf{Sohn} oder \textbf{Kind} von $v$. Ein Knoten, der keine Söhne hat, heißt \textbf{Blatt}.
\item Ein Pfad $P$ in $B$ ist eine Folge von Knoten $v_{0}, ..., v_{n}$ mit $(v_{i}, v_{i+1}) \in E, i = 0, ..., n-1. n$ heißt \textbf{Länge} von $P$.
\item Seien $v, w \in V$. Der Knoten $w$ heißt \textbf{von Knoten $v$ aus erreichbar}, falls es einen Pfad $P$ von $v$ nach $w$ gibt.
\item Jeden Knoten $v$ von $B$ können wir als Wurzel des Teilbaums der von $v$ aus erreichbaren Knoten betrachten. \\\\ Hat $v$ die Söhne $v_{i}, ..., v_{k}$, so heißen die Bäume $B_{1}, ..., B_{k}$ mit den Wurzeln $v_{i}, ..., v_{k}$ die \textbf{Teilbäume} von $V$.
\item Die \textbf{Höhe} eines Knotens $v$ ist das Maximum der Längen aller Pfade, die in $v$ beginnen.
\item Die \textbf{Tiefe} eines Knotens ist die Länge des Pfades von der Wurzel zu $v$. \\\\
Die Knoten der Tiefe $i$ bilden die $i$-te Ebene des Baumes.
\item Die Höhe (und auch Tiefe) eines Baumes (nicht Knotens!) ist gegeben durch die Höhe der Wurzel.
\item Ein leerer Baum besitzt die Höhe/Tiefe $-1$.
\item Die \textbf{Ordnung} eines Baumes ist die maximale Anzahl an Söhnen eines Knotens.
\end{itemize}
\end{definition}
Zur Veranschaulichung wenden wir diese Begriffe auf den folgenden Beispielbaum an: \\
\begin{example}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {50}
    child {node {25}
	  child {node {12.5}
		child {node {6.25}}	    
	  }	  
	  child {node {37.5}}    
    }
    child {node {75}
      child {node {62.5}}
      child {node {87.5}}
    };
\end{tikzpicture}
\caption{Beispielbaum}
\end{figure}
\begin{itemize}
\item 50 ist die Wurzel des Baumes.
\item 6.25 ist Sohn/Kind von 12.5 und Blatt, da es keine Söhne hat.
\item 12.5 ist erreichbar von 50, da es einen Pfad gibt. Generell ist jedes Element eines Wurzelbaums von der Wurzel aus erreichbar.
\item 87.5 ist von 6.25 aus nicht erreichbar, da es keinen Pfad gibt.
\item Die Höhe von 75 beträgt 1. Die Tiefe von 6.25 beträgt 3.
\item Die Höhe/Tiefe des Baumes beträgt 3.
\item Die Ordnung des Baumes ist 2, da kein Knoten mehr als zwei Kinder hat.
\end{itemize}
\end{example}
\section{Binäre Bäume}
\begin{definition}
Ein Baum mit der Ordnung 2 heißt \textbf{binärer Baum}. Liegen die Elemente darin in geordneter Form vor, d.h. der Wert des linken Sohnes ist stets kleiner und der Wert des rechten Sohnes ist stets größer als sein Vater, spricht man von einem \textbf{geordneten binären Baum} \parencite{Wirth} oder \textbf{binären Suchbaum}.
\end{definition}
Diese besonderen Bäume wollen wir nun näher betrachten, da wir in binären Bäumen algorithmisch vorgehen können und so beispielsweise Suchprobleme lösen.
\begin{note}
Sei $n$ die Anzahl der Knoten in einem binären Baum der Höhe $h$. Dann ist die Anzahl der Knoten $n \leq 2^{n+1}-1$ oder äquivalent dazu. Die Höhe $h$ ist mindestens $log_{2}(n+1)-1$, also $\lceil log_{2}(n+1) \rceil -1 \leq h$. Diese Schranke wird für einen binären Baum, in dem alle Ebenen vollständig besetzt sind, angenommen.
\end{note}
\paragraph{Implementierung}
Wie schon eingangs erwähnt, hängen Bäume und Rekursion naturgemäß eng beieinander. Dies zeigt sich, wenn wir eine Implementierung eines binären Baums betrachten:
\begin{lstlisting}
struct node {
	int key;
	type value;
	node left, right;
}
\end{lstlisting}
Ein Knoten besteht also wiederum selbst aus Knoten, im Falle eines Blattes sind \texttt{left} und \texttt{right} dann leer. Wichtig ist die Unterscheidung zwischen \texttt{key} und \texttt{value} -- hier kann es leicht zu Unklarheiten kommen.
\subsection{Traversierung von Binärbäumen}
Auf Baumstrukturen lassen sich verschiedene Operationen durchführen. Grundlage dafür ist häufig ein \textit{traversieren} ({\glqq}durchlaufen{\grqq}) des Baumes. Dafür gibt es drei, jeweils rekursiv implementierte Vorgehensweisen, die unter anderem bei \parencite{Wirth} beschrieben werden: 
\begin{itemize}
\item \textbf{Preorder} -- W, A, B (besuche Wurzel vor den Teilbäumen)
\item \textbf{Inorder} -- A, W, B 
\item \textbf{Postorder} -- A, B, W (besuche Wurzel nach den Teilbäumen)
\end{itemize}
Grundlage dafür ist der allgemeine binäre Baum:
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
 \node {W}
    child {node {A}}
    child {node {B}};
\end{tikzpicture}
\caption{Allgemeiner binärer Baum}
\end{figure}
\begin{example}
Betrachten wir den Baum aus vorangegangenem Beispiel und traversieren wir diesen in allen drei Varianten: \\\\
\textbf{Preorder} \\
\texttt{50, 25, 12.5, 6.25, 37.5, 75, 63.5, 87.5} \\\\
\textbf{Inorder} \\
\texttt{6.25, 12.5, 25, 37.5, 50, 63.5, 75, 87.5} \\\\
\textbf{Postorder} \\
\texttt{6.25, 37.5, 12.5, 25, 63.5, 87.5, 75, 50}
\end{example}
Neben den drei genannten Vorgehenweisen betrachten wir noch die klassische, den Baum in seinen Ebenen ausgebende: \textbf{levelorder}. Auf unser Beispiel bezogen wäre dies:
\texttt{50, 25, 75, 12.5, 37.5, 63.5, 87.5, 6.25}
\subsection{Operationen}
Auf binären Bäumen lassen sich nun drei Operationen konstruieren: \textbf{Suchen, Einfügen} und \textbf{Löschen}.
\paragraph{Suchen}
\label{searchBinTree}
Ein Baum wird durchsucht, in dem jeweils das Suchelement mit dem Wert des Knotens verglichen und anschließend mit dem linken Sohn, falls das gesuchte Element kleiner ist, oder dem rechten Sohn, falls das gesuchte Element größer ist, als neuen Knoten rekursiv fortgefahren.
\begin{lstlisting}
node Search(int k, node Tree) {
	if (Tree == null) {
		return null;	
	}
	else if (k == Tree.key) {
		return Tree;	
	}
	else if (k < Tree.key) {
		Search(k, Tree.left);	
	}
	else if (k > Tree.key) {
		Search(k, Tree.right);	
	}
}
\end{lstlisting}
\paragraph{Einfügen}
Ähnlich läuft das Einfügen eines Elements $k$ mit Wert $v$: 
\begin{lstlisting}
node Insert(int k, node Tree, type v) {
	if (T == null) {
		return "neuer Knoten (k, v)";	
	}
	else if (k < T.key) {
		T.left =	 Insert(k, T.left, v);
	}
	else if (k > T.key) {
		T.right = Insert(k, T.right, v);	
	}
	else if (k == T.key) {
		T.value = v;
	}
	return T;
}
\end{lstlisting}
Ein Einfügen kann folglich auch ein {\glqq}Wert ändern{\grqq} sein.
\paragraph{Löschen}
Das Löschen eines Knotens gestaltet sich etwas aufwendiger. \\
Suche Zeiger T auf den zu löschenden Schlüssel/Knoten und unterscheide:
\begin{itemize}
\item Das zu löschende Element hat keine Kinder: Setze Zeiger T auf null
\item Das zu löschende Element hat ein Kind: Ersetze T durch den Zeiger auf den Kindknoten
\item Das zu löschende Element hat zwei Kinder: Ersetze den gelöschten Knoten durch den kleinsten Schlüssel des rechten Teilbaums.
\end{itemize}
Eine Implementierung davon finden Sie bei \parencite{Wirth}.
\paragraph{Komplexität}
Für die Suche und das Löschen liegt die Komplexität im Durchschnittsfall in $O(n)$, wobei $n$ die Höhe des Baumes ist. Unter günstigen Umständen ist auch eine Komplexität von $O(log(n))$ möglich. Die Komplexität des Einfügens liegt in $O(1)$.
\section{AVL-Bäume}
Ein weiterer Typ von Bäumen sind die sogenannten \textbf{AVL-Bäume}, die durch einen geringen zusätzlichen Aufwand beim Einfügen/Löschen erzeugt werden und eine günstigere Komplexität besitzen.
\begin{definition}
Ein binärer Baum heißt \textbf{(AVL-)ausgeglichen}, falls für jeden Konten $v$ gilt: \\\\
Die Höhen des linken und rechten Teilbaums unterscheiden sich höchstens um 1. \\Diese Differenz der Höhen heißt \textbf{Balance} von $v$.
\[\text{Balance} = h(rT) - h(lT)\]
Ausgeglichene, binäre Suchbäume heißen nun \textbf{AVL-Bäume}.
\end{definition}
\begin{example}
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {7}
    child {node {2}
    child {node {1}}
    child {node {5}}}
    child {node {8}
    child[fill=none] {edge from parent[draw=none]}
    child {node {9}}};
\end{tikzpicture}
\caption{Beispiel für einen AVL-Baum}
\end{figure}
Dieser Baum ist ein AVL-Baum, da er die drei Kriterien erfüllt:
\begin{itemize}
\item binär: Alle Knoten haben höchstens zwei Kinder, mindestens ein Knoten hat mindestens zwei Kinder
\item Suchbaum: Alle Elemente im linken Teilbaum sind kleiner als das Wurzelelement des Teilbaums, alle Elemente im rechten Teilbaum sind größer als der Teilbaum
\item AVL-Kriterium: Die Balancen der Teilbäume unterscheiden sich höchstens um 1.
\end{itemize}
Kein AVL-Baum hingegen ist der folgende:
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {E}
    child {node {A}
    child[fill=none] {edge from parent[draw=none]}
    child {node {C}
    child {node {B}}
    child {node {D}}}}
    child {node {F}
    child[fill=none] {edge from parent[draw=none]}
    child {node {H}
    child {node {G}}
    child[fill=none] {edge from parent[draw=none]}}};
\end{tikzpicture}
\caption{Kein Beispiel für einen AVL-Baum}
\end{figure}
Hier ist die AVL-Bedingung offensichtlich verletzt: Die Balance an den Knoten A und F ist jeweils größer als 1. Damit handelt es sich nicht um einen AVL-Baum.
\end{example}
Um zu ermitteln, ob es sich bei einem Baum um einen AVL-Baum handelt, prüfen wir also zunächst, ob es sich um einen binären Suchbaum handelt. Nun bestimmen wir zu jedem Knoten die Balance: Jedes Blatt hat die Balance 0, falls der rechte Teilbaum eine größere Höhe hat, ist die Balance positiv. Falls der linke Teilbaum eine größere Höhe hat, ist die Balance negativ. Eine Balance von 1 / -1 wird toleriert, alle größeren Balancen führen dazu, dass der Baum kein AVL-Baum sein kann. Ein Knoten mit Balance größer 1 genügt.
\begin{note}
Für die Höhe eines ausgeglichenen Baumes mit $n$ Knoten gilt:
\[h < 1.45 \cdot log_2(n+2) - 1.33\]
Dieses Kriterium eignet sich aus naheliegenden Gründen lediglich für eine Abschätzung. 
\end{note}
\paragraph{Suche}
Da ein AVL-Baum ein binärer Suchbaum ist, verwenden wir zum Suchen die Suchfunktion eines binären Suchbaums. Das AVL-Kriterium hat keine Auswirkungen auf die Komplexität.
\paragraph{Einfügen}
Wir suchen nach dem einzufügenden Element $e$. Falls $e$ nicht im Baum, endet die Suche in einem Blatt. An diesem Blatt verankern wir einen neuen Knoten und füllen ihn mit $e$. \textbf{Hierbei kann die AVL-Bedingung verletzt werden!} \\
Anschließend reorganisieren wir den Baum, um die AVL-Bedingung wiederherzustellen:
\begin{itemize}
\item Prüfe für jeden Knoten $n$ des Suchpfades, ob er ausgeglichen ist.
\item Falls nicht: Rotation
\end{itemize}
\paragraph{Rotation}
Rotation können wir uns vorstellen als eine Art {\glqq}Kippen{\grqq} eines Knotens, um eine Balance wiederherzustellen bzw. zu nivellieren. Wir unterscheiden zwei Rotationsarten:
\begin{itemize}
\item \textbf{Linksrotation (LR)} Ein rechtes Kind kann nach links rotiert werden.
\item \textbf{Rechtsrotation (RR)} Ein linkes Kind kann nach rechts rotiert werden.
\end{itemize}
Rotation ist möglich, da ein vormals linkes Kind -- das kleiner als die Wurzel ist -- auch die neue Wurzel mit der vorherigen Wurzel als neues rechtes Kind -- das dann weiterhin größer ist -- sein kann. Im Prinzip ändert sich folglich jeweils nur die Verwandtschaftsbeziehung zwischen Kind und Wurzel mit dem Effekt, dass auch die Kinder neu, meistens paritätischer, angeordnet werden.
\begin{example}
\begin{figure}[h!]
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {7}
    child {node {2}}
    child {node {42}
    child {node[fill=yellow] {24}
    child {node {15}}}
    child {node {72}}};
\hspace{4cm} $\text{RR(24)} \Rightarrow$ \hspace{4cm}
\node {7}
    child {node {2}}
    child {node[fill=yellow] {24}
    child {node {15}}
    child {node {42}
    child {node {72}}}};
\end{tikzpicture}
\caption{AVL-Baum mit Rechtsrotation des Knotens 24}
\end{figure}
Der hier vorliegende Baum wurde am Knoten 24 nach rechts rotiert. Dadurch ist die 24 die neue Wurzel geworden, während die 42 nun eine Ebene tiefer ein Kind geworden ist. Doch auch nach dieser Rotation zeigt sich, dass die AVL-Bedingung weiterhin nicht erfüllt ist. Also ist ein weiterer Rotationsschritt notwendig:
\begin{figure}[h!]
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {7}
    child {node {2}}
    child {node[fill=yellow] {24}
    child {node {15}}
    child {node {42}
    child {node {72}}}};
\hspace{4cm} $\text{LR(24)} \Rightarrow$ \hspace{4cm}
\node[fill=yellow] {24}
    child {node {7}
    child {node {2}}
    child {node {15}}}
    child {node {42}
    child[fill=none] {edge from parent[draw=none]}
    child {node {72}}};
\end{tikzpicture}
\caption{AVL-Baum mit Linksrotation des Knotens 24}
\end{figure}
Nun zeigt sich, dass die AVL-Bedingung wieder erfüllt ist.
\end{example}
Aus dem Beispiel können wir nun die Fälle für Rotationen unterscheiden:
\begin{itemize}
\item Eine einfache Rotation wird dann notwendig, wenn
\begin{itemize}
\item in einem linken Teilbaum ein linkes Kind eingefügt wird  $\to$ einfache Rechtsrotation
\item in einem rechten Teilbaum ein rechtes Kind eingefügt wird $\to$ einfache Linksrotation
\end{itemize}
\item Eine zweifache Rotation wird hingegen angewandt, falls
\begin{itemize}
\item in einem linken Teilbaum ein rechtes Kind eingefügt wird $\to$ Links-Rechts-Rotation
\item in einem rechten Teilbaum ein linkes Kind eingefügt wird $\to$ Rechts-Links-Rotation
\end{itemize}
\end{itemize}
\paragraph{Löschen}
Zunächst wird wie in einem binären Suchbaum vorgegangen. Anschließend muss -- wie beim Einfügen -- die AVL-Bedingung durch Rotationen wiederhergestellt werden. \\\\
Für die Betrachtung der Komplexität ergibt sich nun:
\begin{theorem}
Die Operationen Suchen, Einfügen und Löschen eines Schlüssels können in $O(log (n))$ durchgeführt werden.
\end{theorem}
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Gegeben ist der folgende Baum.
\begin{itemize}
\item Geben Sie folgende Eigenschaften an: Wurzel, Höhe des Baumes, Höhe des Knotens $E$, Ordnung des Baumes, Balance des Knotens $K$
\item Fügen Sie den Knoten $J$ hinzu.
\item Löschen Sie den Knoten $B$.
\end{itemize}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {F}
    child {node {B}
    child {node {A}}
    child {node {D}
    child {node {C}}
    child {node {E}}}}
    child {node {K}
    child {node {H}}
    child {node {T}
    child[fill=none] {edge from parent[draw=none]}
    child {node {W}}}};
\end{tikzpicture}
\end{figure}
\begin{flushright}
Lösung auf Seite \pageref{a4.1:lsg} \\
\end{flushright}
\paragraph{Aufgabe 2}
Untersuchen und begründen Sie, ob es sich bei diesem Baum (wie gegeben, vor Aufgabe 1) um einen AVL-Baum handelt. Fügen Sie hierzu die Balancen jedes Knotens ein. 
\begin{flushright}
Lösung auf Seite \pageref{a4.2:lsg}
\end{flushright}
\paragraph{Aufgabe 3}
Falls es sich nicht um einen AVL-Baum handelt, ergänzen Sie den Baum zu einem AVL-Baum und führen Sie dann folgende Aufgabe aus.
\begin{itemize}
\item Löschen Sie den Knoten $A$.
\item Fügen Sie den Knoten $Z$ hinzu. 
\end{itemize}
Falls Rotationen erforderlich sind, stellen Sie diese detailliert dar. 
\begin{flushright}
Lösung auf Seite \pageref{a4.3:lsg}
\end{flushright}
\paragraph{Aufgabe 4}
Beschreiben Sie in Worten und Pseudocode das Vorgehen beim Suchen eines Elementes in einem binären Baum. Beurteilen Sie, ob dieses Verfahren auch in einem Baum der Ordnung 3 angewendet werden kann.
\begin{flushright}
Lösung auf Seite \pageref{a4.4:lsg}
\end{flushright}
\pagebreak
\part{Heaps}
Eigentlich auch zu Bäumen gehörend, betrachten wir im Folgenden die Datenstruktur \textbf{Heap}. Als Heap bezeichnen wir eine Struktur (einen {\glqq}Haufen{\grqq}) von Objekten, die über Schlüssel mit einer Priorität versehen sind. Wirth definiert Heaps in \parencite[][S. 92]{Wirth} wie folgt:
\begin{definition}
Ein heap ist definiert als eine Folge von Schlüsseln h[l], h[l+1], ..., h[r] mit den Relationen:
\begin{itemize}
\item h[i] $\leq$ h[2i]
\item h[i] $\leq$ h[2i+1] $\forall i = 1 ... r/2$
\end{itemize}
\end{definition}
Betrachten wir einen Heap als Baum, so ist $h[2i]$ das linke Kind des Knotens $h[i]$ und $h[2i+1]$ entsprechend das rechte. Somit muss jede Wurzel eines Teilbaums kleiner als ihre Kinder sein. Diese Struktur bezeichnen wir als \textbf{Min-Heap}. Invers dazu bezeichnen wir Heaps, in denen die Wurzel eines Teilbaums jeweils größer ist als ihre Kinder als \textbf{Max-Heap}. Beide Varianten finden sich in der Literatur. Da in der Vorlesung lediglich der Max-Heap eingeführt wurde, beschränken wir uns der Übersichtlichkeit halber auf diesen Typ.
\begin{example}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=60mm]
\tikzstyle{level 2}=[sibling distance=30mm]
\tikzstyle{level 3}=[sibling distance=15mm]
 \node {T}
    child {node {S}
    child {node {P}
    child {node {E}}
    child {node {I}}}
    child {node {N}
    child {node {H}}
    child {node {G}}}}
    child {node {R}
    child {node {O}}
    child {node {A}}};
\end{tikzpicture}
\caption{Binärer Max-Heap}
\end{figure}
\end{example}
Wir erkennen und formulieren als Eigenschaften:
\begin{itemize}
\item Es handelt sich um einen (fast) vollständigen Binärbaum, d.h. die Blätter befinden sich auf höchstens zwei verschiedenen Levels.
\item Die Blätter auf dem untersten Level sind linksbündig angeordnet.
\end{itemize}
\paragraph{Implementierung eines Heaps als Array}
Ein Heap lässt sich als Feld/Array implementieren, da für die Indizes die oberen Bedingungen leicht erkannt und umgesetzt werden können. 
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=60mm]
\tikzstyle{level 2}=[sibling distance=30mm]
\tikzstyle{level 3}=[sibling distance=15mm]
 \node {A[1]}
    child {node {A[2]}
    child {node {A[4]}
    child {node {A[8]}}
    child {node {A[9]}}}
    child {node {A[5]}
    child {node {A[10]}}
    child {node {A[11]}}}}
    child {node {A[3]}
    child {node {A[6]}}
    child {node {A[7]}}};
\end{tikzpicture}
\caption{Binärer Max-Heap in Array-Darstellung}
\end{figure}
Als Array erhalten wir damit:
\begin{table}[h]
\begin{tabular}{cccccccccccc}
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\
\hline
11 & T & S & R & P & N & O & A & E & I & H & G \\
\end{tabular}
\end{table}
In $A[0]$ speichern wir die Größe des Heaps, in diesem Fall 11, da es sich um 11 Elemente handelt. \\
Wir können erneut festhalten: 
\begin{itemize}
\item $A[i]$ hat linkes Kind $A[2i]$
\item $A[i]$ hat rechtes Kind $A[2i+1]$
\item $A[i]$ hat Vater $A[\frac{i}{2}]$ (bei Ganzzahldivision)
\item $A[i]$ befindet sich auf dem Level $\lfloor log_{2}(i) \rfloor$
\end{itemize}
\section{Herstellen der Heap-Eigenschaft}
Um sicherzustellen, dass es sich bei einem gegebenen Baum um einen Heap handelt, unterscheiden wir zwei Szenarien.
\begin{itemize}
\item \textbf{Szenario 1:} Der Schlüssel eines Kindes ist größer als der Schlüssel eines Elternknotens. Dann vertauschen wir -- logischerweise -- den Schlüssel des Kindes mit dem Schlüssel des Elternknotens. Das Kind {\glqq}schwimmt nach oben{\grqq}, deshalb heißt dieses Vorgehen auch \textit{swim-up}.
\begin{example}
\begin{figure}[h!]
\hspace{-2cm}
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=50mm]
\tikzstyle{level 2}=[sibling distance=30mm]
\tikzstyle{level 3}=[sibling distance=15mm]
 \node {S}
    child {node {P}
    child {node {N}
    child {node {E}}
    child {node {I}}}
    child {node[fill=yellow] {T}
    child {node {H}}
    child {node {G}}}}
    child {node {R}
    child {node {O}}
    child {node {A}}};
\hspace{4.5cm} $\Rightarrow$ \hspace{4.5cm}
\node {S}
    child {node[fill=yellow] {T}
    child {node {N}
    child {node {E}}
    child {node {I}}}
    child {node {P}
    child {node {H}}
    child {node {G}}}}
    child {node {R}
    child {node {O}}
    child {node {A}}};
\end{tikzpicture}
\caption{Herstellen der Heap-Eigenschaft durch swim{\_}up(T) im ersten Schritt}
\end{figure}
Dieser Baum ist noch kein Heap, da der Knoten T weiterhin größer ist als S. Folglich wiederholen wir das Vorgehen, für alle Knoten, bis die alle Beziehungen zwischen Kindern und Eltern passen.
\end{example}
Wir implementieren die Methode \texttt{swim{\_}up} wie folgt:\\
\begin{lstlisting}
void swim_up(A[], int k) {
	while (k > 1 && A[k/2] < A[k]) {
		exchange(A[k], A[k/2]);
		k = k / 2;	
	}
}
\end{lstlisting}
\item \textbf{Szenario 2:} Der Schlüssel eines Elternknotens ist kleiner als der Schlüssel eines/beider Kinder: Dann vertauschen wir den Schlüssel des Elternknotens mit dem Schlüssel des größeren Kindes.
\begin{lstlisting}
void sink(A[], int k, int heap_size) {
	while (2 * k <=  heap_size) {
		int j = 2 * k;
		if (j <= n && A[j] < A[j+1]) {
			j++;		
		}
		if (A[k] !< A[j]) {
			break;		
		}
		exchange (A[k], A[j]);
		k = j;	
	}
}
\end{lstlisting}
\end{itemize}
\section{Einfügen eines Elementes}
Das Einfügen eines Elementes in einen Heap erfolgt zunächst an der nächsten freien Position im Array. Also falls wir einen bestehenden Heap der Länge 11 betrachten, fügen wir das Element an der Position 12 ein. Anschließend wird die Heap-Eigenschaft durch \texttt{swim{\_}up} wiederhergestellt. \\\\
Die Komplexität des Einfügens liegt in $O(log (n))$. \\\\
Implementierung:
\begin{lstlisting}
void insert(A[], key_type Element) {
	heap_size(A)++;
	A[heap_size(A)] = Element;
	swim_up(A, heap_size);
}
\end{lstlisting}
\section{Löschen des größten Elements}
In einem Heap ist das Löschen zunächst nur für das größte Element (also im Max-Heap die Wurzel) definiert. Wir tauschen die Wurzel mit dem letzten Element im Heap, erhalten dadurch eine neue Wurzel. Das neue letzte Element kann nun einfach entfernt werden, indem wir die Größe des Arrays um 1 verringern. Nun ist abschließend noch die Heap-Eigenschaft durch Absinken lassen der neuen Wurzel herzustellen. \\\\
Auch diese Operation verläuft in $O(log(n))$. \\\\
Implementierung:
\begin{lstlisting}
key_type del_max(A[]) {
	key_type max = A[1];
	exchange(A[1], A[heap_size(A)]);
	heap_size(A)--;
	sink(A, 1, heap_size(A));
	return max;
}
\end{lstlisting}
\section{Heapsort}
Nun lässt sich auf Heaps auch ein Suchalgorithmus definieren, den wir im Folgenden betrachten und anwenden wollen. Auch wenn wir Heaps zunächst als Baumstruktur eingeführt haben, wird von Ihnen in Klausuren verlangt werden, die Konzepte und Methoden -- also auch Heapsort -- auf bloße Arrays anzuwenden. Versuchen Sie daher, sich die Beziehung zwischen Heaps als Bäumen und Heaps als Array zu verinnerlichen.  
\paragraph{Idee}
Ausgehend von einem ungeordneten Feld, stellen wir zunächst die Heap-Eigenschaft her. Nun steht -- zumindest bei Max-Heaps -- der größte Schlüssel in der Wurzel bzw. an Index 1. Nun tauschen wir den Schlüssel der Wurzel mit dem Schlüssel an der letzten Stelle des Feldes. Nun stellen wir die Heap-Eigenschaft über sink wieder her und tauschen die neu gewonnene Wurzel mit dem letzten Feldelement, solange noch Schlüssel ungeordnet sind. Die Anzahl der noch ungeordneten Schlüssel erhalten wir dadurch, dass wir das Feld nach jeder Iteration verkürzen.
\paragraph{Implementation} als Pseudo-Code
\begin{lstlisting}
void heapsort(A[]) {
	int n = A.length;
	for (int k = n/2; k >= 1; k--) {
		sink(A, k, n);
	}
	while (n > 1) {
		exchange(A[1], A[n]);
		n--;
		sink(A, 1, n);	
	}
}
\end{lstlisting}
\begin{example}
\centering
Betrachten wir das ungeordnete Array 
$\left[ \begin{array}{rrrrrrrr}
6 & 9 & 1 & 8 & 2 & 3 & 5 & 4 
\end{array} \right]$. Zunächst stellen wir die Heap-Eigenschaft her, in dem wir \texttt{sink()} aufrufen. \\
$\left[ \begin{array}{rrrrrrrr}
6 & 9 & 1 & 8 & 2 & 3 & 5 & 4
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrr}
6 & 9 & 5 & 8 & 2 & 3 & 1 & 4
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrr}
9 & 6 & 5 & 8 & 2 & 3 & 1 & 4
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrr}
9 & 8 & 5 & 6 & 2 & 3 & 1 & 4
\end{array} \right]$ \\
Nun beginnen die Vertauschungsoperationen: \\
$\left[ \begin{array}{rrrrrrrr}
4 & 8 & 5 & 6 & 2 & 3 & 1 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrr|r}
8 & 4 & 5 & 6 & 2 & 3 & 1 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrr|r}
8 & 6 & 5 & 4 & 2 & 3 & 1 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrr|r}
1 & 6 & 5 & 4 & 2 & 3 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrr|rr}
6 & 1 & 5 & 4 & 2 & 3 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrr|rr}
6 & 4 & 5 & 1 & 2 & 3 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrr|rr}
3 & 4 & 5 & 1 & 2 & 6 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrr|rrr}
5 & 4 & 3 & 1 & 2 & 6 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrr|rrr}
2 & 4 & 3 & 1 & 5 & 6 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrr|rrrr}
4 & 2 & 3 & 1 & 5 & 6 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrrr|rrrr}
1 & 2 & 3 & 4 & 5 & 6 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrr|rrrrr}
3 & 2 & 1 & 4 & 5 & 6 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rrr|rrrrr}
1 & 2 & 3 & 4 & 5 & 6 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{rr|rrrrrr}
2 & 1 & 3 & 4 & 5 & 6 & 8 & 9
\end{array} \right]$ \\
$\left[ \begin{array}{r|rrrrrrr}
1 & 2 & 3 & 4 & 5 & 6 & 8 & 9
\end{array} \right]$ \\
Im letzten Schritt haben wir das gewünschte, aufsteigend sortierte Array erhalten.
\end{example}
\paragraph{Komplexität}
Wie alle anderen Sortieralgorithmen ist auch Heapsort nicht laufzeiteffizienter als $O(n \cdot log(n))$. Für das Herstellen der Heap-Eigenschaft erhalten wir eine Komplexität von $O(n)$, da wir im schlechtesten Fall alle Elemente des Arrays einmal vertauschen müssen. Für die eigentliche Suche erhalten wir $O(n \cdot log(n))$.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Gegeben ist der folgende Baum.
\begin{itemize}
\item Beurteilen und begründen Sie, ob es sich bei diesem Baum um einen Heap handelt.
\item Falls es sich nicht um einen Heap handelt, stellen Sie die Heap-Eigenschaft her.
\item Fügen Sie das Element $X$ hinzu.
\end{itemize}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=50mm]
\tikzstyle{level 2}=[sibling distance=30mm]
\tikzstyle{level 3}=[sibling distance=15mm]
 \node {S}
    child {node {K}
    child {node {D}
    child {node {A}}
    child {node {B}}}
    child {node {G}
    child {node {F}}
    child[fill=none] {edge from parent[draw=none]}}}
    child {node {M}
    child {node {R}}
    child {node {N}}};
\end{tikzpicture}
\end{figure}
\begin{flushright}
Lösung auf Seite \pageref{a5.1:lsg} \\
\end{flushright}
\paragraph{Aufgabe 2} 
Sortieren Sie den Baum aus Aufgabe 1 mittels \texttt{heapsort()} aufsteigend.
\begin{flushright}
Lösung auf Seite \pageref{a5.2:lsg}
\end{flushright}
\paragraph{Aufgabe 3}
Sortieren Sie die folgenden Arrays mittels \texttt{heapsort()} aufsteigend.
\begin{itemize}
\item $\left[ \begin{array}{rrrrrrrr}
7 & 1 & 3 & 9 & 4 & 6 & 5 & 8 
\end{array} \right]$
\item $\left[ \begin{array}{rrrrrrrr}
H & S & G & A & V & D & J & K 
\end{array} \right]$
\end{itemize} 
\begin{flushright}
Lösung auf Seite \pageref{a5.3:lsg}
\end{flushright}
\paragraph{Aufgabe 4}
Erläutern Sie, inwiefern anstatt der Funktion \texttt{sink()} die Funktion \texttt{swim{\_}up()} in \texttt{heapsort()} verwendet werden könnte. Geben Sie an, wie sich der Algorithmus ändert und stellen Sie dies in Pseudocode dar.
\begin{flushright}
Lösung auf Seite \pageref{a5.4:lsg}
\end{flushright}
\pagebreak
\part{Graphentheorie}
Nach unserem Ausflug in besondere Bäume (den Heaps) begeben wir uns nun wieder auf eine abstraktere Ebene, diesmal sogar über Bäume hinaus: Im Folgenden betrachten wir Graphen allgemein und gehen auf Themen wie Pfade, Richtungen oder die Suche nach dem kürzesten Weg ein. Sie haben sicherlich eine Intuition, wenn Sie sich durch eine Stadt bewegen, welcher denn nun der kürzeste Weg ist -- während ein Computer damit beschäftigt wäre, Optionen durchzuprobieren. Insofern erscheint es sinnvoll, dafür praktikable Algorithmen zu finden und zu behandeln.
Doch beginnen wir von vorne. 
\section{Typisierung und Eigenschaften}
\begin{definition}
Ein gerichteter Graph ist ein Paar $G = (V, E)$, wobei $V$ eine nicht-leere Menge und $E \subseteq V \times V$ ist. $V$ heißt die \textbf{Menge der Knoten} und $E$ heißt die \textbf{Menge der (gerichteten) Kanten}. Ein Knoten $v$ heißt \textbf{benachbart} oder \textbf{adjazent} zu $v$, wenn $(v, w) \in E$. $v$ heißt Anfangspunkt und $w$ heißt Endpunkt der Kante $(v, w)$. Eine Kante $(v, v)$, die also von einem Knoten auf diesen selbst gerichtet ist, bezeichnen wir als \textbf{Schleife}.
\end{definition}
\begin{example} 
\begin{figure}[h]
\centering
\begin{tikzpicture}[main/.style = {draw, circle, minimum size=1cm}]
\node[main] (1) {5};
\node[main] (2) [right=of 1] {7};
\node[main] (3) [right=of 2] {3};
\node[main] (4) [below=of 1] {11};
\node[main] (5) [below=of 2] {8};
\node[main] (6) [below=of 4] {2};
\node[main] (7) [below=of 5] {9};
\node[main] (8) [right=of 7] {10};
\draw[->] (1) -- (4);
\draw[->] (2) -- (4);
\draw[->] (2) -- (5);
\draw[->] (3) -- (5);
\draw[->] (3) -- (8);
\draw[->] (4) -- (6);
\draw[->] (4) -- (7);
\draw[->] (4) -- (8);
\draw[->] (5) -- (7);
\end{tikzpicture}
\caption{Gerichteter Graph}
\end{figure}
\end{example}
\begin{definition}
Bei einem \textbf{ungerichteten} Graphen besitzen die Kanten keine Richtung. $v$ und $w$ heißen \textbf{Endpunkte} der Kante $e = \{v, w\}$. Die Kante $\{v, w\}$ heißt \textbf{inzident} zu $v$ und zu $w$. 
\end{definition}
Zu beiden Typen von Graphen können wir nun Eigenschaften und Kennzahlen definieren:
\begin{definition}
Sei $v \in V$. 
\[U(v) := \{w \in V \mid w \text{ adjazent zu } v\}\] heißt \textbf{Umgebung} von $v$. Die Anzahl $\vert U(v) \vert$ der Elemente von $U(v)$ heißt \textbf{Grad} von $v$, kurz $\text{deg}(v)$. \\
Die Anzahl der Knoten $\vert V \vert$ heißt Ordnung von $v$.
\end{definition}
Damit ergibt sich:
\begin{itemize}
\item $\vert V \vert$ und $\vert E \vert$ messen die Größe eines Graphen. Daher geschehen Aufwandsschätzungen über Graphen stets in Abhängigkeit zu diesen beiden Angaben.
\item Einem gerichteten Graphen kann man einem ungerichteten Graphen zuordnen: Kante $(v, w) \to \text{Kante} \{v, w\}$.
\item Dem einem Graphen zugeordneten gerichteten Graphen erhalten wir, wenn wir jede Kante $\{v, w\}$ durch die Kanten $(v, w) \text{und} (w, v)$ ersetzen. 
\end{itemize}
\begin{definition}
Sei $G = (V, E)$ ein ungerichteter Graph.
\begin{itemize}
\item Ein \textbf{Pfad} P ist eine Folge von Knoten $v_0, v_1, ..., v_n$ mit der Eigenschaft $\{v_{i}, v_{i+1}\} \in E, i = 0, ..., n-1$. Dann heißt $v_0$ \textbf{Anfangspunkt} und $v_n$ \textbf{Endpunkt} von $P$. $n$ heißt \textbf{Länge} von P.
\item $w$ heißt von $v$ \textbf{erreichbar}, falls es einen Pfad $P$ mit Ausgangspunkt $v$ und Endpunkt $w$ gibt.
\end{itemize}
Für gerichtete Graphen definieren wir die Begriffe analog, in einem Pfad sind die Kanten dann gerichtet enhalten.
\end{definition}
\begin{satz}
Sei $G=(V, E)$ ein Graph ohne Schleifen. Dann sei $n := \vert V \vert$ und $m := \vert E \vert$.
\begin{itemize}
\item Für einen Graphen gilt: $0 \leq m \leq \binom{n}{2}$
\item Für einen gerichteten Graphen gilt: $0 \leq m \leq n \cdot (n-1)$.
\end{itemize}
Falls Graphen mit Schleifen betrachtet werden, gilt alternativ:
\begin{itemize}
\item Für einen Graphen gilt: $0 \leq m \leq \binom{n}{2}+n$
\item Für einen gerichteten Graphen gilt: $0 \leq m \leq n^{2}$.
\end{itemize}
\end{satz}
Nun haben wir schon eine Vielzahl von Begrifflichkeiten zu Graphen definiert. Sie finden, das reicht? Nun ...
\begin{definition} Für einen Graphen eines beliebigen Typs definieren wir:
\begin{itemize}
\item $G=(V, E)$ heißt \textbf{vollständig}, wenn jeder Knoten mit jedem anderen Knoten durch eine Kante verbunden ist.
\item Besitzt $G$ viele Kanten ($m$ groß im Vergleich zu $\binom{n}{2}$ oder $n \cdot (n-1)$), dann heißt $G$ \textbf{dicht besetzt}.
\item Besitzt $G$ wenige Kanten, dann heißt $G$ analog dazu \textbf{dünn besetzt}.
\end{itemize}
Daraus folgt: \\\\
Für einen vollständigen Graphen gilt $\vert E \vert = n \cdot (n-1)$ im Fall eines gerichteten Graphen und $\vert E \vert = n^{2}$ für einen ungerichteten Graphen.
\end{definition} 
\section{Adjazenzmatrizen und -listen}
\begin{flushright}
{\glqq}Ein guter Nachbar ist besser als viele böse Verwandten.{\grqq} \\
--- \textit{Ungarisches Sprichwort}
\end{flushright}
Falls Sie erinnern, wie wir Adjazenz definiert haben, werden Sie eine Intuition dafür haben, womit wir uns hier beschäftigen wollen: Wir betrachten Nachbarschaftsverhältnisse in Graphen. Dafür nutzen wir zwei Darstellungsformen, die Adjazenzmatrix und die Adjazenzliste. Hierbei handelt es sich tatsächlich nur um verschiedene Darstellungsformen, der Inhalt unterscheidet sich nicht.
\begin{definition}
Die \textbf{Adjazenzmatrix} ist eine $n \times n$-Matrix mit Koeffizienten aus $\{0, 1\}$.
\[ adm[i, j] := \begin{cases}1 & \text{ für } (i, j) \in E \\0 & \text{ sonst }\end{cases}\]
\end{definition}
\begin{example}
\begin{figure}[h]
\centering
\begin{tikzpicture}
[main/.style = {draw, circle, minimum size=1cm}]
\node[main] (1) {1};
\node[main] (2) [right=of 1] {2};
\node[main] (4) [below=of 1] {4};
\node[main] (3) [below=of 2] {3};
\draw (1) -- (2);
\draw (2) -- (3);
\draw (3) -- (4);
\draw (1) -- (4);
\draw (2) -- (4);
\end{tikzpicture}
\caption{Ungerichteter Graph zur Veranschaulichung der Adjazenzmatrix}
\end{figure}
Wir entnehmen der Darstellung, dass die Knoten (1, 2), (2, 3), (3, 4), (1, 4) und (2, 4) benachbart sind. Also muss an diesen Indizes der Matrix eine 1 stehen, in allen übrigen Fällen entsprechend eine 0. \\
\begin{center}
$\left( \begin{array}{rrrr}
0&1&0&1 \\
1&0&1&1 \\
0&1&0&1 \\
1&1&1&0
\end{array} \right)$
\end{center}
Betrachten wir nun vergleichsweise diesen Graphen in einer gerichteten Variation: \\
\begin{figure}[h]
\centering
\begin{tikzpicture}
[main/.style = {draw, circle, minimum size=1cm}]
\node[main] (1) {1};
\node[main] (2) [right=of 1] {2};
\node[main] (4) [below=of 1] {4};
\node[main] (3) [below=of 2] {3};
\draw[->] (1) -- (2);
\draw[->] (2) -- (3);
\draw[->] (3) -- (4);
\draw[->] (1) -- (4);
\draw[->] (2) -- (4);
\end{tikzpicture}
\end{figure}
Hier ergibt sich eine andere Matrix, da die Nachbarschaftsverhältnisse einseitig sind und wir also für jeden Knoten betrachten, auf welche Knoten er (durch einen so gerichteten Pfeil) {\glqq}zugreifen{\grqq} kann. \\
\begin{center}
$\left( \begin{array}{rrrr}
0&1&0&1 \\
0&0&1&1 \\
0&0&0&1 \\
0&0&0&0
\end{array} \right)$
\end{center}
\end{example}
Nun definieren wir alternativ dazu:
\begin{definition}
Die \textbf{Adjazenzliste} $adl[1, ..., n]$ ist ein Array von Listen. Für jeden Knoten $j \in V$ ist die Liste $adl[j]$ definiert durch $i \in adl[j]$ genau dann, wenn $(j, i) \in E$ gilt. \\\\
Also sind in $adl[j]$ ebenfalls die zu $j$ adjazenten Knoten verzeichnet.
\end{definition}
\begin{example}
Für obiges Beispiel ergibt sich nun im Fall des ungerichteten Graphen: \\\\
\texttt{
1: 2, 4 \\
2: 1, 3, 4 \\
3: 2, 4 \\
4: 1, 2, 3}
\\
Und im Fall des gerichteten Graphen: \\
\texttt{
1: 2, 4 \\
2: 3, 4 \\
3: 4 \\
4: }
\end{example}
\paragraph{Eigenschaften}
Die Adjazenzmatrix ist speichereffizient für dicht besetzte Graphen (immer $O(n^{2})$). Die Suche nach einer bestimmten Kante verläuft in $O(1)$, ist also logischerweise sehr effizient. \\\\
Eine Adjazenzliste hat $m$ Einträge bei gerichteten und $2 \cdot m$ Einträge bei ungerichteten Graphen. Für dünn besetzte Graphen ist sie besser geeignet.
\section{Suchverfahren in Graphen}
Wie in anderen Datenstrukturen auch, sind wir interessiert daran, innerhalb von Graphen Elemente zu suchen. Hierbei brauchen wir aber zunächst -- da wir Graphen ganz verschiedenen Aufbaus betrachten -- Wege, alle Knoten eines Graphen effizient zu erreichen. Dafür unterscheiden wir \textbf{Breitensuche} und \textbf{Tiefensuche}. Bei der Breitensuche betrachten wir vorrangig benachbarte Elemente (also Adjazenten) während wir bei der Tiefensuche vorrangig untergeordnete Knoten (also Endknoten eines Pfades) betrachten.
\subsection{Breitensuche}
\paragraph{Idee}
\begin{enumerate}
\item Beginnend von einem Startknoten $S$ besuchen wir alle Nachbarn von $S$, also alle Elemente von $U(S)$.
\item Nun fahren wir fort, in dem wir alle Nachbarn des ersten Nachbarn besuchen $(U(U_{1}(S))$ für $U_{1}(S) = $ 1. Nachbar des Startknotens, dann alle Nachbarn des zweiten Nachbarn von $S$ usw.
\item Falls wir über diese Vorgehenweise nicht alle Knoten erreichen, setzen wir als Ausgangspunkt einen neuen, nicht besuchten Knoten und fahren solange fort, bis alle Knoten erreicht worden sind.
\end{enumerate}
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
for (v in V) {
	v.marked = false;
}
Q = new Queue();
while exists s in V with (s.marked == false) {
	s = pickStartNode();
	s.marked = true;
	Q.enqueue(s);
	while (!Q.is_empty()) {
		v = Q.dequeue();
		visit(v);
		for (u in v.neighbours) {
			if (u.marked == false) {
				u.marked = true;
				Q.enqueue(u);			
			}		
		}	
	}
}
\end{lstlisting}
Wir operieren folglich auf einer \texttt{Queue}, also einer Warteschlange, die nach dem FIFO-Prinzip (First in, first out) funktioniert. Für jeden Knoten in der Warteschlange fügen wir alle Nachbarn der Wartschlange hinzu. Die Abfrage, ob ein Knoten besucht wurde oder nicht (also das Markieren) ist erforderlich, um Dopplungen zu vermeiden, die Laufzeit zu optimieren und das Terminieren sicherzustellen. \\\\
Wir nennen dieses Verfahren Breitensuche oder \textbf{Breadth First Search}-Traversal \parencite[][S. 574ff.]{Grundkurs}.
\subsection{Tiefensuche}
Die Tiefensuche hat zum Ziel, zuerst die Knoten in der Tiefe zu besuchen. Sie heißt daher auch \textbf{Depth First Search}-Traversal. Die Grundlage dieses Algorithmus bildet die Datenstruktur \texttt{Stack}, auf der nach dem LIFO-Prinzip (Last in, first out) gearbeitet wird.
\paragraph{Idee}
\begin{enumerate}
\item Beginnend von einem Startknoten $S$ wird der letzte Nachbar von $S$ besucht. Anschließend besuchen wir dessen letzten Nachbarn und gehen soweit in die Tiefe, bis es keinen unbesuchten Nachbarn mehr auf diesem Pfad gibt.
\item Nun kehren wir zum ersten Knoten zurück, bei dem es einen noch unbesuchten Nachbarn gibt und gehen von hier in die Tiefe, in dem wir jeweils den letzten Nachbarn des letzten Nachbarn besuchen.
\item Dieses Verfahren führen wir fort, bis wir alle Nachbarn besucht haben.
\end{enumerate}
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
for (v in V) {
	v. marked = false;
}
S = new Stack();
while exists s in V with (s.marked == false) {
	s = pickStartNode();
	S.push(s);
	
	while (!S.is_empty()) {
		v = S.pop();
		if (v.marked == false) {
			visit(v);
			v.marked = true;
			for u in v.neighbours {
				if (u.marked == false) {
					S.push(u);				
				}			
			}		
		}	
	}
}
\end{lstlisting}
Wir initialisieren auch hier erneut unseren Graphen, in dem wir alle Knoten als unmarkiert setzen und einen Stack erzeugen. Solange es noch unmarkierte Knoten gibt, wählen wir einen Ausgangsknoten, fügen ihn dem Stapel hinzu und besuchen den ersten Nachbarn, den ersten Nachbarn des ersten Nachbarn, und so weiter. Durch die Stapelstruktur ermöglichen wir, dass keine Elemente vergessen werden und es einen {\glqq}Weg zurück{\grqq} gibt, um die Suche fortsetzen zu können. \\\\
In Bezug auf Klausuren ist es wichtig, jeweils Queue/Stack pro Iteration anzugeben. Nur darüber wird deutlich, ob sie die Vorgehensweise wirklich verstanden haben. \\
\begin{example}
Wir betrachten als folgenden Beispielgraphen das Verwandschaftsverhältnis einer typisch saarländischen Familie: \\
\begin{figure}[h]
\centering
\begin{tikzpicture}
[main/.style = {draw, circle, minimum size=1cm}]
\node[main] (1) {Otto};
\node[main] (2) [right=of 1] {Anna};
\node[main] (3) [below=of 1] {Sara};
\node[main] (4) [right=of 3] {Cem};
\node[main] (5) [right=of 4] {Tom};
\draw[->] (1) -- (2);
\draw[->] (1) -- (3);
\draw[->] (2) -- (3);
\draw[->] (3) -- (4);
\draw[->] (2) -- (5);
\draw[->] (5) -- (4);
\end{tikzpicture}
\caption{Gerichteter Beispielgraph für Breiten-/Tiefensuche}
\end{figure} \\
Beginnen wir mit der \textbf{Breitensuche}:
\begin{enumerate}
\item Als Startknoten wählen wir {\glqq}Otto{\grqq}.
\item Wir markieren diesen Knoten und fügen ihn der Queue hinzu. \\
\texttt{Q = \{Otto\}}, durchlaufene Knoten: \texttt{[Otto]}
\item Wir markieren alle benachbarten Knoten und fügen sie der Queue hinzu. \\
\texttt{Q = \{Anna, Sara\}}, durchlaufene Knoten: \texttt{[Otto]}
\item Wir besuchen Anna, löschen sie von der Queue und fügen ihre Nachbarn hinzu (Sara nicht, da dieser Knoten schon markiert wurde). \\
\texttt{Q = \{Sara, Tom\}}, durchlaufene Knoten: \texttt{[Otto, Anna]}
\item Wir nehmen das nächste Element vom Stapel, also Sara, besuchen sie und markieren (und enqueuen) alle ihre Nachbarn. \\
\texttt{Q = \{Tom, Cem\}}, durchlaufene Knoten: \texttt{[Otto, Anna, Sara]}.
\item Wir nehmen das nächste Element vom Stapel, also Tom, besuchen ihn und markieren alle seine Nachbarn. Da es keinen weiteren unmarkierten Nachbarn gibt, wird der Queue kein weiteres Element hinzugefügt. \\
\texttt{Q = \{Cem\}}, durchlaufene Knoten: \texttt{[Otto, Anna, Sara, Tom]}.
\item Nun besuchen wir Cem, stellen fest, dass es keine Nachbarn gibt und tatsächlich auch alle Knoten besucht wurden. \\
\texttt{Q = \{\}}, durchlaufene Knoten: \texttt{[Otto, Anna, Sara, Tom, Cem]}
\end{enumerate}
Nun betrachten wir als zweites die \textbf{Tiefensuche}:
\begin{enumerate}
\item Wir wählen als Startelement erneut Otto, fügen ihn dem Stapel hinzu. \\
\texttt{S = \{Otto\}}, durchlaufene Knoten: \texttt{[]}
\item Wir nehmen das erste Element vom Stack, besuchen und markieren es. Nun fügen wir alle Nachbarn dem Stack hinzu. \\
\texttt{S = \{Anna, Sara\}}, durchlaufene Knoten: \texttt{[Otto]}
\item Wir entfernen das zuletzt hinzugefügte Element vom Stack, besuchen und markieren es. Nun fügen wir alle Nachbarn dem Stack hinzu. \\
\texttt{S = \{Anna, Cem\}}, durchlaufene Knoten: \texttt{[Otto, Sara]}
\item Wir fahren entsprechend fort. Da Cem keine Nachbarn hat, wird dem Stack nichts hinzugefügt.\\
\texttt{S = \{Anna\}}, durchlaufene Knoten: \texttt{[Otto, Sara, Cem]}
\item Nun betrachten wir Anna, nehmen sie vom Stack und fügen ihren Nachbarn Tom hinzu. Nachbarin Sara ist bereits markiert worden und wird deshalb nicht erneut hinzugefügt. \\
\texttt{S = \{Tom\}}, durchlaufene Knoten: \texttt{[Otto, Sara, Cem, Anna]}
\item Nun ist Tom das nächste Element, das wir vom Stack nehmen. Tom hat keinen weiteren unmarkierten Nachbarn. Der Algorithmus terminiert.\\
\texttt{S = \{\}}, durchlaufene Knoten: \texttt{[Otto, Sara, Cem, Anna, Tom]}
\end{enumerate}
Der geringe Unterschied zwischen Breiten- und Tiefensuche ist der Übungskünstlichkeit geschuldet, in der Praxis ergeben sich grundlegend andere Stacks/Queues und enstprechende Durchlaufreihenfolgen.
\end{example}
\section{Gerichtete Graphen}
Wir erweitern unsere Definition von Graphen nun, indem wir die Wege zwischen einzelnen Knoten mit Gewichten versehen. So können wir Aussagen darüber treffen, ob ein Nachbar eines Knotens nach kürzerer Strecke erreichbar ist als ein anderer.
\begin{definition}
Ein (gerichteter) Graph $G = (V, E)$ mit einer Abbildung $g: E \to \mathbb{R}_{\geq 0}$ heißt \textbf{gewichteter} (gerichteter) Graph. Die Abbildung $g$ heißt Gewichtsfunktion. \\\\
Für $e \in E$ heißt $g(e)$ das \textbf{Gewicht von $e$}. Das Gewicht von $G$ ist die Summe aller Gewichte aller Kanten, also 
\[g(G) = \sum_{e \in E} g(e)\]
\end{definition}
Auch zu gewichteten Graphen können wir Adjazenzmatrizen und Adjazenzlisten nutzen:
\begin{itemize}
\item In einer \textbf{Adjazenzmatrix} zu tragen wir nun anstatt einer 1 bzw. 0 das Gewicht der Kante $g(e)$ ein.
\item Auch die Elemente der \textbf{Adjazenzliste} erweitern wir um die Komponente des Gewichts jeder Kante: \\
Beispiel: $4: \quad 2 \mid 1,5; 7 \mid 0.8$
\end{itemize}
Nun interessieren wir uns dafür, den kürzesten Weg von einem Knoten zu einem anderen zu finden. Dass es mehrere mögliche Wege gibt, ergibt sich aus den bestehenden Quervernetzungen. Typischerweise sind die Wege jeweils unterschiedlich gewichtet, sodass sich die Betrachtung tatsächlich lohnt. 
\subsection{Der Algorithmus von Dijkstra}
Der niederländische Informatiekr \textsc{Edsger W. Dijkstra} entwickelte 1959 einen Algorithmus zur Bestimmung der kürzesten Pfade in einem Graphen. Stellen Sie sich vor, Sie sind mit der Bahn unterwegs von Nord nach Süd. Sie müssen mindestens einmal umsteigen und Ihnen stehen verschiedene Strecken zur Auswahl, jeweils mit unterschiedlichen Fahrzeiten (also Gewichten). Ein Weg führt von Hamburg nach Mannheim über Bremen und Bonn, ein weiterer über Hannover und Frankfurt. Ein dritter führt über Berlin und Dresden.
\paragraph{Idee} Der Algorithmus operiert wie folgt.
\begin{enumerate}
\item Zunächst initialisieren Sie die Suche, indem Sie festhalten, dass der Abstand von ihrem Startknoten zu sich selbst 0 ist und alle weiteren Knoten erst einmal in unerreichbarer Ferne (also unendlich weit entfernt) liegen.
\item Nun wählen Sie den Knoten mit der kleinsten Entfernung -- also logischerweise den Startknoten -- und bestimmen den Abstand zu allen vom Startknoten aus erreichbaren Knoten. Diese Abstände notieren Sie sich.
\item Aus der Menge der noch nicht bearbeiteten Knoten wählen Sie denjenigen mit dem kürzesten Abstand und aktualisieren nun die Abstände der Knoten, falls es eine Möglichkeit gibt, den Weg mit dem aktuellen Knoten zu verkürzen.
\item So verfahren Sie, bis Sie alle Knoten betrachtet haben. Sie erhalten nun einen sogenannten \textbf{Kürzeste-Wege-Baum}.
\begin{definition}
Sei $G = (V, E)$ ein gewichteter Graph und $v \in V$. Ermittelt wird ein Pfad minimaler Länge von $v$ nach $w, \forall w \in V$ und es wird ein Baum $T = (V_T, E_T)$ mit Wurzel $v$ konstruiert. Dieser Baum heißt \textbf{Kürzeste-Wege-Baum}.
\end{definition}
\end{enumerate}
Wenn wir diese Vorgehensweise in Pseudo-Code darstellen wollen, erhalten wir:
\paragraph{Implementation} \quad
\begin{lstlisting}
Funktion Dijkstra(Graph, Startknoten):
	initialisiere(Graph, Startknoten, abstand[], vorgaenger[], Q);
	solange Q nicht leer:
		u := Knoten in Q mit kleinstem Wert in abstand[];
		Q.entferne(u);
		falls (U == Startknoten) {
			T := ({Startknoten}, {});		 		
		}
		sonst:
			V_T.add(u);
			E_T.add(vorgaenger[u], u);
		fuer jeden Nachbarn v von u:
			falls v in Q:
				distanz_update(u, v, abstand[], vorgaenger[]);
\end{lstlisting}
Für die Initialisierungsfunktion ergibt sich zusätzlich:
\begin{lstlisting}
initialisiere(Graph, Startknoten) {
	fuer jeden Knoten v in Graph:
		abstand[v] = infinity;
		vorgaenger[v] = undefined;
	abstand[Startknoten] = 0;
}
\end{lstlisting}
Das Aktualisieren der Distanzen erfolgt nun über folgende Methode:
\begin{lstlisting}
distanz_update(u, v, abstand[], vorgaenger[]) {
	alternativ = abstand[u] + abstand_zwischen(u, v);
	falls alternativ < abstand[v]:
		abstand[v] = alternativ;
		vorgaenger[v] = u;
}
\end{lstlisting}
Hiermit erreichen wir nun, dass die Distanzen verringert werden, falls es einen kürzeren Weg geben. Am Ende dieses Durchlaufs sind die kürzesten Wege ermittelt. \\
\pagebreak
\begin{example}
Gegeben ist der folgende Graph:
\begin{figure}[h]
\centering
\begin{tikzpicture}
[main/.style = {draw, circle, minimum size=1cm, node distance=3cm,}]
\node[main] (1) {Rathaus};
\node[main] (2) [right=of 1] {Marktplatz};
\node[main] (3) [below=of 1] {Wohngebiet};
\node[main] (4) [right=of 3] {Hochschule};
\node[main] (5) [right=of 4] {Supermarkt};
\draw (1) edge node [left] {2} (3);
\draw (1) edge node [above] {3} (2);
\draw (1) edge node [below] {4} (5);
\draw (2) edge node [above] {8} (3);
\draw (3) edge node [above] {5} (4);
\draw (2) edge node [right] {1} (5);
\draw (5) edge node [above] {5} (4);
\end{tikzpicture}
\caption{Beispielgraph für den Dijkstra-Algorithmus}
\end{figure} \\
Wir kürzen die einzelnen Knoten für den Algorithmus nun ab mit RH, MP, WG, HS, SM und wählen das Wohngebiet als Startpunkt des Algorithmus. \\
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|l|}
\hline
u    & WG     & RH            & MP            & HS            & SM            & Bemerkung \\
\hline
init & 0 $\mid$ -- & $\infty$ $\mid$ -- & $\infty$ $\mid$ -- & $\infty$ $\mid$ -- & $\infty$ $\mid$ -- & Initialisierung \\
\hline
WG   &        & 2 $\mid$ WG   & 8 $\mid$ WG        & 5 $\mid$ WG        &               & Knoten mit kürzester Distanz: RH\\
\hline
RH   &        &               & 5 $\mid$ RH   	     &               & 6 $\mid$ RH        & Knoten mit kürzester Distanz: MP \\
\hline
MP   &        &               &               &               &               & Keine weiteren Knoten verblieben. \\
\hline
\end{tabular}
\caption{Beispiel zum Dijkstra-Algorithmus}
\end{table} 
Dieses Beispiel ist zugegebenerweise nicht sonderlich spannend, da vom Wohngebiet aus bereits viele Knoten erreichbar sind und es nur zwei Abkürzungen gibt, die wir finden, nämlich der Weg zum Marktplatz über das Rathaus und der Weg zum Supermarkt über das Rathaus. \\\\
Diese Wege lassen sich nun in einen \textbf{Kürzeste-Wege-Baum }übertragen. \\
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node[main] {WG}
    child {node[main] {HS}
    edge from parent node[right]{5}}
    child {node[main] {RH}
    child {node[main] {SM}
    edge from parent node[right]{4}}
    child {node[main] {MP}
    edge from parent node[right]{3}}
    edge from parent node[right]{2}};
\end{tikzpicture}
\caption{Kürzeste-Wege-Baum zum Dijkstra-Beispiel}
\end{figure}
\end{example}
\pagebreak
\section*{Aufgaben}
\begin{figure}[h]
\centering
\begin{tikzpicture}
[main/.style = {draw, circle, minimum size=1cm, node distance=3cm,}]
\node[main] (1) {A};
\node[main] (2) [right=of 1] {B};
\node[main] (3) [right=of 2] {C};
\node[main] (4) [below=of 1] {E};
\node[main] (5) [right=of 4] {D};
\node[main] (6) [right=of 5] {F};
\draw (1) edge node [right] {\quad 7} (6);
\draw (1) edge node [left] {2} (4);
\draw (2) edge node [above] {6} (3);
\draw (2) edge node [left] {9} (4);
\draw (2) edge node [left] {3} (5);
\draw (3) edge node [right] {2} (5);
\draw (3) edge node [right] {2} (6);
\draw (4) edge node [below] {1} (5);
\end{tikzpicture}
\end{figure}
\paragraph{Aufgabe 1} Geben Sie zu diesem Graph die Adjazenzmatrix und Adjazenzliste an.
\begin{flushright}
Lösung auf Seite \pageref{a6.1:lsg} \\
\end{flushright}
\paragraph{Aufgabe 2} Führen Sie zu diesem Graphen eine Breiten- und Tiefensuche durch. Geben Sie jeweils die Queue/den Stack und die bereits durchlaufenen Knoten an.
\begin{flushright}
Lösung auf Seite \pageref{a6.2:lsg} \\
\end{flushright}
\paragraph{Aufgabe 3} Wenden Sie den Dijkstra-Algorithmus zur Berechnung der kürzesten Wege von Knoten B ausgehend auf den Graphen an. Notieren Sie den Kürzeste-Wege-Baum.
\begin{flushright}
Lösung auf Seite \pageref{a6.3:lsg} \\
\end{flushright}
\paragraph{Aufgabe 4} Erläutern Sie den Unterschied zwischen Stack und Queue und gehen Sie darauf ein, warum die jeweilige Datenstruktur im BFS- bzw. DFS-Traversal-Algorithmus verwendet wird.
\begin{flushright}
Lösung auf Seite \pageref{a6.4:lsg} \\
\end{flushright}
\pagebreak
\part{Hashing}
In den vergangenen Abschnitten und Kapiteln haben wir eine Vielzahl von Datenstrukturen kennengelernt: Bäume, Stacks, Queues, Graphen und Arrays. Alle dienten sie dem Zweck, Daten und ihre Beziehungen zueinander zu visualisieren. In diesem letzten Abschnitt widmen wir uns nun wieder \textbf{Arrays}, der Datenstruktur, die Grundlage für viele andere ist. \\\\
Betrachten wir also eine Menge, beispielsweise jene der natürlichen Zahlen $\mathbb{N}$ oder Mitgliedsstaaten der europäischen Union. Eine Teilmenge davon stellen die Elemente eines Feldes (Arrays) dar. In diesem Array ist die Anordnung zu berücksichtigen, so kann es also durchaus sein, dass die Feldelemente nicht alphabetisch oder ihrer Größe nach geordnet sind. Uns interessiert, an welcher Stelle des Arrays ein gewisses Element seinen Platz gefunden hat. \\\\
Vielleicht denken Sie jetzt, dass sei gar kein so großes Problem, schließlich haben wir ja bereits Suchalgorithmen kennengelernt, doch denken Sie daran, dass dieses Feld ruhig beliebig groß sein kann und die Elemente nicht sortiert sind, sodass der Gedanke an die Binäre Suche Sie nicht zum Ziel führt. \\\\
Stattdessen suchen wir einen mathematischen Weg, eine Abbildung, die uns den Platz eines Elementes unseres Feldes mitteilt: \textbf{Eine Hashfunktion}. Diese Funktion berechnet zu jedem Feldelement einen Speicherort. Wir legen nun eine \textbf{Hashtabelle} an, in der je nach Hash-Wert die Ursprungselemente enthalten sind. Diese Tabelle löst unser Problem und ermöglicht es uns, die Elemente zu finden.
\begin{definition}
Eine Abbildung $H: K \to A$ mit $K$ = Schlüsselraum und $A$ = Adressraum heißt \textbf{Hashfunktion} \parencite[][S. 296]{Wirth}.
\end{definition}
Wir stellen an diese Hashfunktion einige Anforderungen:
\begin{itemize}
\item Die Abbildung soll effizient zu berechnen sein, also idealerweise mit Grundrechenarten auskommen.
\item Die Abbildung soll surjektiv sein, also alle möglichen Hashwerte sollen auch als Ergebnis zustande kommen.
\item Die Schlüssel sollen möglichst gleichmäßig auf den Bereich der Indexwerte verteilt werden.
\end{itemize}
\begin{example}
Beispielsweise könnte diese Abbildung eine Hashfunktion sein:
\[H(k) = ord(k) \text{mod} n\] mit $ord(k)$ = Ordinalzahl des Schlüssels in der Ursprungsmenge.
\end{example}
Nun könnte alles perfekt sein: Jedes Element unseres Feldes wird eindeutig auf einen Hashwert abgebildet, von dort in trivialer Weise gefunden und weiterverwendet. So perfekt ist es leider nicht -- es gibt einen großen Haken:
\section{Kollisionsvermeidung}
\begin{definition}
Als \textbf{Kollision} bezeichnen wir den Fall, dass zu einem Hashwert mehrere Ausgangswerte existieren. 
\end{definition}
Theoretisch könnte es also passieren, dass die Elemente $2$ und $17$ mit einer gegebenen Hashfunktion den gleichen Hashwert ergeben. Dann können wir beide Werte nicht ohne Weiteres in der Hashtabelle speichern. \\\\
Folgende Optionen ergeben sich:
\begin{itemize}
\item \textbf{Direkte Verkettung} -- Anstatt eines Eintrages in der Hashtabelle könnten wir auch eine Menge an Hashwerten als Eintrag speichern. Diese Lösung ist zwar pragmatisch und effizient, birgt jedoch den Nachteil, dass wir eine zusätzliche Liste mit Informationen über diese Sekundärtabelle benötigen, um weiterhin jedes Element finden zu können.
\item \textbf{Offene Adressierung} -- Alternativ suchen wir für unser abzuspeicherndes (also einzutragendes) Element einfach einen anderen, noch freien Platz. Diesen Vorgang nennen wir \textbf{Sondieren}. Falls unsere erste Wahl also durch ein anderes Element belegt sein sollte, berechnen wir mittels einer Sondierungsvorschrift einen alternativen Platz und probieren es dort. Falls wir auch dort scheitern sollten, haben wir immer noch weitere Möglichkeiten.
\end{itemize}
Im Folgenden wollen wir uns näher mit drei Kollisionsvermeidungsstrategien aus dem Bereich der offenen Adressierung beschäftigen.
\subsection{Lineares Sondieren}
Der triviale Ansatz wäre, im Falle einer Kollision den Hashwert einfach um 1 zu erhöhen. Wir gehen also in unserer Hashtabelle einen Schritt weiter und prüfen dort, ob dieser Hashwert noch frei ist. Falls ja, haben wir das Problem gelöst.
\begin{align*}
h[0] &= H(k) \\
h[i] &= (h[0] + 1) \text{ mod } n
\end{align*}
\paragraph{Probleme} Dieser Ansatz birgt den großen Nachteil, dass sich primäre Cluster bilden. Nehmen wir beispielsweise an, fünf Elemente erhalten durch eine Hashfunktion den Hashwert $8$. Nach dieser Methode sind nun -- sofern es sich um die ersten gehashten Werte handelt -- die Einträge 8 -- 12 belegt. Ein sechstes Element ebenfalls mit $h(k) = 8$ müsste nun schon sechs Schritte weitergehen, um einen freien Platz zu finden.
\subsection{Quadratisches Sondieren}
Ein bisschen erfolgsversprechender erscheint der folgende Ansatz: Hier gehen wir nicht nur 1er-Schritte in eine Richtung, sondern verändert Schrittrichtung und Schrittgröße in jeder Iteration.
\begin{align*}
h[0] &= H(k) \\
h[i] &= (h[0] + (-1)^{i+1} \cdot \lfloor (\frac{i+1}{2})^2 \rfloor) \text{ mod } n \\
&= h(k), h(k) + 1^2, h(k) - 1^2, h(k) + 2^2, h(k) - 2^2, ..., h(k) + (\frac{m-1}{2})^2, h(k) - (\frac{m-1}{2})^2
\end{align*} 
\paragraph{Probleme} Auch dieser Ansatz ist nicht unproblematisch: Zwar treten hier keine primären Cluster auf, dafür aber sekundäre. Für zwei Schlüssel mit demselben Hashwert sind auch die folgenden Positionen in der Hashtabelle gleich.
\subsection{Doppeltes Hashing}
Wir suchen also eine Lösung, dasssich für Elemente mit gleichem Hashwert unterschiedliche Sondierungsreihenfolgen ergeben. Dies erreichen wir, indem wir eine zweite Hashfunktion konstruieren und verwenden.
\begin{align*}
h[0] = H(k) \\
h[i] = (h[0] + i \cdot D(k)) \text{ mod } n
\end{align*}
Damit ergeben sich nun also unterschiedliche Sondierungsreihenfolgen auch in dem Fall, dass $H(k_1) = H(k_2)$ ist. Mit diesem Vorgehen approximieren wir das ideale Hashing.
\section*{Aufgabe}
Gegeben seien das Feld \\
\begin{center}
$\left[ \begin{array}{rrrrrrrr}
12 & 15 & 18 & 3 & 5 & 99 & 23 & 1
\end{array} \right]$ \\ und die Hashfunktionen 
\[h(x) = x \text{ mod } 9\] und
\[d(x) = x + 2 \text{ mod } 13\]
\end{center}
\begin{enumerate}
\item Berechnen Sie die Hashtabelle mit $m = 9$ durch \textbf{Lineares Sondieren}.
\item Berechnen Sie die Hashtabelle mit $m = 9$ durch \textbf{Quadratisches Sondieren}.
\item Berechnen Sie die Hashtabelle mit $m = 9$ durch \textbf{Doppeltes Hashing}.
\end{enumerate}
\begin{flushright}
Lösung auf Seite \pageref{a7:lsg}
\end{flushright}
\pagebreak
\part{Lösungen}
\section{Grundlagen der Algorithmik}
\paragraph{Lösung zu Aufgabe 1} \label{a1:lsg}Gesucht sind die Landau-Notationen $O, \Omega, \Theta$ zur Funktion $3n^3 + 7n^2 + 16$. Zunächst bestimmen wir die $O$-Komplexitätsklasse: Da der höchste Grad des Polynoms $3$ ist und sämtliche konstanten Faktoren oder Summanden entfallen, liegt die Funktion in $O(n^3)$. \\\\Für die $\Omega$-Notation überlegen wir uns Folgendes: Der kleinste Grad des Polynoms ist $0$ ($16 \cdot n^0$). Dies ist unsere Unterschranke: Da auch hier konstante Faktoren und Summanden unberücksichtigt bleiben, erhalten wir $\Omega(1)$. \\\\Die $\Theta$-Notation umfasst nun alle Funktionen, die sowohl Teil von $O(n^3)$ als auch von $\Omega(1)$ sind. Dies ist nun keine zeichenbare Funktion, sondern viel mehr der Flächeninhalt zwischen beiden Graphen. \\\\Wir zeichnen nun ein:
\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
	xmin = 0,
	ymin = 0,
	ymax = 50,
    width = \textwidth,
    height = 0.5\textwidth,
    xlabel = {$x$},
    ylabel = {$y$},]
\addplot[
    smooth,
    thick,
    green,
] {x^3};

\addplot[
    smooth,
    thick,
    blue,
] {3 * x^3 + 7 * x^2 + 16};

\addplot[
    smooth,
    thick,
    red,
] {1};
\legend{$O(n^3)$, $f(n)$, $\Omega(1)$}
\end{axis}
\end{tikzpicture}
\caption{Skizze zu Aufgabe 1.1}
\end{figure}
\paragraph{Lösung zu Aufgabe 2} \label{a2:lsg}
Anzugeben sind die $O$-Notationen zu gegebenen Funktionen.
\begin{enumerate}
\item $f(n) = 6n^4 + 3 \cdot log_2(n) \in O(n^4)$, da das Wachstum des Terms $n^4$ im Vergleich zu $log_2(n)$ m größten ist. Konstantglieder entfallen.
\item $f(n) = 6627816n + 13 \in O(n)$, da Konstantglieder immer entfallen und somit $n$ das größte Wachstum verursacht.
\item $f(n) = \frac{72n^{3} + 27n^{2} + 8n + 9}{n!} \in \frac{O(n^3}{O(n!)}$. Diese Angabe genügt, ein weiteres Auflösen ist nicht möglich oder sinnvoll. Diese Funktion kann keine typische Komplexität eines Algorithmus sein, da die Funktion gegen 0 konvergiert. Falls Sie sich an dieser Aufgabe die Zähne ausgebissen haben sollten, bitte ich um Vergebung!
\item $f(n) = 3n \cdot \frac{n!}{2} + n^n \in O(n^n)$, da dieser letzte Summand $n^n$ das größte Wachstum aufweist. Es ist damit Teil der letzten Klasse des exponentiellen Wachstums und selbst innerhalb dieser durch $a=n$ besonders schnell wachsend. 
\end{enumerate}
\paragraph{Lösung zu Aufgabe 3} \label{a3:lsg}
Aus gegebener Tabelle der Komplexitätsklassen wissen wir bereits, dass exponentielles Wachstum im Vergleich am schnellsten wächst. Hier ist nun zu zeigen, dass dies für sowohl für $n^2$ als auch für $n^3$ gilt, also beide Funktionen Teil der Komplexitätsklasse $O(2^n)$ sind (also nicht schneller wachsen als $2^n$). \\\\
Dies beweisen wir induktiv:
\begin{proof}
Aus der Definition der $O$-Notation leiten wir ab, dass wir ein beliebiges $n_0$ und ein $c$ wählen können. Wir setzen $n_0 = 4$, da wir wissen, dass $4^2 \leq 2^4$ ist. $4$ ist die erste natürliche Zahl, ab der dies gilt. Dass kleinere $n$ dies noch nicht erfüllen, ist unerheblich. $c = 1$, da hier kein Konstantfaktor benötigt wird.
\begin{align*}
\text{Induktionsanfang} \quad & 4^2 \leq 2^4 \\
\text{Induktionsvoraussetzung} \quad & n^2 \leq 2^n \\
\text{Induktionsschritt} \quad & n \to n + 1 \\
\text{Ausmult. + Einsetzen der IV} \quad & (n+1)^2 = n^2 + 2n + 1 \leq 2^n + 2n + 1 \\
...
\end{align*}
Der Rest des Beweises ist in den Vorlesungsunterlagen ersichtlich. Für den zweiten Aufgabenteil gehen wir ähnlich vor.
\end{proof}
\paragraph{Lösung zu Aufgabe 4} \label{a4:lsg}
Hier wenden wir das Master-Theorem an.
\begin{itemize}
\item $f(n) = 2 \cdot f(\frac{n}{2n+1}) + n^{2}$, daraus folgt: $a = 2, b = 2n + 1, d = 2$. Nun prüfen wir, ob gilt: \[a < b^d \to 2 < (2n+1)^2 \to 2 < 4n^2 + 2n + 1\]
Offensichtlich gilt die Aussage und somit tritt Fall 1 des Mastertheorems ein und wir erhalten: $O(n^2)$.
\item $g(n) = log(n) \cdot g(\frac{n}{2}) + 3$, also: $a = log(n), b = 2, d = 1$. Wir prüfen, ob die Bedingung des ersten Falls des Master-Theorems gilt:
\[a < b^d \to log(n) < 2^1 \to log(n) < 2\]
Offensichtlich gilt diese Bedingung nicht, da $log(n)$ für alle $n \geq 100$ größer ist als $2$. Also tritt Fall 3 des Mastertheorems ein und wir erhalten: $O(n^{log_2(log(n))})$.
\item $h(n) = sin(n) \cdot h(\frac{n}{\frac{n}{2}}) + O(n^{3})$, also $a = sin(n), b = \frac{n}{2}, d = 3$. Wir prüfen erneut: \[
a < b^d \to sin(n) < (\frac{n}{2})^3\]
Da $sin(n)$ nur Werte zwischen $-1$ und $1$ annimmt, ist die Bedingung für hinreichend große $n$ erfüllt. Damit erhalten wir: $O(n^3)$.
\end{itemize}
Auch bei der Anwendung des Mastertheorems ist für uns die langfristige Entwicklung des Wachstums relevant. Daher sind kleine $n$, die einen Widerspruch darstellen würden, unerheblich.
\paragraph{Lösung zu Aufgabe 5} \label{a1.5:lsg}
Wir betrachten die Anweisungen zunächst einzeln: \\\\
\begin{tabular}{lrp{5cm}}
\texttt{int[] result;} & $O(1)$ & Speicher wird reserviert \\
\texttt{for (dataElement in data)} & $O(n)$ & In Abhängigkeit zu $n$ \\
\texttt{if (...)} & $O(1)$ & konstanter Vergleich \\
\texttt{result.append(dataElement} & $O(1)$ & pro Iteration konstant, wegen Obergrenzenbetrachtung fällt Bedingung nicht ins Gewicht \\
\texttt{return result;} & $O(1)$ & konstante Ausgabe \\
\end{tabular} \\\\
Wir addieren nun alle Komplexitäten und erhalten $O(n + 4) \in O(n)$ als Gesamtkomplexität dieses Algorithmus.
\paragraph{Lösung zu Aufgabe 6} \label{a1.6:lsg}
Hier finden wir drei ineinander verschachtelte For-Schleifen:
\begin{enumerate}
\item Der Iterator der ersten For-Schleife wird in jedem Durchlauf verdoppelt und beginnt bei $i = 0$ bricht ab, sobald $i \geq n$. Wir suchen nun also ein $x$ mit $2^x = n$. Die Gleichung lösen wir durch den Logarithmus und erhalten entsprechend $O(log_{2}(n))$.
\item In der zweiten Schleife halbieren wir $j$ in jedem Durchlauf. $j$ initialisieren wir mit $n$ und lassen die For-Schleife laufen, bis $j \leq 0$. Also gehen wir im Prinzip wie in der ersten Schleife vor, bloß in umgekehrter Richtung. Dennoch erhalten wir auch hier eine Komplexität von $O(log_{2}(n))$.
\item In der dritten For-Schleife lassen wir $k$ von 1 bis $n$ laufen und inkrementieren $k$ in jeder Iteration um 1. Also finden hier insgesamt $n-1$ Durchläufe statt, was uns verdächtig stark an $O(n)$ erinnert, womit wir auch goldrichtig liegen. 
\end{enumerate}
Nun multiplizieren wir die Komplexitäten und erhalten die Gesamtkomplexität $O(n \cdot log_{2}(n)^2)$.
\paragraph{Lösung zu Aufgabe 7} \label{a1.7:lsg}
Wir erkennen, dass es sich um einen rekursiv implementierten Algorithmus handelt, da die Funktion sich zweimal selbst aufruft. Daher findet hier das \textbf{Mastertheorem} Anwendung, da es sich um ein {\glqq}Divide and Conquer{\grqq}-Problem handelt.
\begin{itemize}
\item Es finden zwei Rekursive Aufrufe statt, also setzen wir $a = 2$.
\item In jedem Rekursiven Aufruf halbieren wir die Datenmenge, also gilt $b = 2$.
\item Der Aufwand für das Zusammenfügen ist konstant, damit unerheblich und liegt daher in $O(1)$.
\end{itemize}
Wir setzen zusammen und stellen die Gleichung für das Mastertheorem auf:
\[T(n) = 2 \cdot T(\frac{n}{2}) + O(n^0)\]
Nun tritt der dritte Fall des Mastertheorems ein, da $2 > 2^0 = 1$ gilt. Wir erhalten als Gesamtkomplexität $O(n^{log_2(2)}) = O(n)$.
\section{Suchalgorithmen}
\paragraph{Lösung zu Aufgabe 1}
\label{a2.1:lsg}
Wir teilen das Feld rekursiv und vergleichen mit dem mittleren Element (im Zweifel das kleinere). \\
$\left[ \begin{array}{rrrrrrrrrrrrrrrrr}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 17 & 18 & 19 & 22
\end{array} \right]$ \\\\
$ 7 < 9$, also betrachten wir den linken Teil: \\
$\left[ \begin{array}{rrrrrrrr}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8
\end{array} \right]$ \\\\
$ 7 > 4$, also betrachten wir den rechten Teil: \\
$\left[ \begin{array}{rrrr}
5 & 6 & 7 & 8
\end{array} \right]$ \\\\
$7 > 6$, also betrachten wir den rechten Teil: \\
$\left[ \begin{array}{rr}
7 & 8
\end{array} \right]$ \\\\
$7 = 7$, wir haben das Element gefunden.
\paragraph{Lösung zu Aufgabe 2}
\label{a2.2:lsg}
Randbestimmung für den KMP-Suchalgorithmus. \\
\begin{tabular}{|l|c|}
\hline
Zeichenkette & Rand \\
\hline
$\emptyset$ & -1 \\
O & 0 \\
OT & 0 \\
OTT & 0 \\
OTTO & 1 \\
OTTOS & 0 \\
OTTOSM & 0 \\
OTTOSMO & 1 \\
OTTOSMOP & 0 \\
OTTOSMOPS & 0 \\
OTTOSMOPSK & 0 \\
OTTOSMOPSKO & 1 \\
OTTOSMOPSKOT & 2 \\
OTTOSMOPSKOTZ & 0 \\
OTTOSMOPSKOTZT & 0 \\
\hline
\end{tabular} \\\\
Die maximale Randlänge für dieses Muster beträgt 2 und diese tritt nur ein einziges Mal auf. Damit werden die Vorteile des KMP für dieses Muster unerheblich und es ergibt sich praktisch kein Vorteil gegenüber der einfachen Textsuche. \\\\
\begin{tabular}{|l|c|}
\hline
Zeichenkette & Rand \\
\hline
$\emptyset$ & -1 \\
A & 0 \\
AN & 0 \\
ANA & 1 \\
ANAN & 2 \\
ANANA & 1 \\
ANANAS & 0 \\
ANANASB & 0 \\
ANANASBA & 1 \\
ANANASBAN & 2 \\
ANANASBANA & 3 \\
ANANASBANAN & 4 \\
ANANASBANANA & 5 \\
\hline
\end{tabular} \\\\
Mit einer maximalen Randlänge von 5 ist dieses Muster deutlich besser geeignet als das erste. \\\\
\begin{tabular}{|l|c|}
\hline
Zeichenkette & Rand \\
\hline
$\emptyset$ & -1 \\
B & 0 \\
BO & 0 \\
BON & 0 \\
BONO & 1 \\
BONOB & 1 \\
BONOBO & 2 \\
\hline
\end{tabular} \\\\
Auch dieses Muster ist -- obwohl es sehr kurz ist und einen anderen Anschein macht -- nicht wirklich vorteilhaft im Gegensatz zur einfachen Textsuche.
\paragraph{Lösung zu Aufgabe 3}
\label{a2.3:lsg}
Wir suchen das Wort \textbf{Bonobo} im Suchraum \textbf{Beinahebotnochdasbonobonobohausplatz}. Dafür nutzen wir die Randbestimmung aus der oberen Aufgabe. \\\\ An dieser Stelle verzichte ich auf eine tabellarische Darstellung wie im Beispiel und stelle die Schritte im Folgenden in Textform dar:
\begin{enumerate}
\item Wir legen den Suchtext unter das Muster und stellen fest, dass der erste Buchstabe \textbf{B} übereinstimmt, der zweite jedoch schon nicht mehr. Nun ergibt sich durch die Formel
\[\text{Neuer Beginn} = \text{Suchtextposition} + (\text{Übereinstimmungen} - \text{Randlaenge[Musterindex])}\] der neue Beginn $1 = 0 + (1 - 0)$.
\item An Position 1 tritt keine Übereinstimmung auf, also $2 = 1 + (0 - (-1)$, wir verschieben unser Muster also an Position 2.
\item Auch an Position 2 tritt keine Übereinstimmung auf: $3 = 2 + (0 - (-1)$. So gehen wir vor, bis unser Muster an Index 7 steht.
\item Hier erkennen wir, dass das Muster \textbf{Bo} übereinstimmt. Es ergibt sich: $9 = 7 + (2 - (0)$. Von Position 9 an verschieben wir das Muster weiter in 1er-Schritten, bis wir bei \textbf{bonobonobohausplatz} ankommen, also an Index 17. \textbf{Hier ist das erste Vorkommen des Musters.} Wir verschieben zur Position $21 = 17 + (6 - 2)$, weil hier theoretisch ein weiteres Vorkommen beginnen könnte. 
\item Wir sehen: Das tut es auch. An Position 21 erhalten wir also das zweite Vorkommen des Musters. Wir verschieben zu $25 = 21 + (6 - 2)$.
\item Hier stimmen die ersten beiden Buchstaben überein, also gilt $27 = 25 + (2 - (0))$.
\item An Position 27 stimmen keine Buchstaben überein, wir rücken also wieder um jeweils einen Schritt nach vorne bis zum Index 31, weil ab dort die Anzahl der verbleibenden Zeichen geringer ist als die Länge des Musters.
\end{enumerate}
Wir haben nun also zwei Vorkommen unseres Musters im Text ermittelt.
\paragraph{Lösung zu Aufgabe 4}
\label{a2.4:lsg}
Hier sind selbstverständlich viele Lösungen richtig. Beispiel: \\
Das Verfahren der binären Suche operiert auf einem bereits sortierten Suchraum und halbiert diesen in jedem Rekursionsaufruf. Anschließend wird nur der Teil weiter betrachtet, in dem das Element auf Basis eines Vergleichs mit dem mittleren Element zu erwarten wäre. Der Algorithmus terminiert, wenn das Element gefunden wurde oder der betrachtete Suchraum einelementig ist.
\section{Sortieralgorithmen}
\paragraph{Lösung zu Aufgabe 1}
\label{a3.1:lsg}
\begin{itemize}
\item \textbf{Insertion Sort}
\begin{enumerate}
\item \texttt{[56, 10, 15, 98, 99, 12, 30, 80]}: 56 $>$ 10, also vertauschen..
\item \texttt{[10, 56, 15, 98, 99, 12, 30, 80]}: 10 $<$ 15 $<$ 56, also 15 zw. 10 und 56 einfügen.
\item \texttt{[10, 15, 56, 98, 99, 12, 30, 80]}: 10 $<$ 15 $<$ 56 $<$ 98, keine Änderung.
\item \texttt{[10, 15, 56, 98, 99, 12, 30, 80]}: 10 $<$ 15 $<$ 56 $<$ 98 $<$ 99
\item \texttt{[10, 15, 56, 98, 99, 12, 30, 80]}: 10 $<$ 12 $<$ 15, also 12 zw. 10 und 15 einfügen.
\item \texttt{[10, 12, 15, 56, 98, 99, 30, 80]}: 15 $<$ 30 $<$ 56, also 30 zw. 15 und 56 einfügen.
\item \texttt{[10, 12, 15, 30, 56, 98, 99, 80]}: 56 $<$ 80 $<$ 98, also 80 zw. 56 und 98 einfügen.
\item \texttt{[10, 12, 15, 30, 56, 80, 98, 99]}: Fertig, Algorithmus terminiert.
\end{enumerate}
\item \textbf{Bubblesort}
\begin{enumerate}
\item \texttt{[56, 10, 15, 98, 99, 12, 30, 80]}: 56 $>$ 10, also vertauschen.
\item \texttt{[10, 56, 15, 98, 99, 12, 30, 80]}: 56 $>$ 15, also vertauschen
\item \texttt{[10, 15, 56, 98, 99, 12, 30, 80]}: 56 $<$ 98, betrachten wir 98.
\item \texttt{[10, 15, 56, 98, 99, 12, 30, 80]}: 98 $<$ 99, betrachten wir 99.
\item \texttt{[10, 15, 56, 98, 99, 12, 30, 80]}: 99 $>$ 12, wir vertauschen.
\item \texttt{[10, 15, 56, 98, 12, 99, 30, 80]}: 99 $>$ 30, wir vertauschen.
\item \texttt{[10, 15, 56, 98, 12, 30, 99, 80]}: 99 $>$ 80, wir vertauschen.
\item \texttt{[10, 15, 56, 98, 12, 30, 80, 99]}: Ende des Feldes erreicht, da vertauscht wurde, beginnen wir erneut.
\item \texttt{[10, 15, 56, 98, 12, 30, 80, 99]}: 10 $<$ 15 $<$ 56 $<$ 98, aber 98 $>$ 12 (jeweils einzeln!).
\item \texttt{[10, 15, 56, 12, 98, 30, 80, 99]}: 98 $>$ 30, vertauschen.
\item \texttt{[10, 15, 56, 12, 30, 98, 80, 99]}: 98 $>$ 80, vertauschen.
\item \texttt{[10, 15, 56, 12, 30, 80, 98, 99]}: 98 $<$ 99, nächste Iteration.
\item \texttt{[10, 15, 56, 12, 30, 80, 98, 99]}: ..., 56 $>$ 12, vertauschen.
\item \texttt{[10, 15, 12, 56, 30, 80, 98, 99]}: 56 $>$ 30, vertauschen.
\item \texttt{[10, 15, 12, 30, 56, 80, 98, 99]}: ..., nächste Iteration.
\item \texttt{[10, 15, 12, 30, 56, 80, 98, 99]}: 15 $>$ 12, vertauschen.
\item \texttt{[10, 12, 15, 30, 56, 80, 98, 99]}: ..., nächste Iteration.
\item \texttt{[10, 12, 15, 30, 56, 80, 98, 99]}: ..., Fertig, Algorithmus terminiert (keine Vertauschungen in dieser Iteration).
\end{enumerate}
\item \textbf{Quicksort}
\begin{enumerate}
\item \texttt{[56, 10, 15, 98, 99, 12, 30, 80]}: 56 ist 1.Element, alle $p < 56$ vor 56 und alle $p > 56$ hinter 56 (unsortiert).
\item \texttt{[10, 15, 12, 30, 56, 98, 99, 80]}: Wir betrachten 10 und 98 und verfahren ebenso.
\item \texttt{[10, 15, 12, 30, 56, 80, 98, 99]}: Wir betrachten 15 und 80 und verfahren ebenso.
\item \texttt{[10, 12, 15, 30, 56, 80, 98, 99]}: Fertig, Algorithmus terminiert.
\end{enumerate}
\item \textbf{Mergesort}
\begin{enumerate}
\item \texttt{[56, 10, 15, 98, 99, 12, 30, 80]}: Zerteilen in Hälften.
\item \texttt{[56, 10, 15, 98][99, 12, 30, 80]}: Zerteilen.
\item \texttt{[56, 10][15, 98][99, 12][30, 80]}: Zerteilen und verbinden.
\item \texttt{[56][10][15][98][99][12][30][80]}: Sortieren und verbinden.
\item \texttt{[10, 56][15, 98][12, 99][30, 80]}: Sortieren und verbinden.
\item \texttt{[10, 15, 56, 98][12, 30, 80, 99]}: Sortieren und verbinden.
\item \texttt{[10, 12, 15, 30, 56, 80, 98, 99]}: Fertig, Algorithmus terminiert.
\end{enumerate}
\end{itemize}
\paragraph{Lösung zu Aufgabe 2}
\label{a3.2:lsg}
Auch hier sind natürlich wieder mehrere Lösungen richtig. \\\\
Beim Quicksort-Algorithmus wählen wir ein Vergleichselement (Pivot) und richten unser Array zunächst an diesem Element aus, d.h. alle kleineren Elemente notieren wir links dieses Pivots und alle größeren rechts. Nun suchen wir in dieser linken bzw. rechten Teilmenge wieder je ein Pivot und ordnen entsprechend um. Das wiederholen wir solange, bis die aufgespannten Mengen nur ein Element umfassen. Dann ist das Feld/die Menge sortiert.
\paragraph{Lösung zu Aufgabe 3}
\label{a3.3:lsg}
Anzugeben ist, mit welchem Algorithmus die Felder jeweils am besten zu sortieren sind. 
\begin{itemize}
\item Das Feld \texttt{[1, 9, 2, 8, 3, 7, 4, 6, 5]} ist über Insertion Sort am effizientesten zu sortieren, da wir in diesem Feld ein Muster erkennen: Alle Elemente mit geraden Index-Positionen (1, 2, 3, 4, 5) stehen für sich jeweils in einer richtigen Reihenfolge während alle Elemente auf geraden Index-Positionen (9, 8, 7, 6) in inverser Reihenfolge stehen. Würden wir Bubblesort verwenden, gäbe es einige Iterationen mit Vertauschungen, sodass sich der Aufwand insgesamt erhöht. Für Mergesort und Quicksort ist der Mehraufwand höher als der Nutzen, da die Datenmenge vergleichsweise klein und unkomplex ist.
\item Das Feld \texttt{[9, 7, 7, 2, 5, 4, 7, 3, 1]} wäre aufgrund der häufig vertretenen 7 für einen modifizierten Quicksort optimal (3-Way-Partitioning). Die 7 würde im zweiten rekursiven Aufruf Pivot werden und damit wären sehr schnell viele Elemente abgearbeitet.
\item Das Feld \texttt{[9, 8, 7, 6, 5, 4, 3, 2, 1]} liegt in der genau umgekehrten Reihenfolge vor und sollte daher unbedingt mit einem rekursiven Verfahren sortiert werden. Hier bietet sich Mergesort an, da bei Quicksort die Pivot-Elemente sehr ungünstigerweise jeweils die größten wären. Mergesort ist durch die Parallelität optimal.
\end{itemize}
\paragraph{Lösung zu Aufgabe 4}
\label{a3.4:lsg}
Bei Mergesort und Quicksort handelt es sich um rekursiv operierende Verfahren, die parallel bearbeitet werden. Hierfür bietet sich die Darstellung als Baum an: Für jede Menge, die halbiert wird, zeichnen wir ein Kindelment. Im Fall Mergesort spiegeln wir die Struktur des Baums im Mergeschritt und erhalten als Blatt die sortierte Gesamtmenge.
\section{Bäume}
\paragraph{Lösung zu Aufgabe 1}
\label{a4.1:lsg}
Zu dem gegebenen Baum sind zunächst Definitionen anzuwenden und anschließend zwei Operationen auszuführen.
\begin{itemize}
\item Die Wurzel ist der Knoten $F$.
\item Die Höhe des Baums beträgt 3.
\item Der Knoten $E$ hat die Höhe 1.
\item Die Ordnung des Baumes beträgt 2.
\item Die Balance des Knotens $K$ beträgt 1.
\end{itemize}
Nun ist der Knoten $J$ hinzuzufügen. Da $F < J$, fahren wir im rechten Teilbaum fort. $J < K$, also betrachten wir den linken Teilbaum von $K$ mit $H$ als Wurzel. $J > H$, also betrachten wir den rechten Teilbaum von $H$, der leer ist, also fügen wir dort als neuen Knoten $J$ ein.
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {F}
    child {node {B}
    child {node {A}}
    child {node {D}
    child {node {C}}
    child {node {E}}}}
    child {node {K}
    child {node {H}
    child[fill=none] {edge from parent[draw=none]}
    child {node[fill=yellow] {J}}}
    child {node {T}
    child[fill=none] {edge from parent[draw=none]}
    child {node {W}}}};
\end{tikzpicture}
\caption{Baum aus Aufgabe 1 nach Einfügen des Knotens $J$}
\end{figure} \\\\
Abschließend löschen wir den Knoten $B$. Wir stellen fest, dass $B$ zwei Kinder hat. Also ersetzen wir $B$ durch den kleinsten Schlüssel des rechten Teilbaums, also in unserem Fall $C$. Wir erhalten:
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {F}
    child {node[fill=yellow] {C}
    child {node {A}}
    child {node {D}
    child[fill=none] {edge from parent[draw=none]}
    child {node {E}}}}
    child {node {K}
    child {node {H}
    child[fill=none] {edge from parent[draw=none]}
    child {node {J}}}
    child {node {T}
    child[fill=none] {edge from parent[draw=none]}
    child {node {W}}}};
\end{tikzpicture}
\caption{Baum aus Aufgabe 1 nach Löschen des Knotens $B$}
\end{figure}
\paragraph{Lösung zu Aufgabe 2}
\label{a4.2:lsg}
Wir bestimmen zunächst die Balancen jedes Knotens.
\begin{itemize}
\item Knoten $A, C, E, H, W$ sind Blätter und haben daher die Balance 0.
\item Der Knoten $D$ hat die Balance 0.
\item Der Knoten $T$ hat die Balance 1.
\item Die Knoten $B, K$ haben die Balance 1.
\item Der Knoten $F$ hat die Balance 0.
\end{itemize}
Folglich ist die Balance an keinem Knoten des Baumes größer als $1$, womit dieses Kriterium erfüllt ist. Betrachten wir nun, ob es sich überhaupt um einen binären Suchbaum handelt. Da alle Knoten maximal zwei Kinder haben, ist der Baum binär. Dass es sich um einen Suchbaum handelt, ergibt sich aus der Anordnung der Knoten. \textbf{Also handelt es sich bei diesem Baum um einen AVL-Baum.}
\paragraph{Lösung zu Aufgabe 3}
\label{a4.3:lsg}
Da wir festgestellt haben, dass es sich um einen AVL-Baum handelt, können wir direkt beginnen, den Knoten $A$ zu löschen. Der Knoten $A$ ist ein Blatt, daher kann der entsprechende Zeiger einfach gleich null gesetzt werden. \\\\
Dies hat nun zur Folge, dass die Balance am Knoten $B$ 2 beträgt und der Baum daher rotiert werden muss. Dies ist am Knoten $D$ sinnvoll und möglich. Wir erhalten:
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {F}
    child {node[fill=yellow] {D}
    child {node {C}
    child {node[fill=yellow] {B}}
    child[fill=none] {edge from parent[draw=none]}}
    child {node {E}}}
    child {node {K}
    child {node {H}}
    child {node {T}
    child[fill=none] {edge from parent[draw=none]}
    child {node {W}}}};
\end{tikzpicture}
\caption{AVL-Baum nach Löschen des Knotens $A$ und Rotation um $D$}
\end{figure}
\\\\
Im nächsten Schritt fügen wir in den letzten Baum, den wir erhalten haben, den Knoten $Z$ hinzu. Da $Z$ der letzte Buchstabe des Alphabets ist, wird dieser Knoten also ein rechtes Kind vom Knoten $W$. Damit verliert der Baum erneut die AVL-Eigenschaft und Rotationen werden erforderlich. Wir rotieren den Baum um den Knoten $T$.
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {F}
    child {node {D}
    child {node {C}
    child {node {B}}
    child[fill=none] {edge from parent[draw=none]}}
    child {node {E}}}
    child {node[fill=yellow] {T}
    child {node {K}
    child {node {H}}
    child[fill=none] {edge from parent[draw=none]}}
    child {node {W}
    child[fill=none] {edge from parent[draw=none]}
    child {node[fill=yellow] {Z}}}};
\end{tikzpicture}
\caption{AVL-Baum nach Hinzufügen des Knotens $Z$ und Rotation um $T$}
\end{figure}
\paragraph{Lösung zu Aufgabe 4}
\label{a4.4:lsg}
\textit{Beschreibung und Pseudocode siehe Seite \pageref{searchBinTree}} \\
In einem Baum der Ordnung 3 müsste zunächst definiert werden, was diese Kategorie dann inhaltlich aussagt. Dies könnten Knoten sein, die gleich der Wurzel sind -- jedoch ergibt sich dadurch keinen Einsatzzweck eine Algorithmenerweiterung, weil nicht zwischen Knoten mit gleichem Wert unterschieden wird und werden sollte. Somit sind Sie gut beraten, nur auf binären Suchbäumen zu operieren (auch wenn es Spezialfälle geben mag).
\section{Heaps}
\paragraph{Lösung zu Aufgabe 1} Zu bestimmen ist, ob es sich bei diesem Baum um einen Heap handelt.
\label{a5.1:lsg}
\begin{enumerate}
\item Es handelt sich nicht um einen Heap, da der Knoten $R$ größer ist als der Knoten $M$. 
\item Um die Heap-Eigenschaft herzustellen, muss folglich der Knoten $R$ {\glqq}hochschwimmen{\grqq}. $M$ und $R$ tauschen also.
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=50mm]
\tikzstyle{level 2}=[sibling distance=30mm]
\tikzstyle{level 3}=[sibling distance=15mm]
 \node {S}
    child {node {K}
    child {node {D}
    child {node {A}}
    child {node {B}}}
    child {node {G}
    child {node {F}}
    child[fill=none] {edge from parent[draw=none]}}}
    child {node[fill=yellow] {R}
    child {node[fill=yellow] {M}}
    child {node {N}}};
\end{tikzpicture}
\end{figure}
\item Nun ist der Knoten $X$ hinzuzufügen. Wir fügen ihn an der nächsten freien Position im Heap (bzw. dessen Array) ein, also als rechtes Kind von $G$. Nun stellen wir fest, dass $G < X$, also führen wir \texttt{swim{\_}up(X)} aus. 
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=50mm]
\tikzstyle{level 2}=[sibling distance=30mm]
\tikzstyle{level 3}=[sibling distance=15mm]
 \node[fill=yellow] {X}
    child {node[fill=yellow] {S}
    child {node {D}
    child {node {A}}
    child {node {B}}}
    child {node[fill=yellow] {K}
    child {node {F}}
    child {node[fill=yellow] {G}}}}
    child {node {R}
    child {node {M}}
    child {node {N}}};
\end{tikzpicture}
\end{figure}
\end{enumerate}
\paragraph{Aufgabe 2} \label{a5.2:lsg}
Zunächst bilden wir den letzten aus Aufgabe 1 gewonnenen Heap als Array ab. Anschließend vertauschen wir erstes und letztes Element und lassen den neuen ersten Knoten absinken. \\
\begin{center}
$\left[ \begin{array}{rrrrrrrrrrr}
X & S & R & D & K & M & N & A & B & F & G \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrrrrr}
G & S & R & D & K & M & N & A & B & F & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrrrr|r}
S & G & R & D & K & M & N & A & B & F & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrrrr|r}
S & K & R & D & G & M & N & A & B & F & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrrrr|r}
F & K & R & D & G & M & N & A & B & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrrr|rr}
R & K & F & D & G & M & N & A & B & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrrr|rr}
R & K & N & D & G & M & F & A & B & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrrr|rr}
B & K & N & D & G & M & F & A & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrr|rrr}
N & K & B & D & G & M & F & A & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrr|rrr}
N & K & M & D & G & B & F & A & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrrr|rrr}
A & K & M & D & G & B & F & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrr|rrrr}
M & K & A & D & G & B & F & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrr|rrrr}
M & K & F & D & G & B & A & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrrr|rrrr}
A & K & F & D & G & B & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrr|rrrrr}
K & A & F & D & G & B & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrr|rrrrr}
K & G & F & D & A & B & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrrr|rrrrr}
B & G & F & D & A & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrr|rrrrrr}
G & B & F & D & A & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrr|rrrrrr}
G & D & F & B & A & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrrr|rrrrrr}
A & D & F & B & G & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrr|rrrrrrr}
F & D & A & B & G & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrrr|rrrrrrr}
B & D & A & F & G & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrr|rrrrrrrr}
D & B & A & F & G & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rrr|rrrrrrrr}
A & B & D & F & G & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rr|rrrrrrrrr}
B & A & D & F & G & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{rr|rrrrrrrrr}
A & B & D & F & G & K & M & N & R & S & X \\
\end{array} \right]$ \\
$\left[ \begin{array}{r|rrrrrrrrrr}
A & B & D & F & G & K & M & N & R & S & X \\
\end{array} \right]$ \\
Wir erhalten das fertig sortierte Feld: \\
$\left[ \begin{array}{rrrrrrrrrrr}
A & B & D & F & G & K & M & N & R & S & X \\
\end{array} \right]$ \\
\end{center}
\paragraph{Lösung zu Aufgabe 3}
\label{a5.3:lsg}
Erneut sind zwei Arrays gegeben, die mittels Heapsort aufsteigend sortiert werden sollen. Beim Herstellen der Heap-Eigenschaft wird -- eventuell anders als hier dargestellt -- bei $\frac{n}{2}$ begonnen.
\begin{itemize}
\item
\begin{center}
$ \left[ \begin{array}{rrrrrrrr}
7 & 1 & 3 & 9 & 4 & 6 & 5 & 8
\end{array} \right]$ \\ 
Herstellen der Heap-Eigenschaft: \\
$ \left[ \begin{array}{rrrrrrrr}
7 & 9 & 3 & 1 & 4 & 6 & 5 & 8
\end{array} \right]$ \\ 
$ \left[ \begin{array}{rrrrrrrr}
7 & 9 & 6 & 1 & 4 & 3 & 5 & 8
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrrrrrr}
7 & 9 & 6 & 8 & 4 & 3 & 5 & 1
\end{array} \right]$ \\ 
$ \left[ \begin{array}{rrrrrrrr}
9 & 8 & 6 & 7 & 4 & 3 & 5 & 1
\end{array} \right]$ \\
Nun beginnt das Sortieren: \\
$ \left[ \begin{array}{rrrrrrrr}
1 & 8 & 6 & 7 & 4 & 3 & 5 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrrrrr|r}
8 & 1 & 6 & 7 & 4 & 3 & 5 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrrrrr|r}
8 & 7 & 6 & 1 & 4 & 3 & 5 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrrrrr|r}
5 & 7 & 6 & 1 & 4 & 3 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrrrr|rr}
7 & 5 & 6 & 1 & 4 & 3 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrrrr|rr}
3 & 5 & 6 & 1 & 4 & 7 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrrr|rrr}
6 & 5 & 3 & 1 & 4 & 7 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrrr|rrr}
4 & 5 & 3 & 1 & 6 & 7 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrr|rrrr}
5 & 4 & 3 & 1 & 6 & 7 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrrr|rrrr}
1 & 4 & 3 & 5 & 6 & 7 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrr|rrrrr}
4 & 1 & 3 & 5 & 6 & 7 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rrr|rrrrr}
3 & 1 & 4 & 5 & 6 & 7 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rr|rrrrrr}
3 & 1 & 4 & 5 & 6 & 7 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{rr|rrrrrr}
1 & 3 & 4 & 5 & 6 & 7 & 8 & 9
\end{array} \right]$ \\
$ \left[ \begin{array}{r|rrrrrrr}
1 & 3 & 4 & 5 & 6 & 7 & 8 & 9
\end{array} \right]$ \\
Wir erhalten das fertig sortierte Array: \\
$ \left[ \begin{array}{rrrrrrrr}
1 & 3 & 4 & 5 & 6 & 7 & 8 & 9
\end{array} \right]$ \\
\end{center}
\item
\begin{center}
$ \left[ \begin{array}{rrrrrrrr}
H & S & G & A & V & D & J & K
\end{array} \right] $ \\
Herstellen der Heap-Eigenschaft: \\
$ \left[ \begin{array}{rrrrrrrr}
S & H & G & A & V & D & J & K
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrrrr}
S & V & G & A & H & D & J & K
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrrrr}
V & S & G & A & H & D & J & K
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrrrr}
V & S & J & A & H & D & G & K
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrrrr}
V & S & J & K & H & D & G & A
\end{array} \right] $ \\
Nun beginnen wir das Sortieren: \\
$ \left[ \begin{array}{rrrrrrrr}
A & S & J & K & H & D & G & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrrr|r}
S & A & J & K & H & D & G & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrrr|r}
S & K & J & A & H & D & G & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrrr|r}
G & K & J & A & H & D & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrr|rr}
K & G & J & A & H & D & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrr|rr}
K & H & J & A & G & D & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrrr|rr}
D & H & J & A & G & K & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrr|rrr}
J & H & D & A & G & K & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrrr|rrr}
G & H & D & A & J & K & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrr|rrrr}
H & G & D & A & J & K & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrrr|rrrr}
A & G & D & H & J & K & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrr|rrrrr}
G & A & D & H & J & K & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{rrr|rrrrr}
D & A & G & H & J & K & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{r|rrrrrrr}
A & D & G & H & J & K & S & V
\end{array} \right] $ \\
$ \left[ \begin{array}{r|rrrrrrr}
A & D & G & H & J & K & S & V
\end{array} \right] $ \\
Wir erhalten das sortierte Array: \\
$ \left[ \begin{array}{rrrrrrrr}
A & D & G & H & J & K & S & V
\end{array} \right] $ \\
\end{center}
\end{itemize}
\paragraph{Lösung zu Aufgabe 4}
\label{a5.4:lsg}
Grundsätzlich lassen sich alle Aufrufe von \texttt{sink()} durch \texttt{swim{\_}up()} austauschen. Im Falle des Heapsort werden bei jeder Vertauschung kleinere Knoten in die Wurzel eingefügt, also müssen diese Knoten sukzessive nach unten wandern bzw. alle anderen Knoten müssen entsprechend nach oben schwimmen. Wir müssten also für alle in Bezug zur Wurzel stehenden Elemente \texttt{swim{\_}up()} aufrufen. Das ist aufwendiger und weniger sinnvoll als \texttt{sink()} zu nutzen.
\begin{lstlisting}
void heapsort(A[]) {
	int n = A.length;
	for (int k = n/2; k >= 1; k--) {
		swim_up(A, k, n);
	}
	while (n > 1) {
		exchange(A[1], A[n]);
		n--;
		for (element in A[]) {
			swim_up(A, A[element], n);
		}	
	}
}
\end{lstlisting}
\section{Graphen}
\paragraph{Lösung zu Aufgabe 1}
\label{a6.1:lsg}
Anzugeben sind die Adjazenzmatrix und -liste zum gegebenen Graphen. \\
\begin{center}
$\left( \begin{array}{rrrrrr}
- & - & - & - & 2 & 3 \\
- & - & 6 & 3 & 9 & - \\
- & 6 & - & 2 & - & 2 \\
- & 3 & 2 & - & 1 & - \\
2 & 9 & - & 1 & - & - \\
3 & - & 2 & - & - & - 
\end{array} \right)$ \\
Und die dazugehörige Adjazenzliste: \\
\begin{tabular}{l | l}
A & E: 2, F: 7 \\
B & C: 6, D: 3, E: 9 \\
C & B: 6, D: 2, F: 2 \\
D & B: 3, C: 2, E: 1 \\
E & A: 2, B: 9, D: 1 \\
F & A: 3, C: 2
\end{tabular}
\end{center}
\paragraph{Lösung zu Aufgabe 2} \label{a6.2:lsg} Durchzuführen sind hier sowohl die Breiten- als auch Tiefensuche.
\begin{itemize}
\item Breitensuche 
\begin{enumerate}
\item Wir wählen als Startknoten $B$, markieren ihn und fügen ihn der Queue hinzu. \\
\texttt{Q = \{B\}}, durchlaufene Knoten: []
\item Wir besuchen $B$, fügen alle Nachbarn hinzu. \\
\texttt{Q = \{C, D, E\}}, durchlaufene Knoten: [B]
\item Wir besuchen den Knoten $C$, markieren ihn und fügen alle noch unmarkierten Nachbarn der Queue hinzu. \\
\texttt{Q = \{D, E, F\}}, durchlaufene Knoten: [B, C]
\item Wir besuchen den Knoten $D$, markieren ihn und fügen alle noch unmarkierten Nachbarn hinzu. \\
\texttt{Q = \{E, F\}}, durchlaufene Knoten: [B, C, D]
\item Wir besuchen den Knoten $E$ und verfahren ebenso. \\
\texttt{Q = \{F, A\}}, durchlaufene Knoten: [B, C, D, E]
\item Wir besuchen den Knoten $F$, stellen fest, dass es keine weiteren Nachbarn gibt. \\
\texttt{Q = \{A\}}, durchlaufene Knoten: [B, C, D, E, F]
\item Nun besuchen wir abschließend $A$. Auch $A$ hat keine weiteren unmarkierten Nachbarn. Damit sind wir fertig. \\
\texttt{Q = \{\}}, durchlaufene Knoten: [B, C, D, E, F, A]
\end{enumerate}
\item Tiefensuche
\begin{enumerate}
\item Wir wählen wieder $B$ als Startknoten und fügen ihn dem Stack hinzu. \\
\texttt{S = \{B\}}, durchlaufene Knoten: []
\item Wir nehmen $B$ vom Stapel, markieren es und fügen die Nachbarn hinzu. \\
\texttt{S = \{C, D, E\}}, durchlaufene Knoten: [B]
\item Nun nehmen wir das letzte Element vom Stapel, besuchen und markieren es und fügen die Nachbarn hinzu. \\
\texttt{S = \{C, D, A, D\}}, durchlaufene Knoten: [B, E]
\item Wir besuchen nun erneut den letzten Knoten des Stapels und markieren ihn. \\
\texttt{S = \{C, D, A, C\}}, durchlaufene Knoten: [B, E, D]
\item Nächstes Element ist folglich $C$. \\
\texttt{S = \{C, D, A, F\}}, durchlaufene Knoten: [B, E, D, C]
\item Nächstes Element ist der Knoten $F$. \\
\texttt{S = \{C, D, A, A\}}, durchlaufene Knoten: [B, E, D, C, F]
\item Nächstes Element ist der Knoten $A$. $A$ hat keine weiteren Nachbarn, die noch nicht markiert wurden. \\
\texttt{S = \{C, D, A\}}, durchlaufene Knoten: [B, E, D, C, F, A]
\item Selbiges gilt für das nächste $A$ des Stacks. \\
\texttt{S = \{C, D\}}, durchlaufene Knoten: [B, E, D, C, F, A]
\item Auch Knoten $D$ wurde bereits besucht. \\
\texttt{S = \{C\}}, durchlaufene Knoten: [B, E, D, C, F, A]
\item Der letzte verbleibende Knoten $C$ wurde bereits markiert und kann damit ebenfalls entfernt werden. \\
\texttt{S = \{\}}, durchlaufene Knoten: [B, E, D, C, F, A]
\end{enumerate}
Damit ist die gewünschte Suchreihenfolge gefunden.
\end{itemize} 
\paragraph{Lösung zu Aufgabe 3}
\label{a6.3:lsg}
Nun ist der Dijkstra-Algorithmus auf diesen Graphen anzuwenden. 
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
u    & A 				& B 			& C 			& D 			& E 		& F \\
\hline
init & $\infty \mid$ -- 	& $0 \mid$ B &  $\infty \mid$ -- &  $\infty \mid$ --  &  $\infty \mid$ -- &  $\infty \mid$ --  \\
\hline
B 	&					&			& 6 $\mid$ B	& 3 $\mid$ B	& 9 $\mid$ B & \\
\hline
D	&					&			& 5 $\mid$ D&			& 4 $\mid$ D & \\
\hline
E   	& 6 $\mid$ E			&			&			&			&		&	\\
\hline
C 	&					&			&			&			&		& 8 $\mid$ C \\
\hline
A	& 					&			&			&			&		& 13 $\mid$ A\\
\hline
F 	&					&			&			&			&		& \\
\hline
\end{tabular}
\caption{Dijkstra-Algorithmus zum Graphen}
\end{table}
Daraus ergibt sich nun auch der Kürzeste-Wege-Baum:
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node[main] {B}
    child {node[main] {D}
    child {node[main] {C}
    child {node[main] {F}
    edge from parent node[right]{2}}
    edge from parent node[right]{2}}
    child {node[main] {E}
    child {node[main] {A}
    edge from parent node[right]{2}}
    edge from parent node[right]{1}}
    edge from parent node[right]{3}};
\end{tikzpicture}
\caption{Kürzeste-Wege-Baum zum Graphen}
\end{figure}
\paragraph{Lösung zu Aufgabe 4}\label{a6.4:lsg}
Der hauptsächliche Unterschied liegt im \textit{LIFO}-Prinzip bei der Queue und im \textit{FIFO}-Prinip beim Stack. Während die Queue immer das Element ausgibt, was am längsten Teil der Queue ist (also wartet), gibt der Stack das Element zurück, dass als letztes hinzugefügt wurde. Darüber wird erreicht, dass die Tiefensuche immer weiter in die Tiefe geht und nicht in großen Schritten zu weiter entfernt liegenden Knoten zurück und die Breitensuche eben auf dem Weg nichts außer Acht lässt. Algorithmen mit anderen Datenstrukturen sind denkbar, wenn auch nicht sinnvoll.
\section{Hashing}
\paragraph{Lösung zur Aufgabe} \label{a7:lsg} Zu ermitteln sind drei Hashtabellen.
\begin{itemize}
\item \textbf{Lineares Sondieren}
\begin{center}
\begin{tabular}{|c|c|}
\hline
k & h(k) \\
\hline
12 & 3 \\
15 & 6 \\
18 & 0 \\
3 & 3, 4 \\
5 & 5 \\
99 & 0, 1 \\
23 & 5, 6, 7 \\
1 & 1, 2 \\
\hline
\end{tabular} \\
\end{center}
Dazugehörige Hash-Tabelle:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
18 & 99 & 1 & 12 & 3 & 5 & 15 & 23 & \\
\hline
\end{tabular}
\end{center}
\item \textbf{Quadratisches Sondieren}
\begin{center}
\begin{tabular}{|c|c|}
\hline
k & h(k) \\
\hline
12 & 3 \\
15 & 6 \\
18 & 0 \\
3 & 3, 4 \\
5 & 5 \\
99 & 0, 1 \\
23 & 5, 6, 4, 1, 1, 5, 5, 3, 7 \\
1 & 1, 2 \\
\hline
\end{tabular} \\
\end{center}
Dazugehörige Hash-Tabelle:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
18 & 99 & 1 & 12 & 3 & 5 & 15 & 23 & \\
\hline
\end{tabular}
\end{center}
\item \textbf{Doppeltes Hashing}
\begin{center}
\begin{tabular}{|c|c|}
\hline
k & h(k) \\
\hline
12 & 3 \\
15 & 6 \\
18 & 0 \\
3 & 3, 8 \\
5 & 5 \\
99 & 0, 1 \\
23 & 5, 8, 3, 5, 8, 2 \\
1 & 1, 4 \\
\hline
\end{tabular} \\
\end{center} 
Dazugehörige Hash-Tabelle:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
18 & 99 & 23 & 12 & 1 & 5 & 15 & & 3 \\
\hline
\end{tabular}
\end{center}
\end{itemize}
\pagebreak
\part{Verzeichnisse}
\listoftables
\listoffigures
\section*{Weiterführende Übungsaufgaben}
Falls Sie weitere Aufgaben suchen, finden Sie bei \cite{GrUeb} zu nahezu allen Themen dieses Moduls mehrere, gut ausgearbeitete Übungen und Lösungen. Hilfreich ist es ebenso aber auch, wenn Sie sich eigene Übungsaufgaben ausdenken und diese dann lösen. Dann stoßen Sie unweigerlich auf Fallstricke, Hürden und entwickeln eine Routine, die Sie vermutlich einigermaßen sicher durch die Klausur/Prüfung bringt.
\printbibliography
\end{document}