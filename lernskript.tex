\documentclass[11pt,a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{upgreek}
\usepackage{amsthm}
\usepackage{ulem}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage[backend=biber, style=alphabetic]{biblatex}
\addbibresource{literature.bib}
\author{Roman Wetenkamp}
\title{Algorithmen \& Komplexität}
\subtitle{Theoretische Informatik}

\newtheorem{note}{Bemerkung}
\newtheorem{definition}{Definition}
\newtheorem{satz}{Satz}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Beispiel}
\begin{document}
\vspace{3cm}
\maketitle
\begin{center}
\includegraphics[scale=0.7]{DHBW.jpg}
\end{center}
\pagebreak
\tableofcontents
\pagebreak
\section*{Vorwort}
Die theoretische Informatik ist genau das Thema, vor dem ich vor Beginn meines Studiums am meisten Respekt hatte ... Und das ist es neben den anderen Mathematik-Vorlesungen bis heute: Theoreme, Kalküle und Formeln sind nun einmal nicht die Dinge, mit denen sich die Masse der Informatikstudierenden gerne befasst, mich eingeschlossen. Mit einer (vermutlich) gerade eben bestandenen Logikklausur im ersten Semester sind dies Gründe genug dafür ergänzendes, motivierendes und begreifbares Material zusammenzustellen, dass mich einerseits akut durch die Klausur dieses Semesters bringt und hoffentlich auch für andere einen Nutzen hat. Geteiltes Leid ist halbes Leid! \\\\
Hier möchte ich den Vorlesungsstoff auf meine Art zusammenfassen und komplettieren, um Beispiele ergänzen und mit Aufgaben versehen, wie ich sie in der Vorlesung oder weiteren Büchern antraf. Ich persönlich lerne durch Aufgaben einfach am besten und für diejenigen, denen es auch so geht, gibt es davon hier ausreichend. Lösungen finden sich (in einigen Wochen) im Anhang. \\\\
\textit{Viel Erfolg!}  \\
\begin{flushright}
Roman Wetenkamp \\
Mannheim, den \today
\end{flushright}  
\vfill
\paragraph{Hinweis}
Dieses Dokument ist kein Vorlesungsmaterial, hat keineswegs den Anspruch auf Vollständigkeit und enthält mit Sicherheit Fehler. Desweiteren ist es noch lange nicht vollendet (es ist infrage zustellen, ob es das je sein wird), und doch möchte ich Sie ermutigen, beizutragen! Jegliche Fehler, Probleme oder Anmerkungen können Sie mir gerne über das dazugehörige GitHub-Repository unter der URL \url{https://github.com/RWetenkamp/algokomp} zukommen lassen. Ebenso sind Sie völlig frei darin, dieses Dokument für Ihre legitimen Zwecke zu nutzen -- bitte beachten Sie dennoch Ihr Gewissen. Danke!
\pagebreak
\part{Grundlagen der Algorithmik}
\section{Der Algorithmusbegriff}
Wie der Titel dieses Moduls schon verrät, befassen wir uns mit Algorithmen: Jenen nebulösen Dingen, die uns mit ähnlich Denkenden und Interessierten in Social-Media-Plattformen zusammenbringen, komplexe mathematische Berechnungen ausführen und zunehmend mehr an Einfluss in unserem Leben gewinnen. Wir betrachten hier nun die Wortherkunft, definieren den Begriff anschließend und gehen auf Eigenschaften eines solchen ein.
\paragraph{Wortherkunft}
Der Begriff {\glqq}Algorithmus{\grqq} besteht zum einen aus dem altgriechischen Wort \textit{arithmos} -- Zahl und zum anderen geht er zurück auf den persischen Mathematiker \textsc{al-Charismi} (780-846 n. Chr.), dessen Werk \textit{{\glqq}Algorismus{\grqq}} schon eine gewisse Nähe zum heute üblichen Begriff offenbart. \parencite{brockhaus:algorithmus}
\begin{definition}
Unter einem Algorithmus verstehen wir eindeutige Verarbeitungsvorschriften zur Lösung eines Problems oder einer Problemklasse. Daran geknüpft ist der Gedanke des EVA-Prinzips: Eine Eingabe wird verarbeitet und auf Grundlage dessen wird eine Ausgabe erzeugt, ein Algorithmus ist hier für die Verarbeitung bis zur Ausgabe notwendig. 
\end{definition}
Algorithmen finden in einer Vielzahl von Bereiches unseres Lebens Einsatz: 
\begin{itemize}
\item Zur Untersuchung großer Datenmengen
\item Zur Kommunikation/Suche im Internet
\item In bildgebenden Verfahren oder diagnostischen Anwendungen der Medizin
\item In Assistenzsystemen im Auto
\item Bei der Partnersuche über Online-Dating-Plattformen
\end{itemize}
Damit finden wir intuitiv eine Begründung für die Untersuchung von Algorithmen.
\begin{note}
Ein \textit{Algorithmus} ist von einem \textit{Programm} unbedingt abzugrenzen: Der Algorithmus bezeichnet ein abstraktes Konzept zur Lösung eines gegebenen Problems während ein Programm die konkrete Umsetzung eines Algorithmus in einer Programmiersprache ist. (Implementierung) 
\end{note}
Bei der Betrachtung von Algorithmen betrachten wir die folgenden drei Ziele:
\begin{itemize}
\item Korrektheit
\item Effizienz
\item Einfachheit
\end{itemize}
\paragraph{Einfachheit}
Aus wirtschaftlichen Gründen besteht ein Interesse daran, dass ein Algorithmus so einfach wie möglich ist, beispielsweise in Bezug auf Implementierungskosten. Ein weiterer Aspekt besteht in der Prüfung auf Korrektheit: Zu einem zu komplexen Algorithmus lässt sich nur schwer feststellen, ob dieser korrekt ist.
\paragraph{Korrektheit}
Die Korrektheit ist die zentrale Eigenschaft eines Algorithmus -- Wenn ein Algorithmus nicht das leistet, was er vorgibt zu leisten oder was von ihm erwartet wird, ist er wertlos. Die Spezifikation beschreibt das exakte Verhalten unter Angabe von einer Vorbedingung (Zustand der Daten vor der Ausführung) und einer Nachbedingung (erwünschter Zustand nach der Ausführung).
\begin{definition}
Ein Algorithmus heißt \textbf{partiell korrekt}, falls aus erfüllter Vorbedingung die Nachbedingung folgt. \\
Ein Algorithmus heißt \textbf{total korrekt}, falls er partiell korrekt ist und nach endlich vielen Schritten terminiert.
\end{definition}
Die Korrektheit eines Algorithmus kann manuell, per Implementierung oder mathematisch verifiziert werden, wobei letztere Variante häufig vorzuziehen ist.
\begin{note}
Nicht jeder Algorithmus muss für jede Eingabe terminieren. Unter dem \textbf{Halteproblem} versteht man die Frage, ob ein Algorithmus nach endlich vielen Schritten terminiert. Diese Frage ist algorithmisch nicht entscheidbar, wie \textsc{Alan Turing} feststellte. Stattdessen muss die Termination für jeden Algorithmus einzeln entschieden werden.
\end{note}
\section{Effizienz / Komplexität}
Weitaus komplexer als die Frage der Einfachheit oder Korrektheit ist jene der Komplexität selbst:
\begin{definition}
Die Effizienz / Komplexität eines Algorithmus ist ein Maß für die Menge an Ressourcen (Rechenzeit oder Speicherbedarf), die er benötigt.
\end{definition}
Dabei ist zu beachten, dass es für ein Problem häufig mehrere Algorithmen gibt, die sich in ihrer Komplexität unterscheiden. Daher muss die Komplexität für jeden Algorithmus einzeln bestimmt werden. Dieses Skript befasst sich im Wesentlichen nur mit der Rechenzeit. \\\\
Für eine wirklich präzise Aussage über die Rechenzeit wäre es erforderlich, für jede Implementation eines Algorithmus in einer Programmiersprache einzeln festzustellen, welche Rechenzeit jede einzelne Operation auf dem spezifischen Prozessor des Testsystems benötigt. Da diese Aussagen mittlerweile aufgrund der Entwicklungen auf dem Prozessormarkt an Relevanz verlieren, verlagert sich die Betrachtung auf die Komplexität eines Algorithmus. \\\\
Die Komplexität eines Algorithmus kann folglich keine genaue Angabe in einer Zeiteinheit sein, die beispielsweise aussagt, wie schnell denn nun wirklich der größte gemeinsame Teiler von 443 und 123 mithilfe des euklidischen Algorithmus berechnet werden kann, sondern ist ein abstraktes Konzept, das Vergleiche zwischen Algorithmen ermöglichen soll:
\begin{itemize}
\item Abstraktion konstanter Faktoren
\item Abstraktion unbedeutender Terme
\item Wachstumsverhalten der Laufzeit bei Veränderung der Größe der Eingabe
\end{itemize}
Diese Abstraktion motiviert nun den folgenden Abschnitt.
\section{Maß der Komplexität: $O$-Notation}
Nach dem Mathematiker \textsc{Edmund Landau} definieren wir in Bezug auf das Wachstum von Funktionen eine {\glqq}obere Schranke{\grqq} und nennen diese $O(f)$. Falls eine Funktion $g$ nicht schneller wächst als $f$, so sagen wir, dass $g \in O(f)$. Eine ebenfalls übliche, jedoch irritierende Schreibweise ist $g = O(f)$.
\begin{definition}
Sei $\mathbb{R_{+}} := \{ x \in \mathbb{R} \mid x > 0 \}$ und $\mathbb{R_{+}^{\mathbb{N}}} := \{f \mid f \text{ist eine Funktion der Form } f : \mathbb{N} \to \mathbb{R_{+}}\}$ \\\\
Sei $g \in \mathbb{R_{+}^{\mathbb{N}}}$ gegeben. Dann ist die Menge aller Funktionen, die höchstens so schnell wachsen wie $g$ definiert als: \begin{align*}
O(g) := \{f \in \mathbb{R_{+}^{\mathbb{N}}} \mid \exists n_{0} \in \mathbb{N}: \exists c \in \mathbb{R_{+}}: \forall n \in \mathbb{N}: (n \geq n_{0} \Rightarrow f(n) \leq c \cdot g(n))\}
\end{align*}
\end{definition}
Die $O$-Notation gibt folglich eine Menge vergleichbarer Funktionen an, die es ermöglicht, die Komplexität eines Algorithmus abzuschätzen.
\begin{example}
Behauptung:
$3n^{3} + 2n^{2} + 7 \in O(n^{3})$
\begin{proof}

Gesucht ist eine Konstante $c \in \mathbb{R_{+}}$ und eine Konstante $n_{0} \in \mathbb{N}$, sodass für alle $n \in \mathbb{N}$ mit $n \geq n_{0}$ gilt: \\
$3n^{3} + 2n^{2} + 7 \leq c \cdot n^{3}$ \\\\
Wähle $n_{0} := 1 \text{ und } c := 12$ \footnote{Die Wahl von $c$ und $n$ erfolgt hier durch Ausprobieren -- In diesem Fall ergibt sich 12 als Summe aus den Potenzfaktoren.} \\
Sei nun 
\begin{align}
& 1 \leq n \\
(1)^{3}: \quad & 1 \leq n^{3} \\
(2) \cdot 7: \quad & 7 \leq 7n^{3} \\
(1) \cdot 2n^{2}: \quad & 2n^{2} \leq 2n^{3} \\
\quad & 3n^{3} \leq 3n^{3} \\
(3)+(4)+(5): \quad & 3n^{3} + 2n^{2} + 7 \leq 12n^{3}
\end{align}
Durch die Ungleichungen (3), (4) und (5) ist für jedes Polynom einzeln eine Abschätzung erfolgt. Addiert man diese Ungleichungen nun, ergibt sich (6). Damit ist gezeigt, dass die Behauptung wahr ist und die Funktion in $O(n^{3})$ liegt. Aus der Definition der $O$-Notation ergibt sich, dass jede Funktion mit höchstem Grad 3 in $O(n^{3})$ liegt, da Vorfaktoren und Konstantglieder entfallen. 
\end{proof}
\end{example} 
Die $O$-Notation eines Algorithmus lässt sich auch durch vollständige Induktion beweisen.
\begin{example}
Behauptung: $n \in O(2^{n})$
\begin{proof}
Wähle $n_{0} := 0, c:= 1$. Zu zeigen: $n \leq 2^{n} \forall n \in \mathbb{N}$\\\\
\uline{Beweis durch Induktion nach $n$:} \\
Induktionsanfang (IA): $n = 0 \quad n = 0 \leq 1 = 2^{0} = 2^{n}$ \\
Induktionsvoraussetzung (IV): $n \leq 2^{n}$ z.Z. $n+1 \leq 2^{n+1}$ \\
Induktionsschritt (IS): $n \mapsto n + 1$ \\
Per einfacher Induktion kann man nun zeigen: 
\begin{align}
& n \leq 2^{n} \\
\text{Daraus folgt mit IV:} \quad & n+1 \leq 2^{n} + 2^{n} = 2*2^{n} = 2^{n+1}
\end{align}
\end{proof}
\end{example} 
Die Eigenschaft $f \in O(g)$ induktiv zu zeigen, ist mühsam. Stattdessen können die folgenden Propositionen genutzt werden: 
\begin{note}
\begin{align}
f \in O(f) \\
f \in O(g) \Rightarrow d \cdot f \in O(g) \\
f \in O(n) \land g \in O(n) \Rightarrow f + g \in O(n) \\
f_{1} \in O(g_{1}) \land f_{2} \in O(g_{2}) \Rightarrow f_{1} \cdot f_{2} \in O(g_{1} \cdot g_{2}) \\
f_{1} \in O(g_{1}) \land f_{2} \in O(g_{2}) \Rightarrow \frac{f_{1}}{f_{2}} \in O(\frac{g_{1}}{g_{2}}) \\
f \in O(g) \land g \in O(n) \Rightarrow f \in O(n)
\end{align}
\end{note}
Darüber hinaus gilt folgender Satz: 
\begin{satz}
Seien $f, g: \mathbb{N} \to \mathbb{R_{+}}$. Dann gilt: \\
\[f(n) \in O(g(n)) \iff (\frac{f(n)}{g(n)})_{n \in \mathbb{N}} \text{ ist beschränkt}\]
\end{satz}
Außerdem finden Grenzwertbetrachtungen Eingang:
\begin{lemma}
\begin{align}
f \in O(g) \text{ und } g \in O(f), \text{ wenn } \lim \limits_{n \to \infty} (\frac{f(n)}{g(n)}) = c, c \neq 0 \\
f \in O(g) \text{ und } g \notin O(f), \text{ wenn } \lim \limits_{n \to \infty} (\frac{f(n)}{g(n)}) = 0 \\
f \notin O(g) \text{ und } g \in O(f), \text{ wenn } \lim \limits_{n \to \infty} (\frac{f(n)}{g(n)}) = \infty
\end{align}
\end{lemma}
Neben der $O$-Notation gibt es zwei weitere Notationen, die im Folgenden vorgestellt werden.
\subsection{$\Omega$-Notation}
Neben der Menge aller Funktionen, die höchstens so schnell wächst wie die betrachtete Funktion, bezeichnen wir die Menge der Funktionen, die mindestens so schnell wächst wie die Funktion als $\Omega(n)$.
\begin{definition} Sei $g \in \mathbb{R_{+}}$. Die Menge aller Funktionen, die mindestens so schnell wachsen wie $g$ ist: 
\[\Omega(n) := \{ f \in \mathbb{R_{+}^{\mathbb{N}}} \mid \exists n_{0} \in \mathbb{N}: \exists c \in \mathbb{R_{+}}: \forall n \in \mathbb{N}: (n \geq n_{0} \Rightarrow F(n) \geq c \cdot g(n))\}\]
\end{definition}
Diese Notation fristet ein Schattendasein: In der Regel ist die $O$-Notation geeigneter, da eine Orientierung am {\glqq}Worst-Case-Szenario{\grqq} üblicher ist.
\subsection{$\Theta$-Notation}
Aus $O$-Notation und $\Omega$-Notation ergibt sich nun die Menge der Funktionen, die genau so schnell wachsen wie die betrachtete Funktion. Wir bezeichnen sie mit $\Theta$.
\begin{definition}
\[\Theta(g) = O(g) \cap \Omega(g)\]
\end{definition}
\subsection{Zusammenfassung}
\begin{itemize}
\item Die $O$-Notation umfasst alle Funktionen, die nicht schneller wachsen als $f$. Damit beschreiben wir eine obere Schranke für die Komplexitätsfunktion.
\item Die $\Omega$-Notation umfasst alle Funktionen, die mindestens so schnell wachsen wie $f$. Dies ist eine untere Schranke.
\item Die $\Theta$-Notation umfasst alle Funktionen, die genau so schnell wachsen wie f. Damit erhalten wir ein asymptotisches Maß.  
\end{itemize}
\begin{lemma}
\begin{itemize}
\item $f \in O(g) \text{ und } g \in O(f), \text{ also } f \in O(g) und f \in \Omega (g), \text{ wenn } \lim \limits_{n \to \infty} \frac{f(n)}{g(n)} = c, c \neq 0, \text{ also: } f \in \Theta (g)$
\item $f \in O(g) \text{ und } g \notin O(f), \text{ wenn } \lim \limits_{n \to \infty} \frac{f(n)}{g(n)} = 0, \text{ also: } g \in \Omega (f) \text{ und } f \notin \Omega (g)$
\item $ f \notin O(g) \text{ und } g \in O(f), \text{ also } g \notin \Omega (f) \text{ und } f \in \Omega (g), \text{ wenn } \lim \limits_{n \to \infty} \frac{f(n)}{g(n)} = \infty$
\end{itemize}
\end{lemma}
\section{Rekursionsgleichungen}
Nun haben wir mit den Landau-Symbolen einen mathematischen Weg gefunden, die Komplexität von Algorithmen zu beschreiben. Im Folgenden wenden wir dieses Modell auf konkrete Algorithmen an. \\\\
Bekannte Algorithmen sind beispielsweise Such- oder Sortierverfahren. Diese sind häufig rekursiv, d.h. eine Iteration verarbeitet direkt das Ergebnis einer vorherigen. Dadurch ergeben sich nun sogenannte \textbf{Rekurrenzgleichungen} oder auch \textbf{Rekursionsgleichungen}.
\begin{example}
$T(n) = 2 \cdot T(n-1) + 1$ ist eine Rekurrenzgleichung, da der Funktionswert $n$ direkt vom Funktionswert $n-1$ abhängt. Aus den Programmierkonzepten ist uns Rekursion als ein {\glqq}Wiederaufruf von sich selbst{\grqq} bekannt, dieses Muster findet sich hier in unseren betrachteten Algorithmen.
\end{example}
Um die Komplexität eines rekursiven Algorithmus bestimmen zu können, bedienen wir uns je nach Rekursionsgleichung unterschiedlichen Verfahren. Eines davon ist das sogenannte Mastertheorem. 
\subsection{Mastertheorem}
\begin{satz} Sofern alle Teilprobleme die gleiche Größe haben, können wir zwei Fälle unterscheiden: 
\begin{itemize}
\item Basisfall: $f(n)$ ist konstant für hinreichend kleine Werte von $n$
\item Für größere $n$ lässt sich eine Rekursionsgleichung der folgenden Form bestimmen: 
\[f(n) = a \cdot f(\frac{n}{b}) + O(n^{d})\]
$a$ ist hier die Anzahl der rekursiven Aufrufe, $b$ der Verkleinerungsfaktor des Problems und $d$ der Exponent für die Laufzeit der Vorbereitung der Rekursion und/oder die Zusammensetzung der Teilprobleme. \\\\
Nun gilt Folgendes: \\
\begin{align*}
f(n) \in 
\begin{cases}
O(n^{d}) \quad & \text{ falls } a < b^{d} \\
O(n^{d} \cdot log_{b}(n)) \quad & \text{ falls } a = b^{d} \\
O(n^{log_{b}(a)}) \quad & \text{ falls } a > b^{d}
\end{cases}
\end{align*}
\end{itemize}
\end{satz}
Mithilfe dieses Theorems kann nun die Komplexität eines rekursiven Algorithmus bestimmt werden:
\begin{example}
Gegegeben sei \[f(n) = 2 \cdot f(\frac{n}{2}) + O(n^{2})\] 
Daraus folgt $ a = 2, b = 2, d = 2$. \\\\
Da $2 < 2^{2}$ trifft der erste Fall ein und wir halten fest: $f(n) \in O(n^{2})$.
\end{example}
Über das Mastertheorem sind eine Zahl an Rekursionsgleichungen entscheidbar, jedoch nicht jede. Für die Rekursionsgleichungen, deren Form nicht der für das Mastertheorem benötigten entspricht, werden andere Verfahren nötig.
\subsection{Auflösungsverfahren}
Eine gegebene Rekursionsgleichung lässt sich in eine geschlossene Form überführen, in der keine Abhängigkeit zu vorherigen Funktionswerten mehr besteht. Dafür existieren zwei Ansätze: 
\begin{itemize}
\item \textbf{bottom-up:} Aus berechneten Funktionswerten für kleine n wird eine weitere Berechnungsvorschrift ermittelt. Diese muss anschließend induktiv bewiesen werden.
\item \textbf{top-down:} Die vorherigen Funktionswerte werden solange durch die entsprechende Gleichung ersetzt, bis der definierte Startwert eingesetzt werden kann. Anschließend wird der Term zusammengefasst und es ergibt sich eine geschlossene Rechenvorschrift. 
\end{itemize}
\begin{example}
Gegeben ist die folgende Rekursionsgleichung: $f(1) = 1, f(n) = 2 \cdot f(n-1) + 1$. \\\\
\textbf{bottom-up} \\
\begin{align}
f(1) = 1 \\
f(2) = 2 \cdot 1 + 1 = 3 \\
f(3) = 2 \cdot 3 + 1 = 7 \\
f(4) = 2 \cdot 7 + 1 = 15
\end{align}
Aus den Ergebnissen wird relativ schnell eine Verwandtschaft zu den ersten Potenzen der Zahl 2 deutlich und so lässt sich nun folgende Gleichung aufstellen: 
\[f(n) = 2^{n} - 1\]
Diese Gleichung kann nun für die gegebenen Werte von $n$ überprüft werden und muss anschließend (induktiv) bewiesen werden. \\
\begin{proof}
Zu zeigen: $f(n) = 2 \cdot f(n-1) + 1 \in O(2^{n})$ \\
\begin{align*}
\text{Induktionshypothese: }& \quad f(n) = 2^{n} - 1 \quad \forall n \geq 1 \\
\text{Induktionsanfang: }& \quad f(1) = 2^{1} - 1 = 1 \\
\text{Induktionsvoraussetzung: }& \quad n \to n + 1 \quad f(n+1) = 2^{n+1} -1\\
\text{Induktionsschritt: }& \quad f(n+1) = 2 \cdot f(n) + 1 = 2 \cdot (2^{n} - 1) + 1 = 2^{n+1} - 1
\end{align*}
nach \parencite{UniHH}
\end{proof}
Wir führen Induktionsbeweise für Rekurrenzgleichungen, indem wir zunächst im Induktionsanfang feststellen, dass unsere ermittelte Funktion den Rekursionsstartwert erfüllt. Ist dies nicht der Fall, brechen wir hier ab. Nun induzieren wir, in dem wir $n$ auf $n+1$ abbilden, bilden die Rekursionsgleichung für $n+1$ und setzen nun für $f(n)$ unsere geschlossene Funktion ein. Wir stellen fest, dass unsere Induktionsvoraussetzung erfüllt ist. Damit ist unsere Auflösung der Rekursionsgleichung gezeigt.\\\\
\textbf{top-down} \\
\begin{align}
f(n) =& \quad 2 \cdot f(n-1) + 1 \\
=& \quad 2 \cdot (2 \cdot f(n-2) + 1) + 1 \\
=& \quad 2 \cdot (2 \cdot (... \cdot (2 \cdot 1) + 1) ... ) + 1) + 1 \\
\Rightarrow & \quad 2^{n-1} + 2^{n-2} + ... + 2^{n-(n-1)} + 1 \\
=& \quad \sum_{i = 0}^{n-1} 2^{i} = \frac{2^{(n-1)+1}-1}{2-1} = 2^{n}-1 \in O(2^{n}) 
\end{align}
\end{example}
Je nach Rekursionsgleichung bietet sich mitunter eines der beiden Verfahren mehr an als das andere. Vernachlässigt werden darf beim \textit{bottum-up}-Verfahren nicht, dass ein Induktionsbeweis erforderlich ist! Bei Rekursionsgleichungen mit zweifacher Rekursion (in Beziehung zu den vorherigen zwei Funktionswerten) ist ein \textit{top-down}-Verfahren deutlich komplexer und häufig nicht ratsam.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Veranschaulichen Sie die $O$-, $\Theta$- und $\Omega$-Notation anhand der Funktion $f(n) = 3n^{3} + 7n^{2} + 16$ grafisch. 
\begin{flushright}
Lösung auf Seite \pageref{a1:lsg}
\end{flushright}
\paragraph{Aufgabe 2} Geben Sie zu folgenden Funktionen jeweils die $O$-Notation an.
\begin{enumerate}
\item $f(n) = 6n^{4} + 3 \cdot log_{2}(n)$
\item $f(n) = 6627816n + 13$
\item $f(n) = \frac{72n^{3} + 27n^{2} + 8n + 9}{n!}$
\item $f(n) = 3 \cdot f(n-1) + \frac{f(n-2)}{n^{2}}$
\end{enumerate}
\begin{flushright}
Lösung auf Seite \pageref{a2:lsg}
\end{flushright}
\paragraph{Aufgabe 3}
\begin{enumerate}
\item Zeigen Sie, dass $n^{2} \in O(2n)$.
\item Zeigen Sie, dass $n^{3} \in O(2n)$.
\end{enumerate}
\begin{flushright}
Lösung auf Seite \pageref{a3:lsg} \\
entnommen aus VL
\end{flushright}
\paragraph{Aufgabe 4}
Bestimmen Sie unter Anwendung des Mastertheorems die jeweilige Komplexitätsklasse $O(n)$.
\begin{itemize}
\item $f(n) = 2 \cdot f(\frac{n}{2n+1}) + n^{2}$
\item $g(n) = log(n) \cdot g(\frac{n}{2}) + 3$
\item $h(n) = sin(n) \cdot h(\frac{n}{\frac{n}{2}}) + O(n^{3})$
\end{itemize}
\begin{flushright}
Lösung auf Seite \pageref{a4:lsg} \\
\end{flushright}
\paragraph{Versionshinweis} Weitere Aufgaben zu Rekursionsgleichungen, praktischen Anwendungen auf Pseudo-Code und für $\Theta , \Omega$ folgen in späteren Versionen
\pagebreak
\part{Suchalgorithmen}
Sie studieren Informatik, sind es satt, noch mehr Stunden vor Bildschirmen zu verbringen und suchen sich deshalb ein echtes, physisches Buch aus der Bibliothek Ihrer Hochschule heraus. Sie fragten nach \glqq\textsc{Wirth}: Algorithmen und Datenstrukturen{\grqq} von 1983, weil man es Ihnen in Ihrer Vorlesung empfahl. Die Bibliothekarin deutet auf ein Regal und lässt sie damit allein. \\\\
Wie finden Sie das gesuchte Buch in der Regalreihe mit etwa 300 Büchern?
\section{Lineare Suche}
Sie beginnen ganz links. Nun lesen Sie den Titel des ersten Buches, er passt nicht. Sie lesen den Titel des zweiten Buches, er passt ebenfalls nicht. Sie fahren fort, iterieren über das gesamte Bücherregal und brechen ab, falls Sie es gefunden haben oder am Ende angelangt sind. Dieses Verfahren ist die \textbf{Lineare Suche}.
\paragraph{Idee}
Durchlaufe sukzessive das Feld A, bis das gesuchte Element gefunden ist bzw. das Ende des Feldes erreicht ist.
\paragraph{Implementierung}
\begin{lstlisting}
LinearSearch(A[], E) {
	i = 0;
	While i < A[].length {
		If (A[i] == E) {
			return i;			
		}
		i = i + 1;	
	}
	Return ElementNotFound
}
\end{lstlisting}
\paragraph{Komplexität}
Ganz offensichtlich ist der Aufwand dieses Suchverfahrens durch die Feldlänge (also die Anzahl der Bücher in unserem Regal) bestimmt. \\
\textit{günstigster Fall} \quad Das gewünschte Element steht an Indexposition 0 (also dem ersten Element bzw. es ist das erste Buch im Regal) $\to$ ein Vergleich ist erforderlich
\\
\textit{durchschnittlicher Fall} \quad Das gewünschte Element steht an der mittleren Indexposition $\frac{n}{2}$ bei $n$ Feldelementen. $\to \frac{n}{2}$ Vergleiche sind erforderlich
\\
\textit{ungünstigster Fall} \quad Das gewünschte Element ist nicht im Feld vorhanden. $\to n$ Vergleiche
\\\\
Insgesamt ergibt sich -- da wir bei der Angabe der Komplexität durch die $O$-Notation jeweils den ungünstigsten Fall betrachten -- für die Lineare Suche eine Komplexität von $O(n)$. 
\section{Binäre Suche}
Sie befinden sich in der perfekten Bibliothek: Alle Bücher sind aufsteigend nach ihren Autor*innen sortiert! Dieses Wissen können Sie sich zunutze machen, in dem Sie ihr Wunschbuch mit der \textbf{binären Suche} finden: Sie teilen die Bücherreihe in zwei Hälften und betrachten nun das mittlere Buch: Muss Ihr Buch links oder rechts von dem mittleren Buch stehen (Vergleich anhand des Namens der Autoren)? Oder ist es sogar ihr Buch? Nein, das ist es nicht und der Autor Ihres Buches hat einen im Alphabet weiter hinten stehenden Anfangsbuchstaben, also betrachten Sie die Hälfte rechts des mittleren Buches. Diese Hälfte teilen Sie nun erneut, betrachten die neue Mitte und fahren fort, bis ihre Hälften aus einem Buch bestehen.
\paragraph{Idee} Vergleiche zu suchendes Element $E$ mit der Mitte des Suchbereiches $A[m]$. Falls gefunden: Abbruch. Falls $E < A[m]$: Suche weiter in linker Hälfte. Falls $E > A[m]$: Suche weiter in rechter Hälfte. Das Weitersuchen erfolgt durch einen rekursiven Aufruf des Algorithmus mit der entsprechenden Hälfte des Feldes.
\paragraph{Implementierung}
\begin{lstlisting}
BinarySearch(A[], E, indexLeft, indexRight) {
	if (indexLeft > indexRight) {
		return ElementNotFound	
	}
	else {
		mid = (indexLeft+indexRight) / 2;
		if (E == A[mid]) {
			return mid;		
		}
		else if (E < A[mid] {
			return BinarySearch(A[], E, indexLeft, mid-1);	
		}
		else {
			return BinarySearch(A[], E, mid+1, indexRight);	
		}
	}
}
\end{lstlisting} 
Die Parameter \texttt{indexLeft} und \texttt{indexRight} schränken jeweils das zu betrachtende Feld ein. Zu Beginn ist \texttt{indexLeft} mit $0$ und \texttt{indexRight} mit \texttt{A[].length()} initialisiert. 
\paragraph{Komplexität}
Hier handelt es sich um ein rekursives Verfahren, folglich werden wir eine Rekursionsgleichung erhalten und diese mit den bekannten Verfahren auswerten. Doch zunächst betrachten wir wieder Fälle: \\
\textit{günstigster Fall}: \quad E ist das mittlere Element $\to$ 1 Vergleich\\
\textit{ungünstigster Fall}: \quad E ist nicht im Feld \\
Die Rekursionsgleichung mit den Parametern $a, b \text{und} d$ ergibt sich nun wie folgt:
\begin{itemize}
\item Es gibt einen rekursiven Aufruf, d.h. $a = 1$.
\item Pro Rekursionsaufruf halbieren wir das Feld, daraus folgt $b = 2$.
\item Die Vorbereitung der Rekursion verläuft konstant und ist minimal, also $d = 1$.
\end{itemize}
Die Rekursionsgleichung lautet nun: 
\[f(n) = 1 \cdot f(\frac{n}{2}) + 1\]
Für die Anwendung des Mastertheorems formen wir um:
\[\iff f(n) = 1 \cdot f(\frac{n}{2}) + O(n^{0})\]
Nun ergibt sich: 
\[a = b^{d} \iff 1 = 2^{0} \Rightarrow f(n) \in O(n^{0} \cdot log_{2}(n)) = O(log(n))\]
\section{Textsuche}
Spätestens jetzt sollten Sie das Buch gefunden haben, wenn Sie die binäre Suche verwendet haben, vermutlich schneller als mit der linearen Suche. Jetzt haben Sie einen Arbeitsplatz gefunden, setzen sich und schlagen das Buch auf: Für ein Lesen in Ruhe haben Sie keine Zeit, daher suchen Sie sich die interessanten Teile heraus. Sie suchen, wo \textsc{Niklaus Wirth} Ihnen die $O$-Notation erklären wird. \\\\
Formal suchen Sie folglich alle Vorkommen eines Musters (in Form einer Zeichenkette) in einem Text. Dafür betrachten wir zwei Verfahren.
\subsection{Einfache Textsuche}
Analog zur linearen Suche beginnen Sie einfach, Ihren gesuchten Begriff unter den Text zu legen. Stimmt der erste Buchstabe überein, vergleichen Sie den zweiten. Stellen Sie eine Differenz fest, verschieben Sie Ihr Suchwort um eine Position nach rechts. Derart fahren Sie fort, bis Sie am Ende des Textes angelangt sind.
\paragraph{Idee}
Beginnend beim ersten Zeichen des Textes legt man das Muster der Reihe nach an jede Stelle des Textes an und vergleicht zeichenweise von links nach rechts, ob eine Übereinstimmung vorliegt
\paragraph{Implementierung}
\begin{lstlisting}
int BruteForceSearch(char[] text, char[] pattern) {
	int n = text.length;
	int m = pattern.length;
	int i = 0;
	int j = 0;
	for (int k = 0; i <= n-m; i++) {
		j = 0;
		while (j < m && Text[i+j] == pattern[j]) {		
			j++;		
		}
		if (j == m) {
			return i;		
		}
	}
	return ElementNotFound;
}
\end{lstlisting}
\paragraph{Komplexität}
Offenbar hängt der Aufwand dieses Verfahrens ganz wesentlich davon ab, wie lang das Muster und der Text in Gänze sind. Weniger relevant hingegen ist der Aufbau des Textes, da alle Vorkommen des Musters gesucht werden. Insofern ist es nicht entscheidend, an welcher Stelle das Muster das erste Mal auftritt. \\\\
\textit{günstigster Fall} \quad Im günstigsten Fall werden m Vergleiche benötigt, wenn nämlich die Länge des Textes kleiner als das doppelte der Länge des Musters $m$ ist und das Muster direkt am Beginn des Textes steht. $\to m$ Vergleiche 
\\\\ \textit{ungünstigster Fall}\quad Im ungünstigsten Fall werden $(n-m+1) \cdot m$ Vergleiche benötigt, wobei dieser Fall allgemeingültig ist -- es werden alle Vorkommen des Musters im Text gesucht und nicht lediglich das erste. \\\\
Die Laufzeit des Algorithmus liegt nun in $O(n \cdot m)$.
\subsection{Knuth-Morris-Pratt-Algorithmus (KMP)}
Ihnen ist das zu mühsam geworden, Zeichen für Zeichen des Textes und des Musters zu vergleichen. Sie sehnen sich nach einem intelligenteren, einfacheren Weg. Und, wahrlich den gibt es: Der \textbf{Knuth-Morris-Pratt}-Suchalogorithmus nach \textsc{Donald Ervin Knuth}, \textsc{James Hiram Morris} und \textsc{Vaughan Ronald Pratt}. \\\\
Die Grundlage für diesen Algorithmus bildet die vorangegangene einfache Textsuche. Diese optimieren wir nun, in dem wir beim ersten sich unterscheidenden Zeichen von Text und Muster das Muster nicht um ein einziges Zeichen nach rechts verschieben, sondern um eine bestimmte Anzahl an Zeichen, die sich aus unserem Muster ergibt und als \textbf{Rand} bezeichnet wird.
\begin{definition}
Der \textbf{Rand} eines Musters $M$ mit der Länge $m$ ist eine tabellarische Auflistung aller Teilzeichenketten der Längen $0$ bis $m$ des Musters, für die jeweils die Anzahl der Zeichen eines Prefixes, das zugleich Postfix ist, bestimmt wird.
\end{definition}
\begin{example}
Betrachten wir das Wort \texttt{abbababbab}. \\
Wir beginnen eine tabellarische Auflistung: \\\\
\begin{tabular}{|l|c|c|l|}
\hline
Teilzeichenkette & Länge & Rand & Bemerkung \\
\hline
& 0 & -1 & per Definition, für Teilschritt 2 (Suche) erforderlich \\
\texttt{a} & 1 & 0 & per Definition \\ 
\texttt{ab} & 2 & 0 & $a \neq b$ \\
\texttt{abb} & 3 & 0 & \\
\texttt{abba} & 4 & 0 & $ab \neq ba$, Palindrome haben den Rand 0 \\
\texttt{abbab} & 5 & 2 & gleich: $ab$, Länge: 2 \\
\texttt{abbaba} & 6 & 1 & gleich: a, Länge: 1 \\
\texttt{abbabab} & 7 & 2 & gleich: ab, Länge: 2 \\
\texttt{abbababb} & 8 & 3 & gleich: abb, Länge: 3 \\
\texttt{abbababba} & 9 & 4 & gleich: abba, Länge: 4 \\
\texttt{abbababbab} & 10 & 5 & gleich: abbab, Länge: 5 \\
\hline 
\end{tabular} \\\\
Im folgenden Suchalgorithmus wird jeweils der Rand verwendet, um Verschiebungspositionen zu bestimmen. Daher ist die gesamte Tabelle erforderlich.
\end{example} \pagebreak
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
Rand[0] = -1; 		/* Definition */
Rand[1] = 0;		/* Definition */

for (j = 2; j <= m; j++) {
	while ((i >= 0) and (M[i] != M[j-1])) {
		i = Rand[i];
		i++;
	}
	Rand[j] = i;
}
\end{lstlisting}
Folglich iterieren wir nun über alle möglichen Teilzeichenkettenlängen (\texttt{j}) und vergleichen jeweils mit dem vorherigen Rand (durch \texttt{i}). Wenn unser Rand maximal ist, verlassen wir die \texttt{while}-Schleife und speichern den ermittelten Rand. \\\\
Eine Veranschaulichung der algorithmischen Rand-Bestimmung finden Sie in der Wikipedia. \parencite{KMPRand}
\paragraph{Der Suchalgorithmus}
Ähnlich wie bei der einfachen Textsuche beginnen Sie, indem Sie das zu suchende Muster unter den Suchraum schreiben und vergleichen Zeichen für Zeichen. Entscheidend ist nun die erste Abweichung: Sie bestimmen den \texttt{Rand[]} der Indexposition der Abweichung im Muster und verschieben ihr Muster nun mit unterer Formel. So nutzen Sie aus, dass bis zur ersten Abweichung Gleichheit bestand -- falls bei vier gleichen Zeichen ein Rand von zwei vorliegt, können Sie ihr Muster so verschieben, dass der Beginn nun dort liegt, wo zuvor das mit dem neuen Anfang identische Enge lag. Nun fahren Sie mit den neu gewonnen Index-Positionen fort, die sich aus dem Verschiebung ergeben haben. \\\\
Die Formel für Verschiebungen ist: \\
\texttt{Neuer Beginn des Musters (neue Suchtextposition) = Suchtextposition (aktueller Beginn des Musters) + (Anzahl übereinstimmender Zeichen -- Randlänge)}
\begin{example}
Sie suchen das Muster \texttt{ababaab} im Text \texttt{baabbaababaabbbabba}. \\
\textbf{Schritt 1}: Bestimmung des Randes \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|}
 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
\hline
-1 & 0 & 0 & 1 & 2 & 1 & 1 & 2 \\
\end{tabular} \\\\
\textbf{Schritt 2}: Suche \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0 & 1 & 2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
a&b&a&b&a&a&b& & & & & & & & & & & & \\
\hline
\end{tabular} \\
Erste Abweichung an Index 0 $\to$ Rand[0] = -1, nach Formel oben: $0+(0-(-1)) = 1$. Also verschieben wir das Muster an Indexposition 1. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 &a&b&a&b&a&a&b& & & & & & & & & & &  \\
\hline
\end{tabular} \\
Abweichung an Index 2, Rand[2] = 0, mit Formel: $1+(1-0) = 2$. Wir verschieben also so, dass das erste Zeichen des Musters an Index 2 steht.  \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & &a&b&a&b&a&a&b& & & & & & & & & &   \\
\hline
\end{tabular}\\
Abweichung an Index 4, Rand[2] = 0, mit Formel: $2+(2-0) = 4$. Neue Indexposition des Musters: 4. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & &a&b&a&b&a&a&b& & & & & & & &    \\
\hline
\end{tabular}\\
Abweichung an Index 4, Rand[0] = -1, mit Formel: $4+(0-(-1)) = 5$. Neue Indexposition: 5. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & & &a&b&a&b&a&a&b& & & & & & &    \\
\hline
\end{tabular}\\
Abweichung an Index 6, Rand[1] = 0, mit Formel: $5+(1-0) = 6$. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & & & &a&b&a&b&a&a&b& & & & & &    \\
\hline
\end{tabular}\\
Nun tritt keine Abweichung ein, das erste Vorkommen des Musters im Suchtext ist gefunden. Da wir nun an allen Vorkommen interessiert sind, führen wir unsere Suche fort. Dafür bestimmen wir zunächst den Rand[7] = 2. Mit der Formel folgt: $6+(7-2) = 11$. An Position 11 können wir nun unsere Suche fortsetzen.\\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18 \\
\hline
b&a&a&b&b&a&a&b&a&b&a&a&b&b&b&a&b&b&a \\
\hline
 & & & & & & & & & & &a&b&a&b&a&a&b&    \\
\hline
\end{tabular}\\
$11+(2-0) = 13$. Damit verlassen wir den Suchtext, der Algorithmus terminiert, da kein weiteres Vorkommen gefunden wurde.
\end{example}
Die Stärken des KMP-Algorithmus liegen folglich in der Berücksichtigung von Rändern an Anfang und Ende des Musters, die größere Sprünge als eins (im Vergleich zur einfachen Textsuche) ermöglichen. Liegt nun ein Muster ohne jegliche oder mit wenigen Rändern vor, ist der Laufzeitgewinn marginal.
\paragraph{Implementierung}
als Pseudo-Code
\begin{lstlisting}
KnuthMorrisPrattAlgorithm(Pattern, SearchSpace, Borders) {
	int i = 0;	/* Current position in search space */
	int j = 0;	/* Current position in pattern */

	while (i < SearchSpace.length) {
		// Move pattern until first character of 
		// SearchSpace and Pattern is equal
		while (j >= 0 && SearchSpace[i] != Pattern[j]) {
			j = Borders[j]		
		} 
		
		// Now compare the next character
		i = i + 1;
		j = j + 1;

		// End of pattern: return result		
		if (j == (Pattern.length-1)) {
			print(i - (Pattern.length-1));		
		}
	}
}
\end{lstlisting}
Wie ersichtlich ist, beginnen wir damit, unser Muster unter Anwendung der Ränder an die erste Position zu verschieben, an der das erste Zeichen übereinstimmt. Nun inkrementieren wir beide Zählervariablen und betrachten folgende Zeichen, in dem wir im nächsten Schleifendurchlauf bei Abweichungen weiterverschieben, bis die Länge des Musters erreicht wurde. Dann geben wir diese Indexposition aus und fahren fort. 
\paragraph{Komplexität} 
Da dieser Algorithmus aus zwei Teilproblemen besteht, zerfällt auch die Komplexität: \\
Das Bestimmen der Ränder erfolgt linear für das gesamte Muster, daraus folgt in \textsc{Landau}-Notation: $\text{Rand}(x) \in O(n)$ bei $n$ als Länge des Musters.
Der Suchalgorithmus selbst verläuft ebenfalls linear -- insgesamt erfolgen maximal so viele Durchläufe wie der Text Zeichen enthält. Daraus folgt: $\text{KMPSearch}(x, y, R[]) \in O(m)$ \\
Da beide Teilprobleme getrennt voneinander und aufeinander aufbauend durchlaufen werden, ergibt sich eine Komplexität des gesamten Algorithmus von $O(n + m)$. \parencite{KMPRand}
\subsection{Boyer-Moore-Algorithmus}
Ähnlich dem Ansatz von \textsc{Knuth}, \textsc{Morris} und \textsc{Pratt} haben auch \textsc{Robert S. Boyer} und \textsc{J Strother Moore} einen vergleichbaren Algorithmus zur effizienteren Suche von Zeichenketten in Texten entwickelt. Im Gegensatz zum KMP-Algorithmus werden dort keine Ränder bestimmt, sondern erst im Falle einer Abweichung greifen zwei Heuristiken: Die \textbf{Bad-character}- und \textbf{Good-suffix}-Heuristik, mithilfe derer ebenfalls Sprünge ermittelt werden. Dieses Verfahren hat eine Komplexität von $O(n \cdot m)$ und ist damit im Allgemeinen (dem ungünstigsten Fall) weniger effizient als der KMP. Jedoch verläuft dieses Algorithmus nicht linear, sodass in einem günstigen Fall eine geringere Laufzeit erreicht werden kann. \\\\

Dieses Verfahren wird hier nicht näher dargestellt, da es nicht Teil der Vorlesung war. Weitere Informationen finden Sie bei \parencite[][S. 476 -- 478]{Grundkurs}.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Führen Sie eine binäre Suche nach dem Element \texttt{7} in \texttt{[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 17, 18, 19, 22]} durch. Notieren Sie jeweils Ihre Zwischenergebnisse. 
\begin{flushright}
Lösung auf Seite \pageref{a2.1:lsg}
\end{flushright}
\paragraph{Aufgabe 2} Bestimmen Sie die Ränder der folgenden Muster:
\begin{itemize}
\item \texttt{OTTOSMOPSKOTZT}
\item \texttt{ANANASBANANA}
\item \texttt{BONOBO}
\end{itemize}
Geben Sie jeweils an, wie sehr sich der Einsatz des KMP-Algorithmus bei diesen Mustern lohnt.
\begin{flushright}
Lösung auf Seite \pageref{a2.2:lsg}
\end{flushright}
\paragraph{Aufgabe 3} Wenden Sie den KMP-Algorithmus mit dem Muster \texttt{BONOBO} auf den Suchtext \texttt{BEINAHEBOTNOCHDASBONOBONOBOHAUSPLATZ} an. Stellen Sie Ihre Zwischenergebnisse dar.
\begin{flushright}
Lösung auf Seite \pageref{a2.3:lsg}
\end{flushright}
\paragraph{Aufgabe 4} Erläutern Sie den Algorithmus der binären Suche in Ihren eigenen Worten.
\begin{flushright}
Lösung auf Seite \pageref{a2.4:lsg}
\end{flushright}
\pagebreak
\part{Sortieralgorithmen}
Nach langen, qualvollen Stunden vor dem Bücherregal hassen Sie ihr Leben, ihr Studium und Sie beschließen, es wegzuwerfen -- Sie fangen in der Bibliothek als Fachkraft für Lagerlogistik an und sind nun für die korrekte Anordnung der Bücher im Regal verantwortlich. Auch hier zeigen Sie ihre geistige Brillianz, indem Sie das Problem des Sortierens auf abstrakter Ebene betrachten.
\section{Allgemeines und Eigenschaften}
\begin{definition}
Ein Sortieralgorithmus verarbeitet eine Folge von Elementen $e_{1}, ..., e_{n}$ so, dass am Ende eine Ausgabe $e_{1}, ..., e_{n}$ mit $e_{1} \leq e_{2} \leq ... \leq e_{n}$ erzeugt wird.
\end{definition} 
Als zugrunde liegende Datenstrukturen kommen Arrays, verkettete Listen oder weitere, spezielle Datenstrukturen mit zu Mengen/Relationen ähnlichem Aufbau infrage. \\\\
Da Sie bei Sortiervorgängen häufig Daten im \textit{{key: value}}-Format betrachten, unterscheiden wir zwischen \textbf{Schlüsseldaten}(also den keys) und \textbf{Satellitendaten}(den zugehörigen Werten). So können wie beispielsweise Wetterdaten betrachten: [{08.05.2021: 31.5}, {09.05.2021, 30.2}, {07.05.2021, 16.3}] -- Hier sind die Tage jeweils die Schlüsseldaten und die Temperaturangaben (die Gleitkommazahlen) jeweils Satellitendaten.
\begin{definition} Ein Algorithmus sortiert Daten \textbf{stabil}, falls die Reihenfolge bereits sortierter Elemente mit gleichem Schlüssel nicht geändert wird und so insbesondere nach einem weiteren Kriterium sortierte Daten nach Ausführung des Algorithmus in diesem Kriterium ihre Sortierung beibehalten. Als \textbf{instabil} bezeichnen wir einen Sortieralgorithmus, falls das Gegenteil der Fall ist.
\end{definition}
Ähnlich den Suchalgorithmen unterscheiden sich auch Sortieralgorithmen in weiteren Aspekten: 
\begin{itemize}
\item Laufzeiteffizienz ($\to$ \textsc{Landau}-Notation)
\item Speicherplatzeffizienz
\item Ablaufschema: Rekursiv oder Iterativ
\item Stabilität der Daten
\end{itemize}
Für die Speicherplatzeffizienz ergibt sich folgende Unterscheidung:
\begin{definition}
Falls ein Sortieralgorithmus parallel zur Laufzeit keinen oder einen konstanten zusätzlichen Speicherbedarf benötigt, so wird er \textbf{in-place} ausgeführt. Ein Algorithmus, der hingegen zusätzlichen Speicherbedarf in Abhängigkeit zu den Daten benötigt, wird als \textbf{out-place} bezeichnet.
\end{definition}
\section{Insertion Sort}
Der erste Sortieralgorithmus, den wir betrachten, heißt \textbf{Insertion Sort}. Wie der Name schon andeutet, geht es darum, Elemente einzufügen -- in einen bestehenden, bereits sortierten Teil. \\\\
Sie beginnen links in Ihrer Bücherreihe, starten mit dem zweiten Buch. Sie prüfen nun, ob es im Vergleich zum ersten richtig steht, andernfalls tauschen sie es. Nun betrachten Sie das dritte Buch und fügen es vor/zwischen/nach Buch 1 und 2 ein, je nach richtiger Position. So fahren Sie fort: Sie fügen das jeweils aktuelle Buch in die Reihe der bereits sortierten ein.
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
InsertionSort(Array) {
	int i = 0;
	while (i < (Array.length-1)) {
		x = Array[i];
		int y = 0;
		while (x < Array[i-y]) {
			y++;		
		} 
		Array[i-y] = x;
		i++;
	}
}
\end{lstlisting}
\begin{example} Zu sortieren Sie das Array \texttt{[44, 55, 12, 42, 94, 18, 06, 67]}. \\\\
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
Ausgangswerte & 44 & \uline{55} & 12 & 42 & 94 & 18 & 06 & 67 \\
nach i = 1    & 44 & 55 & \uline{12} & 42 & 94 & 18 & 06 & 67 \\
nach i = 2    & 12 & 44 & 55 & \uline{42} & 94 & 18 & 06 & 67 \\
nach i = 3    & 12 & 42 & 44 & 55 & \uline{94} & 18 & 06 & 67 \\
nach i = 4    & 12 & 42 & 44 & 55 & 94 & \uline{18} & 06 & 67 \\
nach i = 5    & 12 & 18 & 42 & 44 & 55 & 94 & \uline{06} & 67 \\
nach i = 6    & 06 & 12 & 18 & 42 & 44 & 55 & 94 & \uline{67} \\
nach i = 7    & 06 & 12 & 18 & 42 & 44 & 55 & 67 & 94 \\
\hline
\end{tabular} \\
Quelle: \parencite[][S. 493 ff.]{Grundkurs}
\end{example}
\paragraph{Komplexität}
Im günstigsten Fall operiert dieses Verfahren auf einem bereits sortierten Feld. Dann sind -- um dies festzustellen -- $n-1$ Vergleiche erforderlich. Im schlechtesten Fall ist das Feld in genau falscher Richtung sortiert: Dann sind $\frac{n^{2}}{2}$ Vergleiche erforderlich, da die zweite Schleife auch maximal oft durchlaufen wird. \\\\
Insgesamt ergibt sich in \textsc{Landau}-Notation: $\in O(n^{2})$
\paragraph{Eigenschaften}
\begin{itemize}
\item Das Verfahren ist iterativ (erkennbar an den beiden Schleifen)
\item Es ist stabil, da Vertauschungen nur im notwendigen Fall auftreten
\item Das Verfahren operiert in-place, da kein bzw. nur konstanter zusätzlicher Speicher benötigt wird
\end{itemize}

Optimieren ließe sich das Verfahren durch den Einsatz von binärer Suche für die Einfügeposition -- dann reduzieren sich die Vergleiche, nicht jedoch die Verschiebungen.
\section{Bubblesort}
Mit \textbf{Bubble Sort} lernen wir einen weiteren Algorithmus zum Sortieren kennen: Hier vergleichen wir zunächst benachbarte Elemente und tauschen diese bei Bedarf. So durchlaufen wir das gesamte Array, bis keine Vertauschungen mehr notwendig sind und alle Elemente in der korrekten Reihenfolge angeordnet sind.
\paragraph{Idee} Durchlaufe Feld mehrmals und tausche benachbarte Elemente bei Bedarf. Terminiere, wenn in einem Durchlauf keine Vertauschung mehr erfolgt ist.
\begin{example}
Zu sortieren ist das Feld \texttt{[6, 8, 2, 9, 1, 5, 4, 3]}. \\
6 $<$ 8, aber 8 $>$ 2: \texttt{[6, 2, 8, 9, 1, 5, 4, 3]}. \\
8 $<$ 9, aber 9 $>$ 1: \texttt{[6, 2, 8, 1, 9, 5, 4, 3]} \\
9 $>$ 5: \texttt{[6, 2, 8, 1, 5, 9, 4, 3]} \\
9 $>$ 4: \texttt{[6, 2, 8, 1, 5, 4, 9, 3]} \\
9 $>$ 3: \texttt{[6, 2, 8, 1, 5, 4, 3, 9]} \\\\
Nächste Iteration: \\
6 $>$ 2: \texttt{[2, 6, 8, 1, 5, 4, 3, 9]} \\
6 $<$ 8, aber 8 $>$ 1: \texttt{[2, 6, 1, 8, 5, 4, 3, 9]} \\
8 $>$ 5: \texttt{[2, 6, 1, 5, 8, 4, 3, 9]} \\
8 $>$ 4: \texttt{[2, 6, 1, 5, 4, 8, 3, 9]} \\
8 $>$ 3: \texttt{[2, 6, 1, 5, 4, 3, 8, 9]} \\
8 $<$ 9. \\\\
Nächste Iteration:\\
2 $<$ 6, aber 6 $>$ 1: \texttt{[2, 1, 6, 5, 4, 3, 8, 9]} \\
6 $>$ 5: \texttt{[2, 1, 5, 6, 4, 3, 8, 9]} \\
6 $>$ 4: \texttt{[2, 1, 5, 4, 6, 3, 8, 9]} \\
6 $>$ 3: \texttt{[2, 1, 5, 4, 3, 6, 8, 9]} \\
6 $<$ 8 und 8 $<$ 9. \\\\
Nächste Iteration:\\
2 $>$ 1: \texttt{[1, 2, 5, 4, 3, 6, 8, 9]} \\
2 $<$ 5, aber 5 $>$ 4: \texttt{[1, 2, 4, 5, 3, 6, 8, 9]} \\
5 $>$ 3: \texttt{[1, 2, 4, 3, 5, 6, 8, 9]} \\
5 $<$ 6, 6 $<$ 8 und 8 $<$ 9. \\\\
Nächste Iteration: \\
1 $<$ 2, 2 $<$ 4, aber 4 $>$ 3: \texttt{[1, 2, 3, 4, 5, 6, 8, 9]} \\
4 $<$ 5, 5 $<$ 6, 6 $<$ 8 und 8 $<$ 9. Terminiert.
\end{example}
Aus dem Beispiel können wir ableiten, dass in jeder Iteration $n-1$ Vergleiche erfolgen. Dies ist ein Indiz für die Komplexität, die wir später betrachten möchten.
\paragraph{Implementation} als Pseudo-Code
\begin{lstlisting}
BubbleSort(Array) {
	int i = 0;
	int j = 0;
	bool SwapFlag = True;
	while (SwapFlag == true) {
		SwapFlag = false;
		while (i < (Array.length-1) {	
			if (Array[i] > Array[i+1]) {
				swap(Array[i], Array[i+1]);
				SwapFlag = true;			
			}
		}	
	}
}
\end{lstlisting}
Die \texttt{SwapFlag} ist Terminationskriterium: Falls in einer Iteration kein Tausch mehr durchgeführt wurde, ist das Array vollständig sortiert.
\paragraph{Komplexität}
Im günstigsten Fall ist das Array bereits vollständig sortiert, dann werden $n-1$ Vergleiche notwendig. Im ungünstigsten Fall ist das Aray absteigend sortiert, nun sind $\frac{n^{2}}{2}$ Vergleiche erforderlich. \\\\
Somit ergibt sich erneut eine Komplexität von $O(n^{2})$. \\\\
Im Gegensatz zu ursprünglichen Implementierungen haben wir hier bereits eine Optimierung vorgenommen: Die SwapFlag bewirkt, dass im günstigsten Fall $O(n)$ die passende Komplexitätsklasse ist.
\paragraph{Eigenschaften}
\begin{itemize}
\item Auch Bubble Sort arbeitet offensichtlich iterativ.
\item Bubble Sort ist ebenfalls stabil.
\item Der Speicheraufwand ist in-place.
\end{itemize}
Quelle und weiterführende Literatur: \parencite[][S.86 ff.]{Wirth} \parencite[][S.22]{Taschenbuch}
\section{Mergesort}
Die nächste Idee, wie Sie Ihr Bücherregal sortieren können: Mergesort! Sie teilen das Regal in zwei Hälften, sortieren jede Hälfte, in dem Sie diese wiederum halbieren, bis die Hälften aus einem Buch bestehen. Sortieren Sie diesen Teil, indem Sie die Bücher richtig anordnen und die Hälften so wieder von klein nach groß nach einer Art Reißverschlussprinzip verbinden, bis Sie die Gesamtheit der Elemente in sortierter Reihenfolge erhalten. 
\begin{example} Zu sortieren: \texttt{[1, 7, 6, 2, 9, 4, 3, 8, 10, 5]}.\\
Halbieren: \\\texttt{[1, 7, 6, 2, 9] | [4, 3, 8, 10, 5]} \\
\texttt{[1, 7, 6][2, 9] | [4, 3, 8][10, 5]} \\
\texttt{[1, 7][6][2][9] | [4, 3] [8][10][5]} \\
\texttt{[1][7][6][2][9] | [4][3][8][10][5]}\\\\
Sortieren (nach rekursiven Aufrufen): \\
\texttt{[1][2][6][7][9] | [3][4][5][8][10]} \\\\
Merge: \\
\texttt{[1][2][3][4][5][6][7][8][9][10]}
\end{example}
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
MergeSort(Array) {
	if (Array.length <= 1) {
		return Array  ;
  	}
  	else {
		leftHalf = MergeSort(Array[0:(Array.length-2)]);
		rightHalf = MergeSort(Array[(Array.length-a):-1]);
		return merge(leftHalf, rightHalf);
	}
}
merge(A[], B[], al, ar, bl, br, C[]) {
	int i = al;
	int j = bl;
	for (int k = 0; k <= ar-al+br-bl+1; k++) {
		if (i > ar) {
			C[k] = B[j+1];
			continue;		
		}
		if (j > br) {
			C[k] = A[i+1];
			continue;		
		}
		C[k] = (A[i] < B[j] ? A[i++] : B[j++];	
	}	
}
\end{lstlisting} Quelle: \parencite[][S. 28]{Taschenbuch}
\paragraph{Komplexität}
Für Mergesort ergibt sich -- da es sich um einen rekursiven Algorithmus handelt -- eine Rekursionsgleichung:
\[T(n) = T(\frac{n}{2}) + T(\frac{n}{2}) + n = 2T(\frac{n}{2}) + n\]
Jeder Rekursionsteil beschreibt die Komplexität der einzelnen Hälften und $n$ das anschließende Zusammenfügen. Somit ergibt sich unter Anwendung des Mastertheorems eine Komplexität von 
\[T(n) \in O(n \cdot log(n))\]
\paragraph{Eigenschaften}
\begin{itemize}
\item rekursiv
\item stabil
\item Je nach Implementierung in-place oder out-place
\end{itemize}
\section{Quicksort}
Willkommen in der Königksklasse: Quicksort trägt schon einen bezeichnenden, etwas anmaßenden Titel. Wir betrachten nun, ob der Algorithmus dem gerecht wird.
\paragraph{Idee}
Wir sortieren eine Menge an Elementen, in dem wir das erste Element $p$ (das wir als \textbf{Pivot} bezeichnen) herausgreifen und nun unser Feld/Array so verschieben, dass alle kleineren Elemente ${...} < p$ links von $p$ stehen und alle größeren rechts davon - wohlgemerkt noch nicht sortiert. Nun betrachten wir die neu gebildeten Abschnitte $(a_{0}, ..., a_{p-1})$ und $a_{p+1}, ..., a_{n})$ und verfahren auf diesen Teilstapeln oder Teilmengen rekursiv. Folglich betrachten wir erneut die jeweils ersten Elemente und verfahren ebenso. Da wir die Operationen in-place ausführen, ist kein merge-Schritt notwendig (im Gegensatz zu MergeSort).
\begin{example} Wir betrachten ein Feld \texttt{[8, 17, 28, 15, 11, 1, 3, 20, 25, 6, 12, 5]}. \\
Das erste Element ist folglich 8, also erhalten wir: \\
\texttt{[1, 3, 6, 5, 8, 17, 28, 15, 11, 20, 25, 12]}.\\
Wir betrachten nun die beiden Teilmengen \texttt{[1, 3, 6, 5]} und \texttt{[17, 28, 15, 11, 20, 25, 12]}. \\
Da 1 offensichtlich kleinstes Element der ersten Teilmenge ist, 3 anschließend ebenfalls beim nächsten Aufruf, erhalten wir schnell \texttt{[1, 3, 5, 6]}. \\\\
In der zweiten Menge erhalten wir \texttt{[15, 11, 12]} und \texttt{[28, 20, 25]}, die wir schnell zu \texttt{[11, 12, 15]} und \texttt{[20, 25, 28]}, zusammen \texttt{[11, 12, 15, 17, 20, 25, 28]}. \\\\
Abschließend erhalten wir das sortierte Feld \texttt{[1, 3, 5, 6, 8, 11, 12, 15, 17, 20, 25, 28]}.
\end{example}
\paragraph{Implementierung} als Pseudo-Code
\begin{lstlisting}
QuickSort(Array, lo, hi) {
	if (lo < hi) {
		pivot = Array[lo];
		i = lo;
		j = hi+1;	
	}
	while (true) {
		while (Array[++i] < pivot && i < hi) {
					
		}
		while (Array[--j] > pivot && j > lo) {
		
		}
		if (i < j) {
			swap(Array, i, j);		
		}	
		else {
			break;		
		}
	}
	swap(Array, j, lo);
	
	quickSort(Array, lo, j-1);
	quickSort(Array, j+1, hi);
}
swap(Array, i, j) {
	t = Array[i];
	Array[i] = Array[j];
	Array[j] = t;
}
\end{lstlisting} abgewandelt nach \parencite[][S. 29]{Taschenbuch}
\paragraph{Komplexität}
Offensichtlich handelt es sich bei Quicksort ebenfalls um einen rekursiven Sortieralgorithmus. Im günstigsten Fall ergibt sich eine Komplexität von $O(n \cdot log(n))$, da dann das Pivot-Element genau der Median ist und so beide Hälften glatt rekursiv abgearbeitet werden können. Im schlechtesten Fall beträgt die Komplexität $O(n^{2})$, da dann das Pivot-Element jeweils das kleinste oder größte ist. \\\\
Wir geben die Komplexität von Quicksort im Durchschnittsfall mit $O(n \cdot log(n))$ an, behalten allerdings im Kopf, dass in einem ungünstigen Fall eine deutlich schlechtere Laufzeit in Kauf genommen werden muss.
\paragraph{Eigenschaften}
\begin{itemize}
\item rekursiv
\item nicht stabil
\item in-place, da kein Hilfsfeld benötigt wird
\item optimierbar beispielsweise durch Einfügen einer Schranke, bis zu der Insertion Sort genutzt wird (z.B. bei einer geringen zweistelligen Anzahl von Elementen) oder durch 3-Way-Partitioning, bei dem neben $<, >$ auch der Fall $=$ betrachtet wird. Dies ist logischerweise nur dann sinnvoll, falls es mehrere Elemente mit gleichen Schlüsseln gibt und diese bevorzugt als Pivot-Element genutzt werden.
\end{itemize}
\section{Weitere Sortieralgorithmen}
Auf weitere Sortieralgorithmen wie z.B. Selection Sort \parencite{Taschenbuch}, Shellsort oder Shakesort \parencite{Wirth} und \parencite{Grundkurs} kann hier nicht näher eingegangen werden. In genannten Büchern finden Sie nähere Informationen zu diesen Algorithmen.
\section{Allgemeine Komplexitätsbetrachtung von Sortieralgorithmen}
\begin{satz}
Ein Sortieralgorithmus, der auf Vergleichen von Datenelementen beruht, hat mindestens eine Komplexität von $O(n \cdot log(n))$.
\end{satz}
Um diesen Satz zu verstehen, verdeutlichen wir uns zunächst, was eine Sortierung im Allgemeinen bedeutet: \\
Aus der Menge aller möglichen Permutationen (also Anordnungen) von den Elementen unserer Menge suchen wir genau die eine heraus, in der alle Elemente \textit{sortiert} sind, nach unserem Kriterium. Insgesamt gibt es bei $n$ Elementen $n!$ Vergleiche. \\
\paragraph{Beweisidee}
Wir betrachten die Menge aller Permutationen, ordnen ihnen einen Wert zu und vergleichen nun jede Permutation mit der gewünschten, also derjenigen, in dem die Elemente vollständig sortiert sind. Nun können wir eine Hälfte an Permutationen ausmachen, deren Wert {\glqq}kleiner{\grqq} ist und eine weitere Hälfte, deren Wert {\glqq}größer oder gleich{\grqq} der Permutation ist. Hier sind nun $\frac{n!}{2}$ Vergleiche erforderlich. \\
Wir halbieren die Hälften fortlaufend und erhalten im letzten Schritt die eine verbleibende Permutation. \\
Sie ist nach $log(n!)$ Vergleichen erreicht.
\[O(log(n!)) \in O(n \cdot log(n))\]
\begin{proof}
\begin{align}
n^n \geq n! \\
1 \cdot 2 \cdot 3 \cdot ... \cdot (n-1) \cdot n = n \cdot (n-1) \cdot ... \cdot 3 \cdot 2 \cdot 1 \\
(n-k) \cdot (k+1) \geq n \quad \forall 0 \leq k \leq n \\
(n-1) \cdot ((n-1) \cdot 2) \cdot ... \cdot ((n-k) \cdot (k+1)) \cdot (1\cdot n) \geq n \cdot n \cdot ... \cdot n \\
n! \cdot n! \geq n^n \\
(1) \text{ und } (4): \quad \quad n^{2n} \geq n! \cdot n! \geq n^{n} \\
2n \cdot log(n) \geq 2 \cdot log(n!) \geq n \cdot log(n) \\
O(log(n!)) = O(n \cdot log(n))
\end{align}
\end{proof}
Tatsächlich ist damit gezeigt, dass es keinen Sortieralgorithmus gibt, dessen Komplexität geringer ist als $O(n \cdot log(n))$.
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Sortieren Sie das Feld \texttt{[56, 10, 15, 98, 99, 12, 30, 80]} aufsteigend. Notieren Sie Zwischenschritte
\begin{itemize}
\item Insertion Sort
\item Bubblesort
\item Quicksort
\item Mergesort
\end{itemize}
\begin{flushright}
Lösung auf Seite \pageref{a3.1:lsg} \\
entnommen aus \parencite{GrUeb}
\end{flushright}
\paragraph{Aufgabe 2} 
Beschreiben Sie Quicksort mit eigenen Worten.
\begin{flushright}
Lösung auf Seite \pageref{a3.2:lsg}
\end{flushright}
\paragraph{Aufgabe 3} 
Begründen Sie, mit welchen Verfahren Sie folgende Felder in optimaler Laufzeit aufsteigend sortieren können.
\begin{itemize}
\item \texttt{[1, 9, 2, 8, 3, 7, 4, 6, 5]}
\item \texttt{[9, 7, 7, 2, 5, 4, 7, 3, 1]}
\item \texttt{[9, 8, 7, 6, 5, 4, 3, 2, 1]}
\end{itemize}
\begin{flushright}
Lösung auf Seite \pageref{a3.3:lsg}
\end{flushright}
\paragraph{Aufgabe 4}
Entwickeln Sie für Mergesort und Quicksort eine anschauliche Darstellungsform.
\begin{flushright}
Lösung auf Seite \pageref{a3.4:lsg}
\end{flushright}
\pagebreak
\part{Bäume}
Vielleicht haben Sie die letzte Aufgabe vom letzten Teil gelöst -- Mit relativ hoher Wahrscheinlichkeit werden Sie eine Struktur beschrieben oder skizziert haben, die wir im Folgenden näher betrachten: Bäume! \\\\
Damit Sie vor lauter Bäumen den Wald (und die Wahrheit) nicht aus den Augen verlieren, widmen wir uns in aller Ausführlichkeit diesem in der Informatik sehr häufig anzutreffenden Gebilde, denken Sie beispielsweise an Ableitungs-, Entscheidungs-, Syntax- oder Codebäume.
\section{Der Wurzelbaum}
\begin{definition}
Ein Wurzelbaum $B = (V, E, r)$ besteht aus einer endlichen Menge von Konten $V$, einer endlichen Menge von gerichteten Kanten $E \subset V \times V$ und aus der Wurzel $r \in V$. \\
\begin{itemize}
\item Ein Knoten $r$ ist ein Wurzelbaum ($B = (\{r\}, \emptyset, r)$). 
\item Sind $B_{1} = (V_{1}, E_{1}), ..., B_{k} = (V_{k}, E_{k})$ Bäume mit den Wurzeln $r_{1}, ..., r_{k}$, so erweitern wir die Knotenmenge $V$ um eine neue Wurzel $r$ und die Kantenmenge $E$ um die Kanten $(r, r_{i}), i = 1, ... k$.\\\\
Der Baum
\[(V_{1} \cup ... \cup V_{n} \cup \{r\}, \{(r, r_{i}) \mid i = 1, ..., k\} \cup E_{1} \cup ... \cup E_{k}, r)\]
ist ein Wurzelbaum.
\end{itemize}
\end{definition}

Folglich betrachten wir hier nun also einen Baum mit einem Knoten, der den Ursprung darstellt, aus dem sich die anderen Knoten ableiten. In etwa so: \\
\begin{example}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt]
 \node {8}
    child {node {2}}
    child {node {32}
      child {node {16}}
      child {node {64}}
    };
\end{tikzpicture}
\caption{Wurzelbaum}
\end{figure}
\end{example}
\begin{definition}
Ein Baum, der keine Knoten und Kanten besitzt, heißt \textbf{leerer Baum}.
\end{definition}
Zu diesen Bäumen, von denen es die unterschiedlichsten Formen gibt, betrachten wir nun eine Reihe von Eigenschaften und Begrifflichkeiten.
\begin{definition}
\begin{itemize}
\item Ist $e = (v, w) \in E$, so heißt $v$ \textbf{Vater} von $w$ und $w$ \textbf{Sohn} oder \textbf{Kind} von $v$. Ein Knoten, der keine Söhne hat, heißt \textbf{Blatt}.
\item Ein Pfad $P$ in $B$ ist eine Folge von Knoten $v_{0}, ..., v_{n}$ mit $(v_{i}, v_{i+1}) \in E, i = 0, ..., n-1. n$ heißt \textbf{Länge} von $P$.
\item Seien $v, w \in V$. Der Knoten $w$ heißt \textbf{von Knoten $v$ aus erreichbar}, falls es einen Pfad $P$ von $v$ nach $w$ gibt.
\item Jeden Knoten $v$ von $B$ können wir als Wurzel des Teilbaums der von $v$ aus erreichbaren Knoten betrachten. \\\\ Hat $v$ die Söhne $v_{i}, ..., v_{k}$, so heißen die Bäume $B_{1}, ..., B_{k}$ mit den Wurzeln $v_{i}, ..., v_{k}$ die \textbf{Teilbäume} von $V$.
\item Die \textbf{Höhe} eines Knotens $v$ ist das Maximum der Längen aller Pfade, die in $v$ beginnen.
\item Die \textbf{Tiefe} eines Knotens ist die Länge des Pfades von der Wurzel zu $v$. \\\\
Die Knoten der Tiefe $i$ bilden die $i$-te Ebene des Baumes.
\item Die Höhe (und auch Tiefe) eines Baumes (nicht Knotens!) ist gegeben durch die Höhe der Wurzel.
\item Ein leerer Baum besitzt die Höhe/Tiefe $-1$.
\item Die \textbf{Ordnung} eines Baumes ist die maximale Anzahl an Söhnen eines Knotens.
\end{itemize}
\end{definition}
Zur Veranschaulichung wenden wir diese Begriffe auf den folgenden Beispielbaum an: \\
\begin{example}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {50}
    child {node {25}
	  child {node {12.5}
		child {node {6.25}}	    
	  }	  
	  child {node {37.5}}    
    }
    child {node {75}
      child {node {62.5}}
      child {node {87.5}}
    };
\end{tikzpicture}
\caption{Beispielbaum}
\end{figure}
\begin{itemize}
\item 50 ist die Wurzel des Baumes.
\item 6.25 ist Sohn/Kind von 12.5 und Blatt, da es keine Söhne hat.
\item 12.5 ist erreichbar von 50, da es einen Pfad gibt. Generell ist jedes Element eines Wurzelbaums von der Wurzel aus erreichbar.
\item 87.5 ist von 6.25 aus nicht erreichbar, da es keinen Pfad gibt.
\item Die Höhe von 75 beträgt 1. Die Tiefe von 6.25 beträgt 3.
\item Die Höhe/Tiefe des Baumes beträgt 3.
\item Die Ordnung des Baumes ist 2, da kein Knoten mehr als zwei Kinder hat.
\end{itemize}
\end{example}
\section{Binäre Bäume}
\begin{definition}
Ein Baum mit der Ordnung 2 heißt \textbf{binärer Baum}. Liegen die Elemente darin in geordneter Form vor, d.h. der Wert des linken Sohnes ist stets kleiner und der Wert des rechten Sohnes ist stets größer als sein Vater, spricht man von einem \textbf{geordneten binären Baum} \parencite{Wirth} oder \textbf{binären Suchbaum}.
\end{definition}
Diese besonderen Bäume wollen wir nun näher betrachten, da wir in binären Bäumen algorithmisch vorgehen können und so beispielsweise Suchprobleme lösen.
\begin{note}
Sei $n$ die Anzahl der Knoten in einem binären Baum der Höhe $h$. Dann ist die Anzahl der Knoten $n \leq 2^{n+1}-1$ oder äquivalent dazu. Die Höhe $h$ ist mindestens $log_{2}(n+1)-1$, also $\lceil log_{2}(n+1) \rceil -1 \leq h$. Diese Schranke wird für einen binären Baum, in dem alle Ebenen vollständig besetzt sind, angenommen.
\end{note}
\paragraph{Implementierung}
Wie schon eingangs erwähnt, hängen Bäume und Rekursion naturgemäß eng beieinander. Dies zeigt sich, wenn wir eine Implementierung eines binären Baums betrachten:
\begin{lstlisting}
struct node {
	int key;
	type value;
	node left, right;
}
\end{lstlisting}
Ein Knoten besteht also wiederum selbst aus Knoten, im Falle eines Blattes sind \texttt{left} und \texttt{right} dann leer. Wichtig ist die Unterscheidung zwischen \texttt{key} und \texttt{value} -- hier kann es leicht zu Unklarheiten kommen.
\subsection{Traversierung von Binärbäumen}
Auf Baumstrukturen lassen sich verschiedene Operationen durchführen. Grundlage dafür ist häufig ein \textit{traversieren} ({\glqq}durchlaufen{\grqq}) des Baumes. Dafür gibt es drei, jeweils rekursiv implementierte Vorgehensweisen, die unter anderem bei \parencite{Wirth} beschrieben werden: 
\begin{itemize}
\item \textbf{Preorder} -- W, A, B (besuche Wurzel vor den Teilbäumen)
\item \textbf{Inorder} -- A, W, B 
\item \textbf{Postorder} -- A, B, W (besuche Wurzel nach den Teilbäumen)
\end{itemize}
Grundlage dafür ist der allgemeine binäre Baum:
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
 \node {W}
    child {node {A}}
    child {node {B}};
\end{tikzpicture}
\caption{Allgemeiner binärer Baum}
\end{figure}
\begin{example}
Betrachten wir den Baum aus vorangegangenem Beispiel und traversieren wir diesen in allen drei Varianten: \\\\
\textbf{Preorder} \\
\texttt{50, 25, 12.5, 6.25, 37.5, 75, 63.5, 87.5} \\\\
\textbf{Inorder} \\
\texttt{6.25, 12.5, 25, 37.5, 50, 63.5, 75, 87.5} \\\\
\textbf{Postorder} \\
\texttt{6.25, 37.5, 12.5, 25, 63.5, 87.5, 75, 50}
\end{example}
Neben den drei genannten Vorgehenweisen betrachten wir noch die klassische, den Baum in seinen Ebenen ausgebende: \textbf{levelorder}. Auf unser Beispiel bezogen wäre dies:
\texttt{50, 25, 75, 12.5, 37.5, 63.5, 87.5, 6.25}
\subsection{Operationen}
Auf binären Bäumen lassen sich nun drei Operationen konstruieren: \textbf{Suchen, Einfügen} und \textbf{Löschen}.
\paragraph{Suchen}
Ein Baum wird durchsucht, in dem jeweils das Suchelement mit dem Wert des Knotens verglichen und anschließend mit dem linken Sohn, falls das gesuchte Element kleiner ist, oder dem rechten Sohn, falls das gesuchte Element größer ist, als neuen Knoten rekursiv fortgefahren.
\begin{lstlisting}
node Search(int k, node Tree) {
	if (Tree == null) {
		return null;	
	}
	else if (k == Tree.key) {
		return Tree;	
	}
	else if (k < Tree.key) {
		Search(k, Tree.left);	
	}
	else if (k > Tree.key) {
		Search(k, Tree.right);	
	}
}
\end{lstlisting}
\paragraph{Einfügen}
Ähnlich läuft das Einfügen eines Elements $k$ mit Wert $v$: 
\begin{lstlisting}
node Insert(int k, node Tree, type v) {
	if (T == null) {
		return "neuer Knoten (k, v)";	
	}
	else if (k < T.key) {
		T.left =	 Insert(k, T.left, v);
	}
	else if (k > T.key) {
		T.right = Insert(k, T.right, v);	
	}
	else if (k == T.key) {
		T.value = v;
	}
	return T;
}
\end{lstlisting}
Ein Einfügen kann folglich auch ein {\glqq}Wert ändern{\grqq} sein.
\paragraph{Löschen}
Das Löschen eines Knotens gestaltet sich etwas aufwendiger. \\
Suche Zeiger T auf den zu löschenden Schlüssel/Knoten und unterscheide:
\begin{itemize}
\item Das zu löschende Element hat keine Kinder: Setze Zeiger T auf null
\item Das zu löschende Element hat ein Kind: Ersetze T durch den Zeiger auf den Kindknoten
\item Das zu löschende Element hat zwei Kinder: Ersetze den gelöschten Knoten durch den kleinsten Schlüssel des rechten Teilbaums.
\end{itemize}
Eine Implementierung davon finden Sie bei \parencite{Wirth}.
\paragraph{Komplexität}
Für die Suche und das Löschen liegt die Komplexität im Durchschnittsfall in $O(n)$, wobei $n$ die Höhe des Baumes ist. Unter günstigen Umständen ist auch eine Komplexität von $O(log(n))$ möglich. Die Komplexität des Einfügens liegt in $O(1)$.
\section{AVL-Bäume}
Ein weiterer Typ von Bäumen sind die sogenannten \textbf{AVL-Bäume}, die durch einen geringen zusätzlichen Aufwand beim Einfügen/Löschen erzeugt werden und eine günstigere Komplexität besitzen.
\begin{definition}
Ein binärer Baum heißt \textbf{(AVL-)ausgeglichen}, falls für jeden Konten $v$ gilt: \\\\
Die Höhen des linken und rechten Teilbaums unterscheiden sich höchstens um 1. \\Diese Differenz der Höhen heißt \textbf{Balance} von $v$.
\[\text{Balance} = h(rT) - h(lT)\]
Ausgeglichene, binäre Suchbäume heißen nun \textbf{AVL-Bäume}.
\end{definition}
\begin{example}
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {7}
    child {node {2}
    child {node {1}}
    child {node {5}}}
    child {node {8}
    child[fill=none] {edge from parent[draw=none]}
    child {node {9}}};
\end{tikzpicture}
\caption{Beispiel für einen AVL-Baum}
\end{figure}
Dieser Baum ist ein AVL-Baum, da er die drei Kriterien erfüllt:
\begin{itemize}
\item binär: Alle Knoten haben höchstens zwei Kinder, mindestens ein Knoten hat mindestens zwei Kinder
\item Suchbaum: Alle Elemente im linken Teilbaum sind kleiner als das Wurzelelement des Teilbaums, alle Elemente im rechten Teilbaum sind größer als der Teilbaum
\item AVL-Kriterium: Die Balancen der Teilbäume unterscheiden sich höchstens um 1.
\end{itemize}
Kein AVL-Baum hingegen ist der folgende:
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {E}
    child {node {A}
    child[fill=none] {edge from parent[draw=none]}
    child {node {C}
    child {node {B}}
    child {node {D}}}}
    child {node {F}
    child[fill=none] {edge from parent[draw=none]}
    child {node {H}
    child {node {G}}
    child[fill=none] {edge from parent[draw=none]}}};
\end{tikzpicture}
\caption{Kein Beispiel für einen AVL-Baum}
\end{figure}
Hier ist die AVL-Bedingung offensichtlich verletzt: Die Balance an den Knoten A und F ist jeweils größer als 1. Damit handelt es sich nicht um einen AVL-Baum.
\end{example}
Um zu ermitteln, ob es sich bei einem Baum um einen AVL-Baum handelt, prüfen wir also zunächst, ob es sich um einen binären Suchbaum handelt. Nun bestimmen wir zu jedem Knoten die Balance: Jedes Blatt hat die Balance 0, falls der rechte Teilbaum eine größere Höhe hat, ist die Balance positiv. Falls der linke Teilbaum eine größere Höhe hat, ist die Balance negativ. Eine Balance von 1 / -1 wird toleriert, alle größeren Balancen führen dazu, dass der Baum kein AVL-Baum sein kann. Ein Knoten mit Balance größer 1 genügt.
\begin{note}
Für die Höhe eines ausgeglichenen Baumes mit $n$ Knoten gilt:
\[h < 1.45 \cdot log_2(n+2) - 1.33\]
Dieses Kriterium eignet sich aus naheliegenden Gründen lediglich für eine Abschätzung. 
\end{note}
\paragraph{Suche}
Da ein AVL-Baum ein binärer Suchbaum ist, verwenden wir zum Suchen die Suchfunktion eines binären Suchbaums. Das AVL-Kriterium hat keine Auswirkungen auf die Komplexität.
\paragraph{Einfügen}
Wir suchen nach dem einzufügenden Element $e$. Falls $e$ nicht im Baum, endet die Suche in einem Blatt. An diesem Blatt verankern wir einen neuen Knoten und füllen ihn mit $e$. \textbf{Hierbei kann die AVL-Bedingung verletzt werden!} \\
Anschließend reorganisieren wir den Baum, um die AVL-Bedingung wiederherzustellen:
\begin{itemize}
\item Prüfe für jeden Knoten $n$ des Suchpfades, ob er ausgeglichen ist.
\item Falls nicht: Rotation
\end{itemize}
\paragraph{Rotation}
Rotation können wir uns vorstellen als eine Art {\glqq}Kippen{\grqq} eines Knotens, um eine Balance wiederherzustellen bzw. zu nivellieren. Wir unterscheiden zwei Rotationsarten:
\begin{itemize}
\item \textbf{Linksrotation (LR)} Ein rechtes Kind kann nach links rotiert werden.
\item \textbf{Rechtsrotation (RR)} Ein linkes Kind kann nach rechts rotiert werden.
\end{itemize}
Rotation ist möglich, da ein vormals linkes Kind -- das kleiner als die Wurzel ist -- auch die neue Wurzel mit der vorherigen Wurzel als neues rechtes Kind -- das dann weiterhin größer ist -- sein kann. Im Prinzip ändert sich folglich jeweils nur die Verwandtschaftsbeziehung zwischen Kind und Wurzel mit dem Effekt, dass auch die Kinder neu, meistens paritätischer, angeordnet werden.
\begin{example}
\begin{figure}[h!]
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {7}
    child {node {2}}
    child {node {42}
    child {node {24}
    child {node {15}}}
    child {node {72}}};
\hspace{4cm} $\text{RR(24)} \Rightarrow$ \hspace{4cm}
\node {7}
    child {node {2}}
    child {node {24}
    child {node {15}}
    child {node {42}
    child {node {72}}}};
\end{tikzpicture}
\caption{AVL-Baum mit Rechtsrotation des Knotens 24}
\end{figure}
Der hier vorliegende Baum wurde am Knoten 24 nach rechts rotiert. Dadurch ist die 24 die neue Wurzel geworden, während die 42 nun eine Ebene tiefer ein Kind geworden ist. Doch auch nach dieser Rotation zeigt sich, dass die AVL-Bedingung weiterhin nicht erfüllt ist. Also ist ein weiterer Rotationsschritt notwendig:
\begin{figure}[h!]
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {7}
    child {node {2}}
    child {node {24}
    child {node {15}}
    child {node {42}
    child {node {72}}}};
\hspace{4cm} $\text{LR(24)} \Rightarrow$ \hspace{4cm}
\node {24}
    child {node {7}
    child {node {2}}
    child {node {15}}}
    child {node {42}
    child[fill=none] {edge from parent[draw=none]}
    child {node {72}}};
\end{tikzpicture}
\caption{AVL-Baum mit Linksrotation des Knotens 24}
\end{figure}
Nun zeigt sich, dass die AVL-Bedingung wieder erfüllt ist.
\end{example}
Aus dem Beispiel können wir nun die Fälle für Rotationen unterscheiden:
\begin{itemize}
\item Eine einfache Rotation wird dann notwendig, wenn
\begin{itemize}
\item in einem linken Teilbaum ein linkes Kind eingefügt wird  $\to$ einfache Rechtsrotation
\item in einem rechten Teilbaum ein rechtes Kind eingefügt wird $\to$ einfache Linksrotation
\end{itemize}
\item Eine zweifache Rotation wird hingegen angewandt, falls
\begin{itemize}
\item in einem linken Teilbaum ein rechtes Kind eingefügt wird $\to$ Links-Rechts-Rotation
\item in einem rechten Teilbaum ein linkes Kind eingefügt wird $\to$ Rechts-Links-Rotation
\end{itemize}
\end{itemize}
\paragraph{Löschen}
Zunächst wird wie in einem binären Suchbaum vorgegangen. Anschließend muss -- wie beim Einfügen -- die AVL-Bedingung durch Rotationen wiederhergestellt werden. \\\\
Für die Betrachtung der Komplexität ergibt sich nun:
\begin{theorem}
Die Operationen Suchen, Einfügen und Löschen eines Schlüssels können in $O(log (n))$ durchgeführt werden.
\end{theorem}
\pagebreak
\section*{Aufgaben}
\paragraph{Aufgabe 1} Gegeben ist der folgende Baum.
\begin{itemize}
\item Geben Sie folgende Eigenschaften an: Wurzel, Höhe des Baumes, Höhe des Knotens $E$, Ordnung des Baumes, Balance des Knotens $K$
\item Fügen Sie den Knoten $J$ hinzu.
\item Löschen Sie den Knoten $B$.
\end{itemize}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=40mm]
\tikzstyle{level 2}=[sibling distance=20mm]
 \node {F}
    child {node {B}
    child {node {A}}
    child {node {D}
    child {node {C}}
    child {node {E}}}}
    child {node {K}
    child {node {H}}
    child {node {T}
    child[fill=none] {edge from parent[draw=none]}
    child {node {W}}}};
\end{tikzpicture}
\end{figure}
\begin{flushright}
Lösung auf Seite \pageref{a4.1:lsg} \\
\end{flushright}
\paragraph{Aufgabe 2} 
Untersuchen und begründen Sie, ob es sich bei diesem Baum um einen AVL-Baum handelt. Fügen Sie hierzu die Balancen jedes Knotens ein. 
\begin{flushright}
Lösung auf Seite \pageref{a4.2:lsg}
\end{flushright}
\paragraph{Aufgabe 3}
Falls es sich nicht um einen AVL-Baum handelt, ergänzen Sie den Baum zu einem AVL-Baum und führen Sie dann folgende Aufgabe aus.
\begin{itemize}
\item Löschen Sie den Knoten $A$.
\item Fügen Sie den Knoten $Z$ hinzu. 
\end{itemize}
Falls Rotationen erforderlich sind, stellen Sie diese detailliert dar. 
\begin{flushright}
Lösung auf Seite \pageref{a4.3:lsg}
\end{flushright}
\paragraph{Aufgabe 4}
Beschreiben Sie in Worten und Pseudocode das Vorgehen beim Suchen eines Elementes in einem binären Baum. Beurteilen Sie, ob dieses Verfahren auch in einem Baum der Ordnung 3 angewendet werden kann.
\begin{flushright}
Lösung auf Seite \pageref{a4.4:lsg}
\end{flushright}
\pagebreak
\part{Heaps}
Eigentlich auch zu Bäumen gehörend, betrachten wir im Folgenden die Datenstruktur \textbf{Heap}. Als Heap bezeichnen wir eine Struktur (einen {\glqq}Haufen{\grqq}) von Objekten, die über Schlüssel mit einer Priorität versehen sind. Wirth definiert Heaps in \parencite[][S. 92]{Wirth} wie folgt:
\begin{definition}
Ein heap ist definiert als eine Folge von Schlüsseln h[l], h[l+1], ..., h[r] mit den Relationen:
\begin{itemize}
\item h[i] $\leq$ h[2i]
\item h[i] $\leq$ h[2i+1] $\forall i = 1 ... r/2$
\end{itemize}
\end{definition}
Betrachten wir einen Heap als Baum, so ist $h[2i]$ das linke Kind des Knotens $h[i]$ und $h[2i+1]$ entsprechend das rechte. Somit muss jede Wurzel eines Teilbaums kleiner als ihre Kinder sein. Diese Struktur bezeichnen wir als \textbf{Min-Heap}. Invers dazu bezeichnen wir Heaps, in denen die Wurzel eines Teilbaums jeweils größer ist als ihre Kinder als \textbf{Max-Heap}. Beide Varianten finden sich in der Literatur. Da in der Vorlesung lediglich der Max-Heap eingeführt wurde, beschränken wir uns der Übersichtlichkeit halber auf diesen Typ.
\begin{example}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=60mm]
\tikzstyle{level 2}=[sibling distance=30mm]
\tikzstyle{level 3}=[sibling distance=15mm]
 \node {T}
    child {node {S}
    child {node {P}
    child {node {E}}
    child {node {I}}}
    child {node {N}
    child {node {H}}
    child {node {G}}}}
    child {node {R}
    child {node {O}}
    child {node {A}}};
\end{tikzpicture}
\caption{Binärer Max-Heap}
\end{figure}
\end{example}
Wir erkennen und formulieren als Eigenschaften:
\begin{itemize}
\item Es handelt sich um einen (fast) vollständigen Binärbaum, d.h. die Blätter befinden sich auf höchstens zwei verschiedenen Levels.
\item Die Blätter auf dem untersten Level sind linksbündig angeordnet.
\end{itemize}
\paragraph{Implementierung eines Heaps als Array}
Ein Heap lässt sich als Feld/Array implementieren, da für die Indizes die oberen Bedingungen leicht erkannt und umgesetzt werden können. 
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{every node}=[circle,draw,inner sep=1pt, minimum size=1cm]
\tikzstyle{level 1}=[sibling distance=60mm]
\tikzstyle{level 2}=[sibling distance=30mm]
\tikzstyle{level 3}=[sibling distance=15mm]
 \node {A[1]}
    child {node {A[2]}
    child {node {A[4]}
    child {node {A[8]}}
    child {node {A[9]}}}
    child {node {A[5]}
    child {node {A[10]}}
    child {node {A[11]}}}}
    child {node {A[3]}
    child {node {A[6]}}
    child {node {A[7]}}};
\end{tikzpicture}
\caption{Binärer Max-Heap in Array-Darstellung}
\end{figure}
Als Array erhalten wir damit:
\begin{table}[h]
\begin{tabular}{cccccccccccc}
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\
\hline
11 & T & S & R & P & N & O & A & E & I & H & G \\
\end{tabular}
\end{table}
In $A[0]$ speichern wir die Größe des Heaps, in diesem Fall 11, da es sich um 11 Elemente handelt. \\
Wir können erneut festhalten: 
\begin{itemize}
\item $A[i]$ hat linkes Kind $A[2i]$
\item $A[i]$ hat rechtes Kind $A[2i+1]$
\item $A[i]$ hat Vater $A[\frac{i}{2}]$ (bei Ganzzahldivision)
\item $A[i]$ befindet sich auf dem Level $\lfloor log_{2}(i) \rfloor$
\end{itemize}
\pagebreak
\part{Anhang}
\section*{Lösungen}
\subsection{Grundlagen der Algorithmik}
\label{a1:lsg}
\label{a2:lsg}
\label{a3:lsg}
\label{a4:lsg}
\subsection{Suchalgorithmen}
\label{a2.1:lsg}
\label{a2.2:lsg}
\label{a2.3:lsg}
\label{a2.4:lsg}
\subsection{Sortieralgorithmen}
\label{a3.1:lsg}
\label{a3.2:lsg}
\label{a3.3:lsg}
\label{a3.4:lsg}
\subsection{Bäume}
\label{a4.1:lsg}
\label{a4.2:lsg}
\label{a4.3:lsg}
\label{a4.4:lsg}
\section*{Tabellen und Formeln}
\listoffigures
\printbibliography
\end{document}